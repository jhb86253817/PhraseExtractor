[["Sparsity-certifying Graph Decompositions", "We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. Special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. In particular, our colored pebbles generalize and strengthen the previous results of Lee and Streinu and give a new proof of the Tutte-Nash-Williams characterization of arboricity. We also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. Our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and Westermann and Hendrickson.", ["Dense graph", "Structural rigidity", "Crispin Nash-Williams", "Graph theory", "Graph (mathematics)", "Algorithm", "Sparse matrix", "Glossary of graph theory"]], ["A limit relation for entropy and channel capacity per unit cost", "In a quantum mechanical model, Diosi, Feldmann and Kosloff arrived at a conjecture stating that the limit of the entropy of certain mixtures is the relative entropy as system size goes to infinity. The conjecture is proven in this paper for density matrices. The first proof is analytic and uses the quantum law of large numbers. The second one clarifies the relation to channel capacity per unit cost for classical-quantum channels. Both proofs lead to generalization of the conjecture.", ["Density matrix", "Conjecture", "Law of large numbers", "Infinity", "Quantum mechanics", "Channel capacity", "Mathematical proof", "Entropy"]], ["Intelligent location of simultaneously active acoustic emission sources: Part I", "The intelligent acoustic emission locator is described in Part I, while Part II discusses blind source separation, time delay estimation and location of two simultaneously active continuous acoustic emission sources. The location of acoustic emission on complicated aircraft frame structures is a difficult problem of non-destructive testing. This article describes an intelligent acoustic emission source locator. The intelligent locator comprises a sensor antenna and a general regression neural network, which solves the location problem based on learning from examples. Locator performance was tested on different test specimens. Tests have shown that the accuracy of location depends on sound velocity and attenuation in the specimen, the dimensions of the tested area, and the properties of stored data. The location accuracy achieved by the intelligent locator is comparable to that obtained by the conventional triangulation method, while the applicability of the intelligent locator is more general since analysis of sonic ray paths is avoided. This is a promising method for non-destructive testing of aircraft frame structures by the acoustic emission method.", ["Blind signal separation", "Sensor", "Antenna (radio)", "Aircraft", "Neural network", "Attenuation", "Speed of sound", "Nondestructive testing", "Triangulation", "Blindness", "Velocity"]], ["Intelligent location of simultaneously active acoustic emission sources: Part II", "Part I describes an intelligent acoustic emission locator, while Part II discusses blind source separation, time delay estimation and location of two continuous acoustic emission sources. Acoustic emission (AE) analysis is used for characterization and location of developing defects in materials. AE sources often generate a mixture of various statistically independent signals. A difficult problem of AE analysis is separation and characterization of signal components when the signals from various sources and the mode of mixing are unknown. Recently, blind source separation (BSS) by independent component analysis (ICA) has been used to solve these problems. The purpose of this paper is to demonstrate the applicability of ICA to locate two independent simultaneously active acoustic emission sources on an aluminum band specimen. The method is promising for non-destructive testing of aircraft frame structures by acoustic emission analysis.", ["Blind signal separation", "Independent component analysis", "Aluminium", "Nondestructive testing", "Aircraft", "Independence (probability theory)", "Source separation"]], ["On-line Viterbi Algorithm and Its Relationship to Random Walks", "In this paper, we introduce the on-line Viterbi algorithm for decoding hidden Markov models (HMMs) in much smaller than linear space. Our analysis on two-state HMMs suggests that the expected maximum memory used to decode sequence of length $n$ with $m$-state HMM can be as low as $\\Theta(m\\log n)$, without a significant slow-down compared to the classical Viterbi algorithm. Classical Viterbi algorithm requires $O(mn)$ space, which is impractical for analysis of long DNA sequences (such as complete human genome chromosomes) and for continuous data streams. We also experimentally demonstrate the performance of the on-line Viterbi algorithm on a simple HMM for gene finding on both simulated and real DNA sequences.", ["Viterbi algorithm", "Gene", "Genome", "Human genome", "Chromosome", "Vector space", "Gene prediction", "DNA", "Nucleic acid sequence", "Algorithm", "Hidden Markov model"]], ["Real Options for Project Schedules (ROPS)", "Real Options for Project Schedules (ROPS) has three recursive sampling/optimization shells. An outer Adaptive Simulated Annealing (ASA) optimization shell optimizes parameters of strategic Plans containing multiple Projects containing ordered Tasks. A middle shell samples probability distributions of durations of Tasks. An inner shell samples probability distributions of costs of Tasks. PATHTREE is used to develop options on schedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to develop a relative risk analysis among projects.", ["Mathematical optimization", "Simulated annealing", "Probability distribution", "Probability", "Relative risk", "Real options valuation", "Risk", "Risk management", "Recursion"]], ["Sparsely-spread CDMA - a statistical mechanics based analysis", "Sparse Code Division Multiple Access (CDMA), a variation on the standard CDMA method in which the spreading (signature) matrix contains only a relatively small number of non-zero elements, is presented and analysed using methods of statistical physics. The analysis provides results on the performance of maximum likelihood decoding for sparse spreading codes in the large system limit. We present results for both cases of regular and irregular spreading matrices for the binary additive white Gaussian noise channel (BIAWGN) with a comparison to the canonical (dense) random spreading code.", ["Statistical mechanics", "Maximum likelihood", "Additive white Gaussian noise", "Matrix (mathematics)", "Gaussian noise", "Statistical physics", "Code division multiple access", "Statistics", "Physics", "Decoding methods", "Mechanics", "Normal distribution", "Randomness"]], ["Reducing SAT to 2-SAT", "Description of a polynomial time reduction of SAT to 2-SAT of polynomial size.", ["Polynomial time", "2-satisfiability", "Polynomial", "SAT"]], ["Geometric Complexity Theory V: On deciding nonvanishing of a generalized Littlewood-Richardson coefficient", "This article has been withdrawn because it has been merged with the earlier article GCT3 (arXiv: CS/0501076 [cs.CC]) in the series. The merged article is now available as: Geometric Complexity Theory III: on deciding nonvanishing of a Littlewood-Richardson Coefficient, Journal of Algebraic Combinatorics, vol. 36, issue 1, 2012, pp. 103-110. (Authors: Ketan Mulmuley, Hari Narayanan and Milind Sohoni) The new article in this GCT5 slot in the series is: Geometric Complexity Theory V: Equivalence between blackbox derandomization of polynomial identity testing and derandomization of Noether's Normalization Lemma, in the Proceedings of FOCS 2012 (abstract), arXiv:1209.5993 [cs.CC] (full version) (Author: Ketan Mulmuley)", ["ArXiv", "Complexity", "Derandomization", "Coefficient", "Geometry"]], ["Capacity of a Multiple-Antenna Fading Channel with a Quantized Precoding Matrix", "Given a multiple-input multiple-output (MIMO) channel, feedback from the receiver can be used to specify a transmit precoding matrix, which selectively activates the strongest channel modes. Here we analyze the performance of Random Vector Quantization (RVQ), in which the precoding matrix is selected from a random codebook containing independent, isotropically distributed entries. We assume that channel elements are i.i.d. and known to the receiver, which relays the optimal (rate-maximizing) precoder codebook index to the transmitter using B bits. We first derive the large system capacity of beamforming (rank-one precoding matrix) as a function of B, where large system refers to the limit as B and the number of transmit and receive antennas all go to infinity with fixed ratios. With beamforming RVQ is asymptotically optimal, i.e., no other quantization scheme can achieve a larger asymptotic rate. The performance of RVQ is also compared with that of a simpler reduced-rank scalar quantization scheme in which the beamformer is constrained to lie in a random subspace. We subsequently consider a precoding matrix with arbitrary rank, and approximate the asymptotic RVQ performance with optimal and linear receivers (matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show that these approximations accurately predict the performance of finite-size systems of interest. Given a target spectral efficiency, numerical examples show that the amount of feedback required by the linear MMSE receiver is only slightly more than that required by the optimal receiver, whereas the matched filter can require significantly more feedback.", ["Beamforming", "Matched filter", "Quantization (signal processing)", "Antenna (radio)", "Spectral efficiency", "MIMO", "Codebook", "Precoding", "Transmitter", "Mean squared error", "Infinity", "Isotropy", "Asymptotically optimal algorithm", "Linear"]], ["On Almost Periodicity Criteria for Morphic Sequences in Some Particular Cases", "In some particular cases we give criteria for morphic sequences to be almost periodic (=uniformly recurrent). Namely, we deal with fixed points of non-erasing morphisms and with automatic sequences. In both cases a polynomial-time algorithm solving the problem is found. A result more or less supporting the conjecture of decidability of the general problem is given.", ["Time complexity", "Fixed point (mathematics)", "Conjecture", "Decidability (logic)"]], ["Geometric Complexity Theory VI: the flip via saturated and positive integer programming in representation theory and algebraic geometry", "This article belongs to a series on geometric complexity theory (GCT), an approach to the P vs. NP and related problems through algebraic geometry and representation theory. The basic principle behind this approach is called the flip. In essence, it reduces the negative hypothesis in complexity theory (the lower bound problems), such as the P vs. NP problem in characteristic zero, to the positive hypothesis in complexity theory (the upper bound problems): specifically, to showing that the problems of deciding nonvanishing of the fundamental structural constants in representation theory and algebraic geometry, such as the well known plethysm constants--or rather certain relaxed forms of these decision probelms--belong to the complexity class P. In this article, we suggest a plan for implementing the flip, i.e., for showing that these relaxed decision problems belong to P. This is based on the reduction of the preceding complexity-theoretic positive hypotheses to mathematical positivity hypotheses: specifically, to showing that there exist positive formulae--i.e. formulae with nonnegative coefficients--for the structural constants under consideration and certain functions associated with them. These turn out be intimately related to the similar positivity properties of the Kazhdan-Lusztig polynomials and the multiplicative structural constants of the canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum groups. The known proofs of these positivity properties depend on the Riemann hypothesis over finite fields and the related results. Thus the reduction here, in conjunction with the flip, in essence, says that the validity of the P vs. NP conjecture in characteristic zero is intimately linked to the Riemann hypothesis over finite fields and related problems.", ["P versus NP problem", "Representation theory", "Algebraic geometry", "Upper and lower bounds", "Riemann hypothesis", "Decision problem", "Quantum group", "Integer", "Mathematics", "Finite set", "Computational complexity theory", "Polynomial", "Natural number", "Characteristic (algebra)", "Algebraic number", "Integer programming", "Complexity", "Complexity class", "Crystal", "Mathematical proof", "Geometry", "Hypothesis", "Serializability", "Vladimir Drinfel'd", "0 (number)", "Bernhard Riemann", "Finite field"]], ["On Punctured Pragmatic Space-Time Codes in Block Fading Channel", "This paper considers the use of punctured convolutional codes to obtain pragmatic space-time trellis codes over block-fading channel. We show that good performance can be achieved even when puncturation is adopted and that we can still employ the same Viterbi decoder of the convolutional mother code by using approximated metrics without increasing the complexity of the decoding operations.", ["Viterbi decoder", "Fading", "Spacetime", "Decoder"]], ["Differential Recursion and Differentially Algebraic Functions", "Moore introduced a class of real-valued \"recursive\" functions by analogy with Kleene's formulation of the standard recursive functions. While his concise definition inspired a new line of research on analog computation, it contains some technical inaccuracies. Focusing on his \"primitive recursive\" functions, we pin down what is problematic and discuss possible attempts to remove the ambiguity regarding the behavior of the differential recursion operator on partial functions. It turns out that in any case the purported relation to differentially algebraic functions, and hence to Shannon's model of analog computation, fails.", ["Algebraic function", "Analogy", "Recursion", "Stephen Cole Kleene", "Partial function", "Computation", "Analog computer", "Primitive recursive function"]], ["The World as Evolving Information", "This paper discusses the benefits of describing the world as information, especially in the study of the evolution of life and cognition. Traditional studies encounter problems because it is difficult to describe life and cognition in terms of matter and energy, since their laws are valid only at the physical scale. However, if matter and energy, as well as life and cognition, are described in terms of information, evolution can be described consistently as information becoming more complex. The paper presents eight tentative laws of information, valid at multiple scales, which are generalizations of Darwinian, cybernetic, thermodynamic, psychological, philosophical, and complexity principles. These are further used to discuss the notions of life, cognition and their evolution.", ["Cognition"]], ["The Complexity of HCP in Digraps with Degree Bound Two", "The Hamiltonian cycle problem (HCP) in digraphs D with degree bound two is solved by two mappings in this paper. The first bijection is between an incidence matrix C_{nm} of simple digraph and an incidence matrix F of balanced bipartite undirected graph G; The second mapping is from a perfect matching of G to a cycle of D. It proves that the complexity of HCP in D is polynomial, and finding a second non-isomorphism Hamiltonian cycle from a given Hamiltonian digraph with degree bound two is also polynomial. Lastly it deduces P=NP base on the results.", ["Matching (graph theory)", "Hamiltonian path", "Undirected graph", "Directed graph", "Bijection", "Incidence matrix", "P versus NP problem", "Bipartite graph", "Hamiltonian path problem", "Graph (mathematics)", "Isomorphism", "Matrix (mathematics)", "Polynomial", "Degree (graph theory)", "Hamiltonian (quantum mechanics)", "Complexity", "NP (complexity)"]], ["Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo Codes", "It has been observed that particular rate-1/2 partially systematic parallel concatenated convolutional codes (PCCCs) can achieve a lower error floor than that of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can only be identified by means of an exhaustive search, whilst convergence towards low bit error probabilities can be problematic when the systematic output of a rate-1/2 partially systematic PCCC is heavily punctured. In this paper, we present and study a family of rate-1/2 partially systematic PCCCs, which we call pseudo-randomly punctured codes. We evaluate their bit error rate performance and we show that they always yield a lower error floor than that of their rate-1/3 parent codes. Furthermore, we compare analytic results to simulations and we demonstrate that their performance converges towards the error floor region, owning to the moderate puncturing of their systematic output. Consequently, we propose pseudo-random puncturing as a means of improving the bandwidth efficiency of a PCCC and simultaneously lowering its error floor.", ["Bit", "Bit error rate", "Concatenation", "Brute-force search", "Pseudorandom number generator", "Limit of a sequence"]], ["Inapproximability of Maximum Weighted Edge Biclique and Its Applications", "Given a bipartite graph $G = (V_1,V_2,E)$ where edges take on {\\it both} positive and negative weights from set $\\mathcal{S}$, the {\\it maximum weighted edge biclique} problem, or $\\mathcal{S}$-MWEB for short, asks to find a bipartite subgraph whose sum of edge weights is maximized. This problem has various applications in bioinformatics, machine learning and databases and its (in)approximability remains open. In this paper, we show that for a wide range of choices of $\\mathcal{S}$, specifically when $| \\frac{\\min\\mathcal{S}} {\\max \\mathcal{S}} | \\in \\Omega(\\eta^{\\delta-1/2}) \\cap O(\\eta^{1/2-\\delta})$ (where $\\eta = \\max\\{|V_1|, |V_2|\\}$, and $\\delta \\in (0,1/2]$), no polynomial time algorithm can approximate $\\mathcal{S}$-MWEB within a factor of $n^{\\epsilon}$ for some $\\epsilon > 0$ unless $\\mathsf{RP = NP}$. This hardness result gives justification of the heuristic approaches adopted for various applied problems in the aforementioned areas, and indicates that good approximation algorithms are unlikely to exist. Specifically, we give two applications by showing that: 1) finding statistically significant biclusters in the SAMBA model, proposed in \\cite{Tan02} for the analysis of microarray data, is $n^{\\epsilon}$-inapproximable; and 2) no polynomial time algorithm exists for the Minimum Description Length with Holes problem \\cite{Bu05} unless $\\mathsf{RP=NP}$.", ["Machine learning", "Bioinformatics", "Bipartite graph", "Approximation algorithm", "Complete bipartite graph", "Graph (mathematics)", "Heuristic", "Subgraph", "Polynomial time", "Time complexity", "Algorithm", "Microarray", "Statistical significance", "Polynomial"]], ["Refuting the Pseudo Attack on the REESSE1+ Cryptosystem", "We illustrate through example 1 and 2 that the condition at theorem 1 in [8] dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does not hold, namely the condition Z/M - L/Ak < 1/(2 Ak^2) is not sufficient for f(i) + f(j) = f(k). Illuminate through an analysis and ex.3 that there is a logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4 to be invalid. Demonstrate through ex.4 and 5 that each or the combination of qu+1 > qu * D at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) + f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4 and alg.2 based on table 1 are disordered and wrong logically. Further, manifest through a repeated experiment and ex.5 that the data at table 2 is falsified, and the example in [8] is woven elaborately. We explain why Cx = Ax * W^f(x) (% M) is changed to Cx = (Ax * W^f(x))^d (% M) in REESSE1+ v2.1. To the signature fraud, we point out that [8] misunderstands the existence of T^-1 and Q^-1 % (M-1), and forging of Q can be easily avoided through moving H. Therefore, the conclusion of [8] that REESSE1+ is not secure at all (which connotes that [8] can extract a related private key from any public key in REESSE1+) is fully incorrect, and as long as the parameter Omega is fitly selected, REESSE1+ with Cx = Ax * W^f(x) (% M) is secure.", ["Logic", "Fraud", "Deductive reasoning", "Proposition", "Cryptosystem", "Public-key cryptography", "Experiment"]], ["Optimal Routing for Decode-and-Forward based Cooperation in Wireless Networks", "We investigate cooperative wireless relay networks in which the nodes can help each other in data transmission. We study different coding strategies in the single-source single-destination network with many relay nodes. Given the myriad of ways in which nodes can cooperate, there is a natural routing problem, i.e., determining an ordered set of nodes to relay the data from the source to the destination. We find that for a given route, the decode-and-forward strategy, which is an information theoretic cooperative coding strategy, achieves rates significantly higher than that achievable by the usual multi-hop coding strategy, which is a point-to-point non-cooperative coding strategy. We construct an algorithm to find an optimal route (in terms of rate maximizing) for the decode-and-forward strategy. Since the algorithm runs in factorial time in the worst case, we propose a heuristic algorithm that runs in polynomial time. The heuristic algorithm outputs an optimal route when the nodes transmit independent codewords. We implement these coding strategies using practical low density parity check codes to compare the performance of the strategies on different routes.", ["Factorial", "Metaheuristic", "Wireless", "Information theory", "Data", "Data transmission", "Polynomial", "Wireless network", "Low-density parity-check code", "Best, worst and average case", "Polynomial time", "Algorithm", "Forward error correction", "Parity bit", "Routing", "Decode (song)", "Code word"]], ["Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless Networks", "This paper investigates the many-to-one throughput capacity (and by symmetry, one-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has generally been assumed in prior studies that the many-to-one throughput capacity is upper-bounded by the link capacity L. Throughput capacity L is not achievable under 802.11. This paper introduces the notion of \"canonical networks\", which is a class of regularly-structured networks whose capacities can be analyzed more easily than unstructured networks. We show that the throughput capacity of canonical networks under 802.11 has an analytical upper bound of 3L/4 when the source nodes are two or more hops away from the sink; and simulated throughputs of 0.690L (0.740L) when the source nodes are many hops away. We conjecture that 3L/4 is also the upper bound for general networks. When all links have equal length, 2L/3 can be shown to be the upper bound for general networks. Our simulations show that 802.11 networks with random topologies operated with AODV routing can only achieve throughputs far below the upper bounds. Fortunately, by properly selecting routes near the gateway (or by properly positioning the relay nodes leading to the gateway) to fashion after the structure of canonical networks, the throughput can be improved significantly by more than 150%. Indeed, in a dense network, it is worthwhile to deactivate some of the relay nodes near the sink judiciously.", ["IEEE 802.11", "Symmetry", "Ad hoc On-Demand Distance Vector Routing", "Institute of Electrical and Electronics Engineers", "Hops", "Throughput"]], ["On the Achievable Rate Regions for Interference Channels with Degraded Message Sets", "The interference channel with degraded message sets (IC-DMS) refers to a communication model in which two senders attempt to communicate with their respective receivers simultaneously through a common medium, and one of the senders has complete and a priori (non-causal) knowledge about the message being transmitted by the other. A coding scheme that collectively has advantages of cooperative coding, collaborative coding, and dirty paper coding, is developed for such a channel. With resorting to this coding scheme, achievable rate regions of the IC-DMS in both discrete memoryless and Gaussian cases are derived, which, in general, include several previously known rate regions. Numerical examples for the Gaussian case demonstrate that in the high-interference-gain regime, the derived achievable rate regions offer considerable improvements over these existing results.", ["Communication", "A priori and a posteriori", "Memorylessness", "Causality", "Dirty paper coding", "Integrated circuit"]], ["A Low Complexity Algorithm and Architecture for Systematic Encoding of Hermitian Codes", "We present an algorithm for systematic encoding of Hermitian codes. For a Hermitian code defined over GF(q^2), the proposed algorithm achieves a run time complexity of O(q^2) and is suitable for VLSI implementation. The encoder architecture uses as main blocks q varying-rate Reed-Solomon encoders and achieves a space complexity of O(q^2) in terms of finite field multipliers and memory elements.", ["Reed-Solomon", "Encoder", "Run time (program lifecycle phase)", "Flash memory", "Finite field", "Time complexity", "Algorithm", "Very-large-scale integration", "Complexity", "Code"]], ["Learning from compressed observations", "The problem of statistical learning is to construct a predictor of a random variable $Y$ as a function of a related random variable $X$ on the basis of an i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable predictors are drawn from some specified class, and the goal is to approach asymptotically the performance (expected loss) of the best predictor in the class. We consider the setting in which one has perfect observation of the $X$-part of the sample, while the $Y$-part has to be communicated at some finite bit rate. The encoding of the $Y$-values is allowed to depend on the $X$-values. Under suitable regularity conditions on the admissible predictors, the underlying family of probability distributions and the loss function, we give an information-theoretic characterization of achievable predictor performance in terms of conditional distortion-rate functions. The ideas are illustrated on the example of nonparametric regression in Gaussian noise.", ["Random variable", "Nonparametric regression", "Loss function", "Joint probability distribution", "Bit rate", "Function (mathematics)", "Normal distribution", "Gaussian noise", "Statistics", "Probability", "Probability distribution", "Conditional probability", "Independent and identically distributed random variables", "Information theory", "Machine learning", "Non-parametric statistics", "Admissible decision rule", "Regression analysis", "Sample (statistics)", "Randomness"]], ["Revisiting the Issues On Netflow Sample and Export Performance", "The high volume of packets and packet rates of traffic on some router links makes it exceedingly difficult for routers to examine every packet in order to keep detailed statistics about the traffic which is traversing the router. Sampling is commonly applied on routers in order to limit the load incurred by the collection of information that the router has to undertake when evaluating flow information for monitoring purposes. The sampling process in nearly all cases is a deterministic process of choosing 1 in every N packets on a per-interface basis, and then forming the flow statistics based on the collected sampled statistics. Even though this sampling may not be significant for some statistics, such as packet rate, others can be severely distorted. However, it is important to consider the sampling techniques and their relative accuracy when applied to different traffic patterns. The main disadvantage of sampling is the loss of accuracy in the collected trace when compared to the original traffic stream. To date there has not been a detailed analysis of the impact of sampling at a router in various traffic profiles and flow criteria. In this paper, we assess the performance of the sampling process as used in NetFlow in detail, and we discuss some techniques for the compensation of loss of monitoring detail.", ["NetFlow", "Statistics", "Netflow", "Router", "Sampling (statistics)", "Network packet"]], ["Optimal Synthesis of Multiple Algorithms", "In this paper we give a definition of \"algorithm,\" \"finite algorithm,\" \"equivalent algorithms,\" and what it means for a single algorithm to dominate a set of algorithms. We define a derived algorithm which may have a smaller mean execution time than any of its component algorithms. We give an explicit expression for the mean execution time (when it exists) of the derived algorithm. We give several illustrative examples of derived algorithms with two component algorithms. We include mean execution time solutions for two-algorithm processors whose joint density of execution times are of several general forms. For the case in which the joint density for a two-algorithm processor is a step function, we give a maximum-likelihood estimation scheme with which to analyze empirical processing time data.", ["Maximum likelihood", "Step function", "Central processing unit", "Mean", "Empirical"]], ["Hybrid-ARQ in Multihop Networks with Opportunistic Relay Selection", "This paper develops a contention-based opportunistic feedback technique towards relay selection in a dense wireless network. This technique enables the forwarding of additional parity information from the selected relay to the destination. For a given network, the effects of varying key parameters such as the feedback probability are presented and discussed. A primary advantage of the proposed technique is that relay selection can be performed in a distributed way. Simulation results find its performance to closely match that of centralized schemes that utilize GPS information, unlike the proposed method. The proposed relay selection method is also found to achieve throughput gains over a point-to-point transmission strategy.", ["Wireless network", "Simulation", "Feedback", "Throughput", "Global Positioning System", "Probability"]], ["Opportunistic Relay Selection with Limited Feedback", "It has been shown that a decentralized relay selection protocol based on opportunistic feedback from the relays yields good throughput performance in dense wireless networks. This selection strategy supports a hybrid-ARQ transmission approach where relays forward parity information to the destination in the event of a decoding error. Such an approach, however, suffers a loss compared to centralized strategies that select relays with the best channel gain to the destination. This paper closes the performance gap by adding another level of channel feedback to the decentralized relay selection problem. It is demonstrated that only one additional bit of feedback is necessary for good throughput performance. The performance impact of varying key parameters such as the number of relays and the channel feedback threshold is discussed. An accompanying bit error rate analysis demonstrates the importance of relay selection.", ["Bit error rate", "Communications protocol", "Wireless network", "Decentralization", "Bit", "Channel (communications)", "Throughput", "Transmission (telecommunications)", "Automatic repeat request", "Wireless"]], ["On packet lengths and overhead for random linear coding over the erasure channel", "We assess the practicality of random network coding by illuminating the issue of overhead and considering it in conjunction with increasingly long packets sent over the erasure channel. We show that the transmission of increasingly long packets, consisting of either of an increasing number of symbols per packet or an increasing symbol alphabet size, results in a data rate approaching zero over the erasure channel. This result is due to an erasure probability that increases with packet length. Numerical results for a particular modulation scheme demonstrate a data rate of approximately zero for a large, but finite-length packet. Our results suggest a reduction in the performance gains offered by random network coding.", ["Linear code", "Modulation", "Network packet", "Data", "Binary erasure channel", "Alphabet", "Forward error correction"]], ["P-adic arithmetic coding", "A new incremental algorithm for data compression is presented. For a sequence of input symbols algorithm incrementally constructs a p-adic integer number as an output. Decoding process starts with less significant part of a p-adic integer and incrementally reconstructs a sequence of input symbols. Algorithm is based on certain features of p-adic numbers and p-adic norm. p-adic coding algorithm may be considered as of generalization a popular compression technique - arithmetic coding algorithms. It is shown that for p = 2 the algorithm works as integer variant of arithmetic coding; for a special class of models it gives exactly the same codes as Huffman's algorithm, for another special model and a specific alphabet it gives Golomb-Rice codes.", ["P-adic number", "Arithmetic coding", "Golomb coding", "Integer", "Algorithm", "Data compression", "Arithmetic", "Alphabet"]], ["Universal Source Coding for Monotonic and Fast Decaying Monotonic Distributions", "We study universal compression of sequences generated by monotonic distributions. We show that for a monotonic distribution over an alphabet of size $k$, each probability parameter costs essentially $0.5 \\log (n/k^3)$ bits, where $n$ is the coded sequence length, as long as $k = o(n^{1/3})$. Otherwise, for $k = O(n)$, the total average sequence redundancy is $O(n^{1/3+\\epsilon})$ bits overall. We then show that there exists a sub-class of monotonic distributions over infinite alphabets for which redundancy of $O(n^{1/3+\\epsilon})$ bits overall is still achievable. This class contains fast decaying distributions, including many distributions over the integers and geometric distributions. For some slower decays, including other distributions over the integers, redundancy of $o(n)$ bits overall is achievable, where a method to compute specific redundancy rates for such distributions is derived. The results are specifically true for finite entropy monotonic distributions. Finally, we study individual sequence redundancy behavior assuming a sequence is governed by a monotonic distribution. We show that for sequences whose empirical distributions are monotonic, individual redundancy bounds similar to those in the average case can be obtained. However, even if the monotonicity in the empirical distribution is violated, diminishing per symbol individual sequence redundancies with respect to the monotonic maximum likelihood description length may still be achievable.", ["Maximum likelihood", "Infinity", "Geometry", "Integer", "Entropy", "Monotonic function", "Probability distribution", "Likelihood function", "Probability", "Logarithm", "Empirical", "Parameter", "Alphabet"]], ["Lessons Learned from the deployment of a high-interaction honeypot", "This paper presents an experimental study and the lessons learned from the observation of the attackers when logged on a compromised machine. The results are based on a six months period during which a controlled experiment has been run with a high interaction honeypot. We correlate our findings with those obtained with a worldwide distributed system of lowinteraction honeypots.", ["Honeypot (computing)", "Distributed computing", "Scientific control"]], ["Availability assessment of SunOS/Solaris Unix Systems based on Syslogd and wtmpx logfiles : a case study", "This paper presents a measurement-based availability assessment study using field data collected during a 4-year period from 373 SunOS/Solaris Unix workstations and servers interconnected through a local area network. We focus on the estimation of machine uptimes, downtimes and availability based on the identification of failures that caused total service loss. Data corresponds to syslogd event logs that contain a large amount of information about the normal activity of the studied systems as well as their behavior in the presence of failures. It is widely recognized that the information contained in such event logs might be incomplete or imperfect. The solution investigated in this paper to address this problem is based on the use of auxiliary sources of data obtained from wtmpx files maintained by the SunOS/Solaris Unix operating system. The results obtained suggest that the combined use of wtmpx and syslogd log files provides more complete information on the state of the target systems that is useful to provide availability estimations that better reflect reality.", ["Local area network", "Solaris (operating system)", "Operating system", "Syslog", "SunOS", "Data", "Unix"]], ["Empirical analysis and statistical modeling of attack processes based on honeypots", "Honeypots are more and more used to collect data on malicious activities on the Internet and to better understand the strategies and techniques used by attackers to compromise target systems. Analysis and modeling methodologies are needed to support the characterization of attack processes based on the data collected from the honeypots. This paper presents some empirical analyses based on the data collected from the Leurr{\\'e}.com honeypot platforms deployed on the Internet and presents some preliminary modeling studies aimed at fulfilling such objectives.", ["Empirical", "Statistical model", "Empiricism", "Statistics", "Honeypot (computing)", "Scientific modelling", "Data"]], ["An architecture-based dependability modeling framework using AADL", "For efficiency reasons, the software system designers' will is to use an integrated set of methods and tools to describe specifications and designs, and also to perform analyses such as dependability, schedulability and performance. AADL (Architecture Analysis and Design Language) has proved to be efficient for software architecture modeling. In addition, AADL was designed to accommodate several types of analyses. This paper presents an iterative dependency-driven approach for dependability modeling using AADL. It is illustrated on a small example. This approach is part of a complete framework that allows the generation of dependability analysis and evaluation models from AADL models to support the analysis of software and system architectures, in critical application domains.", ["Software system", "Software architecture", "Scheduling (computing)", "Architecture", "Model-driven architecture", "Iteration", "Scientific modelling"]], ["A Hierarchical Approach for Dependability Analysis of a Commercial Cache-Based RAID Storage Architecture", "We present a hierarchical simulation approach for the dependability analysis and evaluation of a highly available commercial cache-based RAID storage system. The archi-tecture is complex and includes several layers of overlap-ping error detection and recovery mechanisms. Three ab-straction levels have been developed to model the cache architecture, cache operations, and error detection and recovery mechanism. The impact of faults and errors oc-curring in the cache and in the disks is analyzed at each level of the hierarchy. A simulation submodel is associated with each abstraction level. The models have been devel-oped using DEPEND, a simulation-based environment for system-level dependability analysis, which provides facili-ties to inject faults into a functional behavior model, to simulate error detection and recovery mechanisms, and to evaluate quantitative measures. Several fault models are defined for each submodel to simulate cache component failures, disk failures, transmission errors, and data errors in the cache memory and in the disks. Some of the parame-ters characterizing fault injection in a given submodel cor-respond to probabilities evaluated from the simulation of the lower-level submodel. Based on the proposed method-ology, we evaluate and analyze 1) the system behavior un-der a real workload and high error rate (focusing on error bursts), 2) the coverage of the error detection mechanisms implemented in the system and the error latency distribu-tions, and 3) the accumulation of errors in the cache and in the disks.", ["Data", "RAID", "High-availability cluster", "Abstraction layer", "Hierarchy", "Disk storage", "Computer data storage", "Error detection and correction", "Simulation", "Ping", "Cache", "Burst error", "CPU cache", "Dependability"]], ["Sensor Networks with Random Links: Topology Design for Distributed Consensus", "In a sensor network, in practice, the communication among sensors is subject to:(1) errors or failures at random times; (3) costs; and(2) constraints since sensors and networks operate under scarce resources, such as power, data rate, or communication. The signal-to-noise ratio (SNR) is usually a main factor in determining the probability of error (or of communication failure) in a link. These probabilities are then a proxy for the SNR under which the links operate. The paper studies the problem of designing the topology, i.e., assigning the probabilities of reliable communication among sensors (or of link failures) to maximize the rate of convergence of average consensus, when the link communication costs are taken into account, and there is an overall communication budget constraint. To consider this problem, we address a number of preliminary issues: (1) model the network as a random topology; (2) establish necessary and sufficient conditions for mean square sense (mss) and almost sure (a.s.) convergence of average consensus when network links fail; and, in particular, (3) show that a necessary and sufficient condition for both mss and a.s. convergence is for the algebraic connectivity of the mean graph describing the network topology to be strictly positive. With these results, we formulate topology design, subject to random link failures and to a communication cost constraint, as a constrained convex optimization problem to which we apply semidefinite programming techniques. We show by an extensive numerical study that the optimal design improves significantly the convergence speed of the consensus algorithm and can achieve the asymptotic performance of a non-random network at a fraction of the communication cost.", ["Semidefinite programming", "Optimization problem", "Signal-to-noise ratio", "Almost surely", "Random graph", "Necessary and sufficient condition", "Convex optimization", "Probability", "Big O notation", "Rate of convergence", "Mathematical optimization", "Algorithm", "Topology", "Sensor", "Optimal design", "Communication", "Network topology", "Mean", "Graph (mathematics)", "Randomness", "Budget constraint"]], ["Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian Vector Broadcast Channels", "MIMO technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. In a MIMO-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a Gaussian vector broadcast channel. Recently, researchers showed that ``dirty paper coding'' (DPC) is the optimal transmission strategy for Gaussian vector broadcast channels. So far, there has been little study on how this fundamental result will impact the cross-layer design for MIMO-based mesh networks. To fill this gap, we consider the problem of jointly optimizing DPC power allocation in the link layer at each node and multihop/multipath routing in a MIMO-based mesh networks. It turns out that this optimization problem is a very challenging non-convex problem. To address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. For the transformed problem, we develop an efficient solution procedure that integrates Lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. In our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using DPC.", ["Subgradient method", "Cutting-plane method", "Lagrange multiplier", "Conjugate gradient method", "Matrix (mathematics)", "Mathematical optimization", "Optimization problem", "Calculus", "Differential calculus", "Gradient", "Channel capacity", "Dirty paper coding", "Lagrangian", "MIMO", "Routing", "Communication", "Mesh networking"]], ["Architecture for Pseudo Acausal Evolvable Embedded Systems", "Advances in semiconductor technology are contributing to the increasing complexity in the design of embedded systems. Architectures with novel techniques such as evolvable nature and autonomous behavior have engrossed lot of attention. This paper demonstrates conceptually evolvable embedded systems can be characterized basing on acausal nature. It is noted that in acausal systems, future input needs to be known, here we make a mechanism such that the system predicts the future inputs and exhibits pseudo acausal nature. An embedded system that uses theoretical framework of acausality is proposed. Our method aims at a novel architecture that features the hardware evolability and autonomous behavior alongside pseudo acausality. Various aspects of this architecture are discussed in detail along with the limitations.", ["Embedded system", "Technology", "Semiconductor", "Hardware", "Architecture"]], ["The on-line shortest path problem under partial monitoring", "The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (defined as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with linear complexity in the number of rounds n and in the number of edges. An extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented.", ["Directed acyclic graph", "Graph (mathematics)", "Algorithm", "Simulation", "Shortest path problem", "Multi-armed bandit", "Packet switching", "Tree (graph theory)"]], ["A neural network approach to ordinal regression", "Ordinal regression is an important type of learning, which has properties of both classification and regression. Here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories. Our approach is a generalization of the perceptron method for ordinal regression. On several benchmark datasets, our method (NNRank) outperforms a neural network classification method. Compared with the ordinal regression methods using Gaussian processes and support vector machines, NNRank achieves comparable performance. Moreover, NNRank has the advantages of traditional neural networks: learning in both online and batch modes, handling very large training datasets, and making rapid predictions. These features make NNRank a useful and complementary tool for large-scale data processing tasks such as information retrieval, web page ranking, collaborative filtering, and protein ranking in Bioinformatics.", ["Perceptron", "Support vector machine", "Information retrieval", "Bioinformatics", "Protein", "Web page", "Neural network", "Linnaean taxonomy", "Computer data processing", "Gaussian process", "Collaborative filtering", "Regression analysis"]], ["On the Kolmogorov-Chaitin Complexity for short sequences", "A drawback of Kolmogorov-Chaitin complexity (K) as a function from s to the shortest program producing s is its noncomputability which limits its range of applicability. Moreover, when strings are short, the dependence of K on a particular universal Turing machine U can be arbitrary. In practice one can approximate it by computable compression methods. However, such compression methods do not always provide meaningful approximations--for strings shorter, for example, than typical compiler lengths. In this paper we suggest an empirical approach to overcome this difficulty and to obtain a stable definition of the Kolmogorov-Chaitin complexity for short sequences. Additionally, a correlation in terms of distribution frequencies was found across the output of two models of abstract machines, namely unidimensional cellular automata and deterministic Turing machine.", ["Universal Turing machine", "Turing machine", "Cellular automaton", "Dimension", "Alan Turing", "Frequency", "Complexity", "Gregory Chaitin", "Compiler", "Function (mathematics)", "String (computer science)", "Determinism", "Correlation and dependence"]], ["Fast paths in large-scale dynamic road networks", "Efficiently computing fast paths in large scale dynamic road networks (where dynamic traffic information is known over a part of the network) is a practical problem faced by several traffic information service providers who wish to offer a realistic fast path computation to GPS terminal enabled vehicles. The heuristic solution method we propose is based on a highway hierarchy-based shortest path algorithm for static large-scale networks; we maintain a static highway hierarchy and perform each query on the dynamically evaluated network.", ["Computing", "Computation", "Hierarchy", "Heuristic", "Global Positioning System", "Shortest path problem", "Network theory", "Highway"]], ["Differential Diversity Reception of MDPSK over Independent Rayleigh Channels with Nonidentical Branch Statistics and Asymmetric Fading Spectrum", "This paper is concerned with optimum diversity receiver structure and its performance analysis of differential phase shift keying (DPSK) with differential detection over nonselective, independent, nonidentically distributed, Rayleigh fading channels. The fading process in each branch is assumed to have an arbitrary Doppler spectrum with arbitrary Doppler bandwidth, but to have distinct, asymmetric fading power spectral density characteristic. Using 8-DPSK as an example, the average bit error probability (BEP) of the optimum diversity receiver is obtained by calculating the BEP for each of the three individual bits. The BEP results derived are given in exact, explicit, closed-form expressions which show clearly the behavior of the performance as a function of various system parameters.", ["Rayleigh fading", "Spectral density", "Bit error rate", "Phase (waves)", "Fading", "Phase-shift keying", "Probability", "Diversity scheme", "Doppler effect"]], ["Novelty and Collective Attention", "The subject of collective attention is central to an information age where millions of people are inundated with daily messages. It is thus of interest to understand how attention to novel items propagates and eventually fades among large populations. We have analyzed the dynamics of collective attention among one million users of an interactive website -- \\texttt{digg.com} -- devoted to thousands of novel news stories. The observations can be described by a dynamical model characterized by a single novelty factor. Our measurements indicate that novelty within groups decays with a stretched-exponential law, suggesting the existence of a natural time scale over which attention fades.", ["Information Age"]], ["Novel algorithm to calculate hypervolume indicator of Pareto approximation set", "Hypervolume indicator is a commonly accepted quality measure for comparing Pareto approximation set generated by multi-objective optimizers. The best known algorithm to calculate it for $n$ points in $d$-dimensional space has a run time of $O(n^{d/2})$ with special data structures. This paper presents a recursive, vertex-splitting algorithm for calculating the hypervolume indicator of a set of $n$ non-comparable points in $d>2$ dimensions. It splits out multiple child hyper-cuboids which can not be dominated by a splitting reference point. In special, the splitting reference point is carefully chosen to minimize the number of points in the child hyper-cuboids. The complexity analysis shows that the proposed algorithm achieves $O((\\frac{d}{2})^n)$ time and $O(dn^2)$ space complexity in the worst case.", ["Run time (program lifecycle phase)", "Data structure", "Algorithm", "Four-dimensional space", "Analysis of algorithms"]], ["A Doubly Distributed Genetic Algorithm for Network Coding", "We present a genetic algorithm which is distributed in two novel ways: along genotype and temporal axes. Our algorithm first distributes, for every member of the population, a subset of the genotype to each network node, rather than a subset of the population to each. This genotype distribution is shown to offer a significant gain in running time. Then, for efficient use of the computational resources in the network, our algorithm divides the candidate solutions into pipelined sets and thus the distribution is in the temporal domain, rather that in the spatial domain. This temporal distribution may lead to temporal inconsistency in selection and replacement, however our experiments yield better efficiency in terms of the time to convergence without incurring significant penalties.", ["Genetic algorithm", "Node (networking)", "Subset", "Genotype", "Population", "Algorithm"]], ["Text Line Segmentation of Historical Documents: a Survey", "There is a huge amount of historical documents in libraries and in various National Archives that have not been exploited electronically. Although automatic reading of complete pages remains, in most cases, a long-term objective, tasks such as word spotting, text/image alignment, authentication and extraction of specific fields are in use today. For all these tasks, a major step is document segmentation into text lines. Because of the low quality and the complexity of these documents (background noise, artifacts due to aging, interfering lines),automatic text line segmentation remains an open research field. The objective of this paper is to present a survey of existing methods, developed during the last decade, and dedicated to documents of historical interest.", ["Library", "Authentication", "Artifact (archaeology)"]], ["Phase Transitions in the Coloring of Random Graphs", "We consider the problem of coloring the vertices of a large sparse random graph with a given number of colors so that no adjacent vertices have the same color. Using the cavity method, we present a detailed and systematic analytical study of the space of proper colorings (solutions). We show that for a fixed number of colors and as the average vertex degree (number of constraints) increases, the set of solutions undergoes several phase transitions similar to those observed in the mean field theory of glasses. First, at the clustering transition, the entropically dominant part of the phase space decomposes into an exponential number of pure states so that beyond this transition a uniform sampling of solutions becomes hard. Afterward, the space of solutions condenses over a finite number of the largest states and consequently the total entropy of solutions becomes smaller than the annealed one. Another transition takes place when in all the entropically dominant states a finite fraction of nodes freezes so that each of these nodes is allowed a single color in all the solutions inside the state. Eventually, above the coloring threshold, no more solutions are available. We compute all the critical connectivities for Erdos-Renyi and regular random graphs and determine their asymptotic values for large number of colors. Finally, we discuss the algorithmic consequences of our findings. We argue that the onset of computational hardness is not associated with the clustering transition and we suggest instead that the freezing transition might be the relevant phenomenon. We also discuss the performance of a simple local Walk-COL algorithm and of the belief propagation algorithm in the light of our results.", ["Random graph", "Phase space", "Entropy", "Degree (graph theory)", "Phase transition", "Mean field theory", "Density matrix", "Belief propagation", "Annealing (metallurgy)", "Algorithm", "Finite set", "Graph (mathematics)"]], ["Parametric Learning and Monte Carlo Optimization", "This paper uncovers and explores the close relationship between Monte Carlo Optimization of a parametrized integral (MCO), Parametric machine-Learning (PL), and `blackbox' or `oracle'-based optimization (BO). We make four contributions. First, we prove that MCO is mathematically identical to a broad class of PL problems. This identity potentially provides a new application domain for all broadly applicable PL techniques: MCO. Second, we introduce immediate sampling, a new version of the Probability Collectives (PC) algorithm for blackbox optimization. Immediate sampling transforms the original BO problem into an MCO problem. Accordingly, by combining these first two contributions, we can apply all PL techniques to BO. In our third contribution we validate this way of improving BO by demonstrating that cross-validation and bagging improve immediate sampling. Finally, conventional MC and MCO procedures ignore the relationship between the sample point locations and the associated values of the integrand; only the values of the integrand at those locations are considered. We demonstrate that one can exploit the sample location information using PL techniques, for example by forming a fit of the sample locations to the associated values of the integrand. This provides an additional way to apply PL techniques to improve MCO.", ["Cross-validation (statistics)", "Personal computer", "Integral", "Algorithm", "Problem domain", "Mathematical optimization", "Monte Carlo", "Oracle"]], ["A Disciplined Approach to Adopting Agile Practices: The Agile Adoption Framework", "Many organizations aspire to adopt agile processes to take advantage of the numerous benefits that it offers to an organization. Those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer satisfaction. To date however, there is no structured process (at least in the public domain) that guides organizations in adopting agile practices. To address this problem we present the Agile Adoption Framework. The framework consists of two components: an agile measurement index, and a 4-Stage process, that together guide and assist the agile adoption efforts of organizations. More specifically, the agile measurement index is used to identify the agile potential of projects and organizations. The 4-Stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and (b) guided by their potential, what set of agile practices can and should be introduced.", ["Customer satisfaction", "Rate of return", "Software quality", "Public domain"]], ["Antenna Combining for the MIMO Downlink Channel", "A multiple antenna downlink channel where limited channel feedback is available to the transmitter is considered. In a vector downlink channel (single antenna at each receiver), the transmit antenna array can be used to transmit separate data streams to multiple receivers only if the transmitter has very accurate channel knowledge, i.e., if there is high-rate channel feedback from each receiver. In this work it is shown that channel feedback requirements can be significantly reduced if each receiver has a small number of antennas and appropriately combines its antenna outputs. A combining method that minimizes channel quantization error at each receiver, and thereby minimizes multi-user interference, is proposed and analyzed. This technique is shown to outperform traditional techniques such as maximum-ratio combining because minimization of interference power is more critical than maximization of signal power in the multiple antenna downlink. Analysis is provided to quantify the feedback savings, and the technique is seen to work well with user selection and is also robust to receiver estimation error.", ["Quantization error", "Downlink", "MIMO", "Channel state information", "Receiver (radio)", "Antenna (radio)", "Signal (electronics)", "Euclidean vector"]], ["Low Density Lattice Codes", "Low density lattice codes (LDLC) are novel lattice codes that can be decoded efficiently and approach the capacity of the additive white Gaussian noise (AWGN) channel. In LDLC a codeword x is generated directly at the n-dimensional Euclidean space as a linear transformation of a corresponding integer message vector b, i.e., x = Gb, where H, the inverse of G, is restricted to be sparse. The fact that H is sparse is utilized to develop a linear-time iterative decoding scheme which attains, as demonstrated by simulations, good error performance within ~0.5dB from capacity at block length of n = 100,000 symbols. The paper also discusses convergence results and implementation considerations.", ["Euclidean space", "Additive white Gaussian noise", "Linear map", "Integer", "Euclidean vector", "Density", "Iteration", "Gaussian noise", "Decibel", "Time complexity", "Normal distribution"]], ["Supporting Knowledge and Expertise Finding within Australia's Defence Science and Technology Organisation", "This paper reports on work aimed at supporting knowledge and expertise finding within a large Research and Development (R&D) organisation. The paper first discusses the nature of knowledge important to R&D organisations and presents a prototype information system developed to support knowledge and expertise finding. The paper then discusses a trial of the system within an R&D organisation, the implications and limitations of the trial, and discusses future research questions.", ["Defence Science and Technology Organisation", "Information system", "Australia", "Research and development", "Prototype"]], ["Distance preserving mappings from ternary vectors to permutations", "Distance-preserving mappings (DPMs) are mappings from the set of all q-ary vectors of a fixed length to the set of permutations of the same or longer length such that every two distinct vectors are mapped to permutations with the same or even larger Hamming distance than that of the vectors. In this paper, we propose a construction of DPMs from ternary vectors. The constructed DPMs improve the lower bounds on the maximal size of permutation arrays.", ["Hamming distance", "Permutation"]], ["A Language-Based Approach for Improving the Robustness of Network Application Protocol Implementations", "The secure and robust functioning of a network relies on the defect-free implementation of network applications. As network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. In this paper, we present a domain-specific language, Zebu, for describing protocol message formats and related processing constraints. From a Zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. Zebu is easy to use, as it builds on notations used in RFCs to describe protocol grammars. Zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. Finally, Zebu-based applications are robust, as the Zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. Using a mutation analysis in the context of SIP and RTSP, we show that Zebu significantly improves application robustness.", ["Domain-specific language", "Compiler", "Communications protocol", "Request for Comments", "Real Time Streaming Protocol", "Parsing", "Session Initiation Protocol", "Computer network", "Specification (technical standard)"]], ["Calculating Valid Domains for BDD-Based Interactive Configuration", "In these notes we formally describe the functionality of Calculating Valid Domains from the BDD representing the solution space of valid configurations. The formalization is largely based on the CLab configuration framework.", ["bdd"]], ["Preconditioned Temporal Difference Learning", "This paper has been withdrawn by the author. This draft is withdrawn for its poor quality in english, unfortunately produced by the author when he was just starting his science route. Look at the ICML version instead: http://icml2008.cs.helsinki.fi/papers/111.pdf", ["Science", "Author"]], ["Trellis-Coded Quantization Based on Maximum-Hamming-Distance Binary Codes", "Most design approaches for trellis-coded quantization take advantage of the duality of trellis-coded quantization with trellis-coded modulation, and use the same empirically-found convolutional codes to label the trellis branches. This letter presents an alternative approach that instead takes advantage of maximum-Hamming-distance convolutional codes. The proposed source codes are shown to be competitive with the best in the literature for the same computational complexity.", ["Computational complexity theory", "Literature", "Quantum mechanics", "Convolutional code"]], ["A Better Good-Turing Estimator for Sequence Probabilities", "We consider the problem of estimating the probability of an observed string drawn i.i.d. from an unknown distribution. The key feature of our study is that the length of the observed string is assumed to be of the same order as the size of the underlying alphabet. In this setting, many letters are unseen and the empirical distribution tends to overestimate the probability of the observed letters. To overcome this problem, the traditional approach to probability estimation is to use the classical Good-Turing estimator. We introduce a natural scaling model and use it to show that the Good-Turing sequence probability estimator is not consistent. We then introduce a novel sequence probability estimator that is indeed consistent under the natural scaling model.", ["Empirical", "Alphabet", "Alan Turing"]], ["GLRT-Optimal Noncoherent Lattice Decoding", "This paper presents new low-complexity lattice-decoding algorithms for noncoherent block detection of QAM and PAM signals over complex-valued fading channels. The algorithms are optimal in terms of the generalized likelihood ratio test (GLRT). The computational complexity is polynomial in the block length; making GLRT-optimal noncoherent detection feasible for implementation. We also provide even lower complexity suboptimal algorithms. Simulations show that the suboptimal algorithms have performance indistinguishable from the optimal algorithms. Finally, we consider block based transmission, and propose to use noncoherent detection as an alternative to pilot assisted transmission (PAT). The new technique is shown to outperform PAT.", ["Likelihood-ratio test", "Computational complexity theory", "Complex number", "Quadrature amplitude modulation", "Polynomial"]], ["On restrictions of balanced 2-interval graphs", "The class of 2-interval graphs has been introduced for modelling scheduling and allocation problems, and more recently for specific bioinformatic problems. Some of those applications imply restrictions on the 2-interval graphs, and justify the introduction of a hierarchy of subclasses of 2-interval graphs that generalize line graphs: balanced 2-interval graphs, unit 2-interval graphs, and (x,x)-interval graphs. We provide instances that show that all the inclusions are strict. We extend the NP-completeness proof of recognizing 2-interval graphs to the recognition of balanced 2-interval graphs. Finally we give hints on the complexity of unit 2-interval graphs recognition, by studying relationships with other graph classes: proper circular-arc, quasi-line graphs, K_{1,5}-free graphs, ...", ["Bioinformatics", "NP-complete", "Graph (mathematics)", "Hierarchy"]], ["Exploiting Social Annotation for Automatic Resource Discovery", "Information integration applications, such as mediators or mashups, that require access to information resources currently rely on users manually discovering and integrating them in the application. Manual resource discovery is a slow process, requiring the user to sift through results obtained via keyword-based search. Although search methods have advanced to include evidence from document contents, its metadata and the contents and link structure of the referring pages, they still do not adequately cover information sources -- often called ``the hidden Web''-- that dynamically generate documents in response to a query. The recently popular social bookmarking sites, which allow users to annotate and share metadata about various information sources, provide rich evidence for resource discovery. In this paper, we describe a probabilistic model of the user annotation process in a social bookmarking system del.icio.us. We then use the model to automatically find resources relevant to a particular information domain. Our experimental results on data obtained from \\emph{del.icio.us} show this approach as a promising method for helping automate the resource discovery task.", ["Social bookmarking", "Delicious (website)", "Annotation", "Statistical model", "Information integration", "Mashup (web application hybrid)", "Metadata"]], ["Personalizing Image Search Results on Flickr", "The social media site Flickr allows users to upload their photos, annotate them with tags, submit them to groups, and also to form social networks by adding other users as contacts. Flickr offers multiple ways of browsing or searching it. One option is tag search, which returns all images tagged with a specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an insect or a car, tag search results will include many images that are not relevant to the sense the user had in mind when executing the query. We claim that users express their photography interests through the metadata they add in the form of contacts and image annotations. We show how to exploit this metadata to personalize search results for the user, thereby improving search performance. First, we show that we can significantly improve search precision by filtering tag search results by user's contacts or a larger social network that includes those contact's contacts. Secondly, we describe a probabilistic model that takes advantage of tag information to discover latent topics contained in the search results. The users' interests can similarly be described by the tags they used for annotating their images. The latent topics found by the model are then used to personalize search results by finding images on topics that are of interest to the user.", ["Social media", "Flickr", "Insect", "Photography", "Statistical model", "Beetle", "Social network", "Metadata", "Index term", "Uploading and downloading"]], ["Settling the Complexity of Computing Two-Player Nash Equilibria", "We settle a long-standing open question in algorithmic game theory. We prove that Bimatrix, the problem of finding a Nash equilibrium in a two-player game, is complete for the complexity class PPAD Polynomial Parity Argument, Directed version) introduced by Papadimitriou in 1991. This is the first of a series of results concerning the complexity of Nash equilibria. In particular, we prove the following theorems: Bimatrix does not have a fully polynomial-time approximation scheme unless every problem in PPAD is solvable in polynomial time. The smoothed complexity of the classic Lemke-Howson algorithm and, in fact, of any algorithm for Bimatrix is not polynomial unless every problem in PPAD is solvable in randomized polynomial time. Our results demonstrate that, even in the simplest form of non-cooperative games, equilibrium computation and approximation are polynomial-time equivalent to fixed point computation. Our results also have two broad complexity implications in mathematical economics and operations research: Arrow-Debreu market equilibria are PPAD-hard to compute. The P-Matrix Linear Complementary Problem is computationally harder than convex programming unless every problem in PPAD is solvable in polynomial time.", ["Operations research", "Nash equilibrium", "Mathematical economics", "Convex optimization", "RP (complexity)", "Economics", "Economic equilibrium", "Complexity class", "Polynomial time", "Non-cooperative game", "Polynomial-time reduction", "Polynomial", "Game theory", "Complexity", "Algorithm", "Polynomial-time approximation scheme", "Computation", "Arrow-Debreu model", "Fixed point (mathematics)", "Randomized algorithm"]], ["Locally Decodable Codes From Nice Subsets of Finite Fields and Prime Factors of Mersenne Numbers", "A k-query Locally Decodable Code (LDC) encodes an n-bit message x as an N-bit codeword C(x), such that one can probabilistically recover any bit x_i of the message by querying only k bits of the codeword C(x), even after some constant fraction of codeword bits has been corrupted. The major goal of LDC related research is to establish the optimal trade-off between length and query complexity of such codes. Recently [Y] introduced a novel technique for constructing locally decodable codes and vastly improved the upper bounds for code length. The technique is based on Mersenne primes. In this paper we extend the work of [Y] and argue that further progress via these methods is tied to progress on an old number theory question regarding the size of the largest prime factors of Mersenne numbers. Specifically, we show that every Mersenne number m=2^t-1 that has a prime factor p>m^\\gamma yields a family of k(\\gamma)-query locally decodable codes of length Exp(n^{1/t}). Conversely, if for some fixed k and all \\epsilon > 0 one can use the technique of [Y] to obtain a family of k-query LDCs of length Exp(n^\\epsilon); then infinitely many Mersenne numbers have prime factors arger than known currently.", ["Mersenne prime", "Number theory", "Subset", "Prime factor", "Prime number", "Marin Mersenne", "Bit", "Nice", "Probability", "Decision tree model"]], ["A Cut-free Sequent Calculus for Bi-Intuitionistic Logic: Extended Version", "Bi-intuitionistic logic is the extension of intuitionistic logic with a connective dual to implication. Bi-intuitionistic logic was introduced by Rauszer as a Hilbert calculus with algebraic and Kripke semantics. But her subsequent ``cut-free'' sequent calculus for BiInt has recently been shown by Uustalu to fail cut-elimination. We present a new cut-free sequent calculus for BiInt, and prove it sound and complete with respect to its Kripke semantics. Ensuring completeness is complicated by the interaction between implication and its dual, similarly to future and past modalities in tense logic. Our calculus handles this interaction using extended sequents which pass information from premises to conclusions using variables instantiated at the leaves of failed derivation trees. Our simple termination argument allows our calculus to be used for automated deduction, although this is not its main purpose.", ["Automated theorem proving", "Kripke semantics", "Intuitionistic logic", "Sequent calculus", "Temporal logic", "Calculus", "Cut-elimination theorem", "David Hilbert", "Completeness", "Logic", "Saul Kripke", "Logical connective", "Modal logic", "Deductive reasoning", "Semantics", "Argument"]], ["Traitement Des Donnees Manquantes Au Moyen De L'Algorithme De Kohonen", "Nous montrons comment il est possible d'utiliser l'algorithme d'auto organisation de Kohonen pour traiter des donn\\'ees avec valeurs manquantes et estimer ces derni\\`eres. Apr\\`es un rappel m\\'ethodologique, nous illustrons notre propos \\`a partir de trois applications \\`a des donn\\'ees r\\'eelles. ----- We show how it is possible to use the Kohonen self-organizing algorithm to deal with data which contain missing values and to estimate them. After a methodological recall, we illustrate our purpose from three real databases applications.", ["Missing data", "Self-organization", "Algorithm", "Database", "Methodology"]], ["Self-Organization applied to Dynamic Network Layout", "As networks and their structure have become a major field of research, a strong demand for network visualization has emerged. We address this challenge by formalizing the well established spring layout in terms of dynamic equations. We thus open up the design space for new algorithms. Drawing from the knowledge of systems design, we derive a layout algorithm that remedies several drawbacks of the original spring layout. This new algorithm relies on the balancing of two antagonistic forces. We thus call it {\\em arf} for \"attractive and repulsive forces\". It is, as we claim, particularly suited for a dynamic layout of smaller networks ($n < 10^3$). We back this claim with several application examples from on going complex systems research.", ["Systems theory", "Complex systems", "Force-based algorithms (graph drawing)", "Systems design", "Knowledge"]], ["Information Theoretic Proofs of Entropy Power Inequalities", "While most useful information theoretic inequalities can be deduced from the basic properties of entropy or mutual information, up to now Shannon's entropy power inequality (EPI) is an exception: Existing information theoretic proofs of the EPI hinge on representations of differential entropy using either Fisher information or minimum mean-square error (MMSE), which are derived from de Bruijn's identity. In this paper, we first present an unified view of these proofs, showing that they share two essential ingredients: 1) a data processing argument applied to a covariance-preserving linear transformation; 2) an integration over a path of a continuous Gaussian perturbation. Using these ingredients, we develop a new and brief proof of the EPI through a mutual information inequality, which replaces Stam and Blachman's Fisher information inequality (FII) and an inequality for MMSE by Guo, Shamai and Verd\\'u used in earlier proofs. The result has the advantage of being very simple in that it relies only on the basic properties of mutual information. These ideas are then generalized to various extended versions of the EPI: Zamir and Feder's generalized EPI for linear transformations of the random variables, Takano and Johnson's EPI for dependent variables, Liu and Viswanath's covariance-constrained EPI, and Costa's concavity inequality for the entropy power.", ["Differential entropy", "Fisher information", "Minimum mean square error", "Linear map", "Mutual information", "Mean", "Normal distribution", "Dependent and independent variables", "Continuous function", "Covariance", "Computer data processing", "Random variable", "Entropy", "Information theory", "Linear", "Inequality (mathematics)"]], ["The Invar Tensor Package", "The Invar package is introduced, a fast manipulator of generic scalar polynomial expressions formed from the Riemann tensor of a four-dimensional metric-compatible connection. The package can maximally simplify any polynomial containing tensor products of up to seven Riemann tensors within seconds. It has been implemented both in Mathematica and Maple algebraic systems.", ["Riemann curvature tensor", "Bernhard Riemann", "Scalar (mathematics)", "Polynomial", "Abstract algebra", "Tensor product", "Algebraic number", "Metric (mathematics)", "Tensor", "Mathematica"]], ["Assessment and Propagation of Input Uncertainty in Tree-based Option Pricing Models", "This paper aims to provide a practical example on the assessment and propagation of input uncertainty for option pricing when using tree-based methods. Input uncertainty is propagated into output uncertainty, reflecting that option prices are as unknown as the inputs they are based on. Option pricing formulas are tools whose validity is conditional not only on how close the model represents reality, but also on the quality of the inputs they use, and those inputs are usually not observable. We provide three alternative frameworks to calibrate option pricing tree models, propagating parameter uncertainty into the resulting option prices. We finally compare our methods with classical calibration-based results assuming that there is no options market established. These methods can be applied to pricing of instruments for which there is not an options market, as well as a methodological tool to account for parameter and model uncertainty in theoretical option pricing.", ["Binomial options pricing model", "Calibration", "Observable", "Valuation of options", "Methodology", "Classical mechanics"]], ["Unicast and Multicast Qos Routing with Soft Constraint Logic Programming", "We present a formal model to represent and solve the unicast/multicast routing problem in networks with Quality of Service (QoS) requirements. To attain this, first we translate the network adapting it to a weighted graph (unicast) or and-or graph (multicast), where the weight on a connector corresponds to the multidimensional cost of sending a packet on the related network link: each component of the weights vector represents a different QoS metric value (e.g. bandwidth, cost, delay, packet loss). The second step consists in writing this graph as a program in Soft Constraint Logic Programming (SCLP): the engine of this framework is then able to find the best paths/trees by optimizing their costs and solving the constraints imposed on them (e.g. delay < 40msec), thus finding a solution to QoS routing problems. Moreover, c-semiring structures are a convenient tool to model QoS metrics. At last, we provide an implementation of the framework over scale-free networks and we suggest how the performance can be improved.", ["Semiring", "Quality of service", "Weighted graph", "Scale-free network", "Logic", "Millisecond", "Unicast", "Multicast", "Logic programming", "Packet loss"]], ["Low-density graph codes that are optimal for source/channel coding and binning", "We describe and analyze the joint source/channel coding properties of a class of sparse graphical codes based on compounding a low-density generator matrix (LDGM) code with a low-density parity check (LDPC) code. Our first pair of theorems establish that there exist codes from this ensemble, with all degrees remaining bounded independently of block length, that are simultaneously optimal as both source and channel codes when encoding and decoding are performed optimally. More precisely, in the context of lossy compression, we prove that finite degree constructions can achieve any pair $(R, D)$ on the rate-distortion curve of the binary symmetric source. In the context of channel coding, we prove that finite degree codes can achieve any pair $(C, p)$ on the capacity-noise curve of the binary symmetric channel. Next, we show that our compound construction has a nested structure that can be exploited to achieve the Wyner-Ziv bound for source coding with side information (SCSI), as well as the Gelfand-Pinsker bound for channel coding with side information (CCSI). Although the current results are based on optimal encoding and decoding, the proposed graphical codes have sparse structure and high girth that renders them well-suited to message-passing and other efficient decoding procedures.", ["Binary symmetric channel", "SCSI", "Lossy compression", "Matrix (mathematics)", "Forward error correction", "Generator matrix", "Data compression", "Low-density parity-check code"]], ["Transaction-Oriented Simulation In Ad Hoc Grids", "This paper analyses the possibilities of performing parallel transaction-oriented simulations with a special focus on the space-parallel approach and discrete event simulation synchronisation algorithms that are suitable for transaction-oriented simulation and the target environment of Ad Hoc Grids. To demonstrate the findings a Java-based parallel transaction-oriented simulator for the simulation language GPSS/H is implemented on the basis of the promising Shock Resistant Time Warp synchronisation algorithm and using the Grid framework ProActive. The validation of this parallel simulator shows that the Shock Resistant Time Warp algorithm can successfully reduce the number of rolled back Transaction moves but it also reveals circumstances in which the Shock Resistant Time Warp algorithm can be outperformed by the normal Time Warp algorithm. The conclusion of this paper suggests possible improvements to the Shock Resistant Time Warp algorithm to avoid such problems.", ["Java (programming language)", "Discrete event simulation", "Transaction processing", "Synchronization", "Grid computing", "Simulation language", "Algorithm", "Simulation"]], ["On-line Chain Partitions of Up-growing Semi-orders", "On-line chain partition is a two-player game between Spoiler and Algorithm. Spoiler presents a partially ordered set, point by point. Algorithm assigns incoming points (immediately and irrevocably) to the chains which constitute a chain partition of the order. The value of the game for orders of width $w$ is a minimum number $\\fVal(w)$ such that Algorithm has a strategy using at most $\\fVal(w)$ chains on orders of width at most $w$. We analyze the chain partition game for up-growing semi-orders. Surprisingly, the golden ratio comes into play and the value of the game is $\\lfloor\\frac{1+\\sqrt{5}}{2}\\; w \\rfloor$.", ["Partially ordered set", "Golden ratio"]], ["Analysis of the 802.11e Enhanced Distributed Channel Access Function", "The IEEE 802.11e standard revises the Medium Access Control (MAC) layer of the former IEEE 802.11 standard for Quality-of-Service (QoS) provision in the Wireless Local Area Networks (WLANs). The Enhanced Distributed Channel Access (EDCA) function of 802.11e defines multiple Access Categories (AC) with AC-specific Contention Window (CW) sizes, Arbitration Interframe Space (AIFS) values, and Transmit Opportunity (TXOP) limits to support MAC-level QoS and prioritization. We propose an analytical model for the EDCA function which incorporates an accurate CW, AIFS, and TXOP differentiation at any traffic load. The proposed model is also shown to capture the effect of MAC layer buffer size on the performance. Analytical and simulation results are compared to demonstrate the accuracy of the proposed approach for varying traffic loads, EDCA parameters, and MAC layer buffer space.", ["Channel access method", "IEEE 802.11", "IEEE 802.11e-2005", "Mathematical model", "Institute of Electrical and Electronics Engineers", "Simulation", "Quality of service", "Local area network", "Media Access Control", "Access control"]], ["Performance Analysis of the IEEE 802.11e Enhanced Distributed Coordination Function using Cycle Time Approach", "The recently ratified IEEE 802.11e standard defines the Enhanced Distributed Channel Access (EDCA) function for Quality-of-Service (QoS) provisioning in the Wireless Local Area Networks (WLANs). The EDCA uses Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) and slotted Binary Exponential Backoff (BEB) mechanism. We present a simple mathematical analysis framework for the EDCA function. Our analysis considers the fact that the distributed random access systems exhibit cyclic behavior where each station successfully transmits a packet in a cycle. Our analysis shows that an AC-specific cycle time exists for the EDCA function. Validating the theoretical results via simulations, we show that the proposed analysis accurately captures EDCA saturation performance in terms of average throughput, medium access delay, and packet loss ratio. The cycle time analysis is a simple and insightful substitute for previously proposed more complex EDCA models.", ["Mathematical analysis", "Packet loss", "Carrier sense multiple access", "IEEE 802.11e-2005", "Institute of Electrical and Electronics Engineers", "IEEE 802.11", "Mathematics", "Carrier sense multiple access with collision avoidance", "Random access"]], ["Fairness Provision in the IEEE 802.11e Infrastructure Basic Service Set", "Most of the deployed IEEE 802.11e Wireless Local Area Networks (WLANs) use infrastructure Basic Service Set (BSS) in which an Access Point (AP) serves as a gateway between wired and wireless domains. We present the unfairness problem between the uplink and the downlink flows of any Access Category (AC) in the 802.11e Enhanced Distributed Channel Access (EDCA) when the default settings of the EDCA parameters are used. We propose a simple analytical model to calculate the EDCA parameter settings that achieve weighted fair resource allocation for all uplink and downlink flows. We also propose a simple model-assisted measurement-based dynamic EDCA parameter adaptation algorithm. Moreover, our dynamic solution addresses the differences in the transport layer and the Medium Access Control (MAC) layer interactions of User Datagram Protocol (UDP) and Transmission Control Protocol (TCP). We show that proposed Contention Window (CW) and Transmit Opportunity (TXOP) limit adaptation at the AP provides fair UDP and TCP access between uplink and downlink flows of the same AC while preserving prioritization among ACs.", ["IEEE 802.11", "User Datagram Protocol", "Transmission Control Protocol", "Wireless", "Gateway (telecommunications)", "Transport Layer", "Uplink", "IEEE 802.11e-2005", "Service set (802.11 network)", "Wireless access point", "Wireless LAN", "Media Access Control", "Local area network", "Institute of Electrical and Electronics Engineers", "Infrastructure", "IEEE 802", "Algorithm", "Downlink", "Access control", "Transmission (telecommunications)"]], ["An Achievable Rate Region for Interference Channels with Conferencing", "In this paper, we propose an achievable rate region for discrete memoryless interference channels with conferencing at the transmitter side. We employ superposition block Markov encoding, combined with simultaneous superposition coding, dirty paper coding, and random binning to obtain the achievable rate region. We show that, under respective conditions, the proposed achievable region reduces to Han and Kobayashi achievable region for interference channels, the capacity region for degraded relay channels, and the capacity region for the Gaussian vector broadcast channel. Numerical examples for the Gaussian case are given.", ["Memorylessness", "Data binning", "Dirty paper coding", "Transmitter", "Superposition principle"]], ["An algebraic generalization of Kripke structures", "The Kripke semantics of classical propositional normal modal logic is made algebraic via an embedding of Kripke structures into the larger class of pointed stably supported quantales. This algebraic semantics subsumes the traditional algebraic semantics based on lattices with unary operators, and it suggests natural interpretations of modal logic, of possible interest in the applications, in structures that arise in geometry and analysis, such as foliated manifolds and operator algebras, via topological groupoids and inverse semigroups. We study completeness properties of the quantale based semantics for the systems K, T, K4, S4, and S5, in particular obtaining an axiomatization for S5 which does not use negation or the modal necessity operator. As additional examples we describe intuitionistic propositional modal logic, the logic of programs PDL, and the ramified temporal logic CTL.", ["Kripke semantics", "Modal logic", "Normal modal logic", "Temporal logic", "Topology", "Algebraic semantics", "Propositional calculus", "Semigroup", "Lattice (order)", "Intuitionistic logic", "Completeness (order theory)", "Logic", "Geometry", "Negation", "Abstract algebra", "Completeness", "Axiomatic system", "Semantics", "Unary operation", "Manifold"]], ["Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets", "In spatially distributed multiuser antenna systems, the received signal contains multiple carrier-frequency offsets (CFOs) arising from mismatch between the oscillators of transmitters and receivers. This results in a time-varying rotation of the data constellation, which needs to be compensated at the receiver before symbol recovery. In this paper, a new approach for blind CFO estimation and symbol recovery is proposed. The received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual Multiple-Input Multiple-Output (MIMO) problem. By applying blind MIMO system estimation techniques, the system response can be estimated and decoupled versions of the user symbols can be recovered, each one of which contains a distinct CFO. By applying a decision feedback Phase Lock Loop (PLL), the CFO can be mitigated and the transmitted symbols can be recovered. The estimated MIMO system response provides information about the CFOs that can be used to initialize the PLL, speed up its convergence, and avoid ambiguities usually linked with PLL.", ["Phase-locked loop", "MIMO", "Antenna (radio)", "Receiver (radio)", "Transmitter", "Constellation", "Frequency", "Blindness", "Polyphase system", "Oscillation"]], ["A study of structural properties on profiles HMMs", "Motivation: Profile hidden Markov Models (pHMMs) are a popular and very useful tool in the detection of the remote homologue protein families. Unfortunately, their performance is not always satisfactory when proteins are in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm and tool that tries to improve pHMM performance by using structural information while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs. Each pHMM is constructed by weighting each residue in an aligned protein according to a specific structural property of the residue. Properties used were primary, secondary and tertiary structures, accessibility and packing. HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP database to perform our experiments. Throughout, we apply leave-one-family-out cross-validation over protein superfamilies. First, we used the MAMMOTH-mult structural aligner to align the training set proteins. Then, we performed two sets of experiments. In a first experiment, we compared structure weighted models against standard pHMMs and against each other. In a second experiment, we compared the voting model against individual pHMMs. We compare method performance through ROC curves and through Precision/Recall curves, and assess significance through the paired two tailed t-test. Our results show significant performance improvements of all structurally weighted models over default HMMER, and a significant improvement in sensitivity of the combined models over both the original model and the structurally weighted models.", ["HMMER", "Protein", "Structural Classification of Proteins", "Receiver operating characteristic", "Motivation", "Homology (biology)", "Cross-validation (statistics)", "Database", "Student's t-test", "Training set", "Hidden Markov model"]], ["Extensive Games with Possibly Unaware Players", "Standard game theory assumes that the structure of the game is common knowledge among players. We relax this assumption by considering extensive games where agents may be unaware of the complete structure of the game. In particular, they may not be aware of moves that they and other agents can make. We show how such games can be represented; the key idea is to describe the game from the point of view of every agent at every node of the game tree. We provide a generalization of Nash equilibrium and show that every game with awareness has a generalized Nash equilibrium. Finally, we extend these results to games with awareness of unawareness, where a player i may be aware that a player j can make moves that i is not aware of, and to subjective games, where payers may have no common knowledge regarding the actual game and their beliefs are incompatible with a common prior.", ["Nash equilibrium", "Game theory", "Game tree", "Subjectivity"]], ["Large System Analysis of Game-Theoretic Power Control in UWB Wireless Networks with Rake Receivers", "This paper studies the performance of partial-Rake (PRake) receivers in impulse-radio ultrawideband wireless networks when an energy-efficient power control scheme is adopted. Due to the large bandwidth of the system, the multipath channel is assumed to be frequency-selective. By using noncooperative game-theoretic models and large system analysis, explicit expressions are derived in terms of network parameters to measure the effects of self- and multiple-access interference at a receiving access point. Performance of the PRake is compared in terms of achieved utilities and loss to that of the all-Rake receiver.", ["Rake receiver", "Ultra-wideband", "Wireless network", "Frequency", "Wireless", "Multipath propagation", "Channel access method", "Wireless access point", "Fading", "Receiver (radio)", "Radio", "Non-cooperative game", "Power control", "Channel (communications)", "System analysis", "Energy", "Bandwidth (computing)", "Network analysis (electrical circuits)"]], ["Introduction to Arabic Speech Recognition Using CMUSphinx System", "In this paper Arabic was investigated from the speech recognition problem point of view. We propose a novel approach to build an Arabic Automated Speech Recognition System (ASR). This system is based on the open source CMU Sphinx-4, from the Carnegie Mellon University. CMU Sphinx is a large-vocabulary; speaker-independent, continuous speech recognition system based on discrete Hidden Markov Models (HMMs). We build a model using utilities from the OpenSource CMU Sphinx. We will demonstrate the possible adaptability of this system to Arabic voice recognition.", ["Speech recognition", "CMU Sphinx", "Carnegie Mellon University", "Vocabulary", "Arabic language", "Speech", "Open source"]], ["A Note on the Inapproximability of Correlation Clustering", "We consider inapproximability of the correlation clustering problem defined as follows: Given a graph $G = (V,E)$ where each edge is labeled either \"+\" (similar) or \"-\" (dissimilar), correlation clustering seeks to partition the vertices into clusters so that the number of pairs correctly (resp. incorrectly) classified with respect to the labels is maximized (resp. minimized). The two complementary problems are called MaxAgree and MinDisagree, respectively, and have been studied on complete graphs, where every edge is labeled, and general graphs, where some edge might not have been labeled. Natural edge-weighted versions of both problems have been studied as well. Let S-MaxAgree denote the weighted problem where all weights are taken from set S, we show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\\delta})$ essentially belongs to the same hardness class in the following sense: if there is a polynomial time algorithm that approximates S-MaxAgree within a factor of $\\lambda = O(\\log{|V|})$ with high probability, then for any choice of S', S'-MaxAgree can be approximated in polynomial time within a factor of $(\\lambda + \\epsilon)$, where $\\epsilon > 0$ can be arbitrarily small, with high probability. A similar statement also holds for $S-MinDisagree. This result implies it is hard (assuming $NP \\neq RP$) to approximate unweighted MaxAgree within a factor of $80/79-\\epsilon$, improving upon a previous known factor of $116/115-\\epsilon$ by Charikar et. al. \\cite{Chari05}.", ["Polynomial time", "Algorithm", "Graph (mathematics)", "Correlation and dependence", "Probability", "Polynomial", "Charikar", "Vertex (graph theory)", "Lambda calculus", "Cluster analysis"]], ["Arabic Speech Recognition System using CMU-Sphinx4", "In this paper we present the creation of an Arabic version of Automated Speech Recognition System (ASR). This system is based on the open source Sphinx-4, from the Carnegie Mellon University. Which is a speech recognition system based on discrete hidden Markov models (HMMs). We investigate the changes that must be made to the model to adapt Arabic voice recognition. Keywords: Speech recognition, Acoustic model, Arabic language, HMMs, CMUSphinx-4, Artificial intelligence.", ["Speech recognition", "Carnegie Mellon University", "Artificial intelligence", "Hidden Markov model", "Arabic language", "Sphinx", "Open source", "Markov chain"]], ["On the Hardness of Approximating Stopping and Trapping Sets in LDPC Codes", "We prove that approximating the size of stopping and trapping sets in Tanner graphs of linear block codes, and more restrictively, the class of low-density parity-check (LDPC) codes, is NP-hard. The ramifications of our findings are that methods used for estimating the height of the error-floor of moderate- and long-length LDPC codes based on stopping and trapping set enumeration cannot provide accurate worst-case performance predictions.", ["Best, worst and average case", "NP-hard", "Low-density parity-check code"]], ["The Wiretap Channel with Feedback: Encryption over the Channel", "In this work, the critical role of noisy feedback in enhancing the secrecy capacity of the wiretap channel is established. Unlike previous works, where a noiseless public discussion channel is used for feedback, the feed-forward and feedback signals share the same noisy channel in the present model. Quite interestingly, this noisy feedback model is shown to be more advantageous in the current setting. More specifically, the discrete memoryless modulo-additive channel with a full-duplex destination node is considered first, and it is shown that the judicious use of feedback increases the perfect secrecy capacity to the capacity of the source-destination channel in the absence of the wiretapper. In the achievability scheme, the feedback signal corresponds to a private key, known only to the destination. In the half-duplex scheme, a novel feedback technique that always achieves a positive perfect secrecy rate (even when the source-wiretapper channel is less noisy than the source-destination channel) is proposed. These results hinge on the modulo-additive property of the channel, which is exploited by the destination to perform encryption over the channel without revealing its key to the source. Finally, this scheme is extended to the continuous real valued modulo-$\\Lambda$ channel where it is shown that the perfect secrecy capacity with feedback is also equal to the capacity in the absence of the wiretapper.", ["Telephone tapping", "Public-key cryptography", "Duplex (telecommunications)", "Feedback", "Noisy-channel coding theorem", "Encryption", "Half-duplex", "Memorylessness", "Modular arithmetic", "Noise (electronics)", "Real number", "Channel (communications)"]], ["Kekul\\'e Cells for Molecular Computation", "The configurations of single and double bonds in polycyclic hydrocarbons are abstracted as Kekul\\'e states of graphs. Sending a so-called soliton over an open channel between ports (external nodes) of the graph changes the Kekul\\'e state and therewith the set of open channels in the graph. This switching behaviour is proposed as a basis for molecular computation. The proposal is highly speculative but may have tremendous impact. Kekul\\'e states with the same boundary behaviour (port assignment) can be regarded as equivalent. This gives rise to the abstraction of Kekul\\'e cells. The basic theory of Kekul\\'e states and Kekul\\'e cells is developed here, up to the classification of Kekul\\'e cells with $\\leq 4$ ports. To put the theory in context, we generalize Kekul\\'e states to semi-Kekul\\'e states, which form the solutions of a linear system of equations over the field of the bits 0 and 1. We briefly study so-called omniconjugated graphs, in which every port assignment of the right signature has a Kekul\\'e state. Omniconjugated graphs may be useful as connectors between computational elements. We finally investigate some examples with potentially useful switching behaviour.", ["Soliton", "Cell (biology)", "System of linear equations", "Chemical bond", "Hydrocarbon", "Covalent bond", "Computation", "Linear system", "Molecule", "Polycyclic compound"]], ["Using Image Attributes for Human Identification Protocols", "A secure human identification protocol aims at authenticating human users to a remote server when even the users' inputs are not hidden from an adversary. Recently, the authors proposed a human identification protocol in the RSA Conference 2007, which is loosely based on the ability of humans to efficiently process an image. The advantage being that an automated adversary is not effective in attacking the protocol without human assistance. This paper extends that work by trying to solve some of the open problems. First, we analyze the complexity of defeating the proposed protocols by quantifying the workload of a human adversary. Secondly, we propose a new construction based on textual CAPTCHAs (Reverse Turing Tests) in order to make the generation of automated challenges easier. We also present a brief experiment involving real human users to find out the number of possible attributes in a given image and give some guidelines for the selection of challenge questions based on the results. Finally, we analyze the previously proposed protocol in detail for the relationship between the secrets. Our results show that we can construct human identification protocols based on image evaluation with reasonably ``quantified'' security guarantees based on our model.", ["RSA Conference", "Server (computing)", "Challenge-response authentication"]], ["Parallel computing for the finite element method", "A finite element method is presented to compute time harmonic microwave fields in three dimensional configurations. Nodal-based finite elements have been coupled with an absorbing boundary condition to solve open boundary problems. This paper describes how the modeling of large devices has been made possible using parallel computation, New algorithms are then proposed to implement this formulation on a cluster of workstations (10 DEC ALPHA 300X) and on a CRAY C98. Analysis of the computation efficiency is performed using simple problems. The electromagnetic scattering of a plane wave by a perfect electric conducting airplane is finally given as example.", ["Finite element method", "Computing", "Parallel computing", "Plane wave", "Digital Equipment Corporation", "Workstation", "Fixed-wing aircraft", "Microwave", "Boundary value problem"]], ["Parallel computation of the rank of large sparse matrices from algebraic K-theory", "This paper deals with the computation of the rank and of some integer Smith forms of a series of sparse matrices arising in algebraic K-theory. The number of non zero entries in the considered matrices ranges from 8 to 37 millions. The largest rank computation took more than 35 days on 50 processors. We report on the actual algorithms we used to build the matrices, their link to the motivic cohomology and the linear algebra and parallelizations required to perform such huge computations. In particular, these results are part of the first computation of the cohomology of the linear group GL_7(Z).", ["Algebraic K-theory", "Linear algebra", "Algebra", "Motivic cohomology", "Matrix (mathematics)", "Integer", "Matrix group", "K-theory", "Parallel computing", "Sparse matrix", "Computation", "Linear"]], ["Scaling Laws of Cognitive Networks", "We consider a cognitive network consisting of n random pairs of cognitive transmitters and receivers communicating simultaneously in the presence of multiple primary users. Of interest is how the maximum throughput achieved by the cognitive users scales with n. Furthermore, how far these users must be from a primary user to guarantee a given primary outage. Two scenarios are considered for the network scaling law: (i) when each cognitive transmitter uses constant power to communicate with a cognitive receiver at a bounded distance away, and (ii) when each cognitive transmitter scales its power according to the distance to a considered primary user, allowing the cognitive transmitter-receiver distances to grow. Using single-hop transmission, suitable for cognitive devices of opportunistic nature, we show that, in both scenarios, with path loss larger than 2, the cognitive network throughput scales linearly with the number of cognitive users. We then explore the radius of a primary exclusive region void of cognitive transmitters. We obtain bounds on this radius for a given primary outage constraint. These bounds can help in the design of a primary network with exclusive regions, outside of which cognitive users may transmit freely. Our results show that opportunistic secondary spectrum access using single-hop transmission is promising.", ["Path loss", "Throughput", "Power law", "Cognition", "Radius", "Transmitter"]], ["A Nice Labelling for Tree-Like Event Structures of Degree 3", "We address the problem of &#64257;nding nice labellings for event structures of degree 3. We develop a minimum theory by which we prove that the labelling number of an event structure of degree 3 is bounded by a linear function of the height. The main theorem we present in this paper states that event structures of degree 3 whose causality order is a tree have a nice labelling with 3 colors. Finally, we exemplify how to use this theorem to construct upper bounds for the labelling number of other event structures of degree 3.", ["Causality", "Linear function", "Nice"]], ["Power control algorithms for CDMA networks based on large system analysis", "Power control is a fundamental task accomplished in any wireless cellular network; its aim is to set the transmit power of any mobile terminal, so that each user is able to achieve its own target SINR. While conventional power control algorithms require knowledge of a number of parameters of the signal of interest and of the multiaccess interference, in this paper it is shown that in a large CDMA system much of this information can be dispensed with, and effective distributed power control algorithms may be implemented with very little information on the user of interest. An uplink CDMA system subject to flat fading is considered with a focus on the cases in which a linear MMSE receiver and a non-linear MMSE serial interference cancellation receiver are adopted; for the latter case new formulas are also given for the system SINR in the large system asymptote. Experimental results show an excellent agreement between the performance and the power profile of the proposed distributed algorithms and that of conventional ones that require much greater prior knowledge.", ["Code division multiple access", "Uplink", "Wireless", "Power control", "System analysis", "Cellular network", "Algorithm", "Asymptote", "Signal (electronics)"]], ["Power control and receiver design for energy efficiency in multipath CDMA channels with bandlimited waveforms", "This paper is focused on the cross-layer design problem of joint multiuser detection and power control for energy-efficiency optimization in a wireless data network through a game-theoretic approach. Building on work of Meshkati, et al., wherein the tools of game-theory are used in order to achieve energy-efficiency in a simple synchronous code division multiple access system, system asynchronism, the use of bandlimited chip-pulses, and the multipath distortion induced by the wireless channel are explicitly incorporated into the analysis. Several non-cooperative games are proposed wherein users may vary their transmit power and their uplink receiver in order to maximize their utility, which is defined here as the ratio of data throughput to transmit power. In particular, the case in which a linear multiuser detector is adopted at the receiver is considered first, and then, the more challenging case in which non-linear decision feedback multiuser detectors are employed is considered. The proposed games are shown to admit a unique Nash equilibrium point, while simulation results show the effectiveness of the proposed solutions, as well as that the use of a decision-feedback multiuser receiver brings remarkable performance improvements.", ["Nash equilibrium", "Bandlimiting", "Multipath propagation", "Wireless", "Mathematical optimization", "Code division multiple access", "Feedback", "Simulation", "Game theory", "Waveform", "Channel access method", "Energy", "Wireless network", "Computer network", "Uplink", "Data", "Power control", "Linear", "Throughput", "Efficient energy use", "Distortion"]], ["Bounded Pushdown dimension vs Lempel Ziv information density", "In this paper we introduce a variant of pushdown dimension called bounded pushdown (BPD) dimension, that measures the density of information contained in a sequence, relative to a BPD automata, i.e. a finite state machine equipped with an extra infinite memory stack, with the additional requirement that every input symbol only allows a bounded number of stack movements. BPD automata are a natural real-time restriction of pushdown automata. We show that BPD dimension is a robust notion by giving an equivalent characterization of BPD dimension in terms of BPD compressors. We then study the relationships between BPD compression, and the standard Lempel-Ziv (LZ) compression algorithm, and show that in contrast to the finite-state compressor case, LZ is not universal for bounded pushdown compressors in a strong sense: we construct a sequence that LZ fails to compress signicantly, but that is compressed by at least a factor 2 by a BPD compressor. As a corollary we obtain a strong separation between finite-state and BPD dimension.", ["Finite-state machine", "Algorithm", "Pushdown automaton", "Automaton", "Finite set", "Data compression", "Alphabet (computer science)", "LZ77 and LZ78", "Stack (data structure)", "Abraham Lempel", "Dimension", "Entropy (information theory)"]], ["Light Logics and Optimal Reduction: Completeness and Complexity", "Typing of lambda-terms in Elementary and Light Affine Logic (EAL, LAL, resp.) has been studied for two different reasons: on the one hand the evaluation of typed terms using LAL (EAL, resp.) proof-nets admits a guaranteed polynomial (elementary, resp.) bound; on the other hand these terms can also be evaluated by optimal reduction using the abstract version of Lamping's algorithm. The first reduction is global while the second one is local and asynchronous. We prove that for LAL (EAL, resp.) typed terms, Lamping's abstract algorithm also admits a polynomial (elementary, resp.) bound. We also show its soundness and completeness (for EAL and LAL with type fixpoints), by using a simple geometry of interaction model (context semantics).", ["Soundness", "Mathematical proof", "Completeness", "Semantics", "Logic", "Polynomial", "Complexity", "Lambda calculus", "Geometry", "Algorithm"]], ["Optimum Linear LLR Calculation for Iterative Decoding on Fading Channels", "On a fading channel with no channel state information at the receiver, calculating true log-likelihood ratios (LLR) is complicated. Existing work assume that the power of the additive noise is known and use the expected value of the fading gain in a linear function of the channel output to find approximate LLRs. In this work, we first assume that the power of the additive noise is known and we find the optimum linear approximation of LLRs in the sense of maximum achievable transmission rate on the channel. The maximum achievable rate under this linear LLR calculation is almost equal to the maximum achievable rate under true LLR calculation. We also observe that this method appears to be the optimum in the sense of bit error rate performance too. These results are then extended to the case that the noise power is unknown at the receiver and a performance almost identical to the case that the noise power is perfectly known is obtained.", ["Expected value", "Channel state information", "Noise power", "Linear approximation", "Bit error rate", "Linear function", "Fading", "Likelihood function", "Additive white Gaussian noise", "Bit"]], ["Physical Layer Network Coding", "A main distinguishing feature of a wireless network compared with a wired network is its broadcast nature, in which the signal transmitted by a node may reach several other nodes, and a node may receive signals from several other nodes simultaneously. Rather than a blessing, this feature is treated more as an interference-inducing nuisance in most wireless networks today (e.g., IEEE 802.11). This paper shows that the concept of network coding can be applied at the physical layer to turn the broadcast property into a capacity-boosting advantage in wireless ad hoc networks. Specifically, we propose a physical-layer network coding (PNC) scheme to coordinate transmissions among nodes. In contrast to straightforward network coding which performs coding arithmetic on digital bit streams after they have been received, PNC makes use of the additive nature of simultaneously arriving electromagnetic (EM) waves for equivalent coding operation. And in doing so, PNC can potentially achieve 100% and 50% throughput increases compared with traditional transmission and straightforward network coding, respectively, in multi-hop networks. More specifically, the information-theoretic capacity of PNC is almost double that of traditional transmission in the SNR region of practical interest (higher than 0dB). We believe this is a first paper that ventures into EM-wave-based network coding at the physical layer and demonstrates its potential for boosting network capacity.", ["IEEE 802.11", "Physical Layer", "Wireless network", "Ad hoc", "Decibel", "Signal-to-noise ratio", "Transmission (telecommunications)", "Information theory", "Wireless", "Institute of Electrical and Electronics Engineers", "Forward error correction", "Signal (electronics)", "Bit", "Interference (communication)"]], ["Algebraic Distributed Space-Time Codes with Low ML Decoding Complexity", "\"Extended Clifford algebras\" are introduced as a means to obtain low ML decoding complexity space-time block codes. Using left regular matrix representations of two specific classes of extended Clifford algebras, two systematic algebraic constructions of full diversity Distributed Space-Time Codes (DSTCs) are provided for any power of two number of relays. The left regular matrix representation has been shown to naturally result in space-time codes meeting the additional constraints required for DSTCs. The DSTCs so constructed have the salient feature of reduced Maximum Likelihood (ML) decoding complexity. In particular, the ML decoding of these codes can be performed by applying the lattice decoder algorithm on a lattice of four times lesser dimension than what is required in general. Moreover these codes have a uniform distribution of power among the relays and in time, thus leading to a low Peak to Average Power Ratio at the relays.", ["Representation theory", "Clifford algebra", "Group representation", "Algebra over a field", "Dimension", "Spacetime", "Matrix (mathematics)", "Power of two", "Uniform distribution (continuous)", "Complexity", "Maximum likelihood", "Lattice (order)"]], ["STBCs from Representation of Extended Clifford Algebras", "A set of sufficient conditions to construct $\\lambda$-real symbol Maximum Likelihood (ML) decodable STBCs have recently been provided by Karmakar et al. STBCs satisfying these sufficient conditions were named as Clifford Unitary Weight (CUW) codes. In this paper, the maximal rate (as measured in complex symbols per channel use) of CUW codes for $\\lambda=2^a,a\\in\\mathbb{N}$ is obtained using tools from representation theory. Two algebraic constructions of codes achieving this maximal rate are also provided. One of the constructions is obtained using linear representation of finite groups whereas the other construction is based on the concept of right module algebra over non-commutative rings. To the knowledge of the authors, this is the first paper in which matrices over non-commutative rings is used to construct STBCs. An algebraic explanation is provided for the 'ABBA' construction first proposed by Tirkkonen et al and the tensor product construction proposed by Karmakar et al. Furthermore, it is established that the 4 transmit antenna STBC originally proposed by Tirkkonen et al based on the ABBA construction is actually a single complex symbol ML decodable code if the design variables are permuted and signal sets of appropriate dimensions are chosen.", ["Representation theory", "ABBA", "Matrix (mathematics)", "Tensor product", "Finite set", "Commutative ring", "Commutative property", "Algebraic number", "Algebra", "Tensor", "Finite group", "Clifford algebra", "Module (mathematics)", "Permutation", "Ring (mathematics)", "Noncommutative ring", "Unitary authority"]], ["Signal Set Design for Full-Diversity Low-Decoding-Complexity Differential Scaled-Unitary STBCs", "The problem of designing high rate, full diversity noncoherent space-time block codes (STBCs) with low encoding and decoding complexity is addressed. First, the notion of $g$-group encodable and $g$-group decodable linear STBCs is introduced. Then for a known class of rate-1 linear designs, an explicit construction of fully-diverse signal sets that lead to four-group encodable and four-group decodable differential scaled unitary STBCs for any power of two number of antennas is provided. Previous works on differential STBCs either sacrifice decoding complexity for higher rate or sacrifice rate for lower decoding complexity.", ["Power of two", "Spacetime", "Antenna (radio)", "Complexity"]], ["Noncoherent Low-Decoding-Complexity Space-Time Codes for Wireless Relay Networks", "The differential encoding/decoding setup introduced by Kiran et al, Oggier et al and Jing et al for wireless relay networks that use codebooks consisting of unitary matrices is extended to allow codebooks consisting of scaled unitary matrices. For such codebooks to be used in the Jing-Hassibi protocol for cooperative diversity, the conditions that need to be satisfied by the relay matrices and the codebook are identified. A class of previously known rate one, full diversity, four-group encodable and four-group decodable Differential Space-Time Codes (DSTCs) is proposed for use as Distributed DSTCs (DDSTCs) in the proposed set up. To the best of our knowledge, this is the first known low decoding complexity DDSTC scheme for cooperative wireless networks.", ["Wireless", "Wireless network", "Cooperative diversity", "Spacetime", "Complexity", "Character encoding", "Matrix (mathematics)", "Unitary matrix", "Codebook"]], ["Narratives within immersive technologies", "The main goal of this project is to research technical advances in order to enhance the possibility to develop narratives within immersive mediated environments. An important part of the research is concerned with the question of how a script can be written, annotated and realized for an immersive context. A first description of the main theoretical framework and the ongoing work and a first script example is provided. This project is part of the program for presence research, and it will exploit physiological feedback and Computational Intelligence within virtual reality.", ["Physiology", "Virtual reality"]], ["Existence Proofs of Some EXIT Like Functions", "The Extended BP (EBP) Generalized EXIT (GEXIT) function introduced in \\cite{MMRU05} plays a fundamental role in the asymptotic analysis of sparse graph codes. For transmission over the binary erasure channel (BEC) the analytic properties of the EBP GEXIT function are relatively simple and well understood. The general case is much harder and even the existence of the curve is not known in general. We introduce some tools from non-linear analysis which can be useful to prove the existence of EXIT like curves in some cases. The main tool is the Krasnoselskii-Rabinowitz (KR) bifurcation theorem.", ["Function (mathematics)", "Asymptotic analysis", "Dense graph", "Curve", "Theorem", "Graph (mathematics)", "Mathematical proof"]], ["Computing Extensions of Linear Codes", "This paper deals with the problem of increasing the minimum distance of a linear code by adding one or more columns to the generator matrix. Several methods to compute extensions of linear codes are presented. Many codes improving the previously known lower bounds on the minimum distance have been found.", ["Linear code", "Computing", "Matrix (mathematics)", "Generator matrix"]], ["A-infinity structure on simplicial complexes", "A discrete (finite-difference) analogue of differential forms is considered, defined on simplicial complexes, including triangulations of continuous manifolds. Various operations are explicitly defined on these forms, including exterior derivative and exterior product. The latter one is non-associative. Instead, as anticipated, it is a part of non-trivial A-infinity structure, involving a chain of poly-linear operations, constrained by nilpotency relation: (d + \\wedge + m + ...)^n = 0 with n=2.", ["Exterior derivative", "Exterior algebra", "Differential form", "Finite set", "Finite difference", "Linear map", "Derivative", "Infinity", "Manifold", "Continuous function", "Linear", "Complex number", "Nilpotent group", "Pushforward (differential)", "Associative property", "Simplicial complex", "Discrete space"]], ["Joint universal lossy coding and identification of stationary mixing sources", "The problem of joint universal source coding and modeling, treated in the context of lossless codes by Rissanen, was recently generalized to fixed-rate lossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We extend these results to variable-rate lossy block coding of stationary ergodic sources and show that, for bounded metric distortion measures, any finitely parametrized family of stationary sources satisfying suitable mixing, smoothness and Vapnik-Chervonenkis learnability conditions admits universal schemes for joint lossy source coding and identification. We also give several explicit examples of parametric sources satisfying the regularity conditions.", ["Parametric family", "Ergodicity", "Data compression", "Coordinate system"]], ["Opportunistic Communications in an Orthogonal Multiaccess Relay Channel", "The problem of resource allocation is studied for a two-user fading orthogonal multiaccess relay channel (MARC) where both users (sources) communicate with a destination in the presence of a relay. A half-duplex relay is considered that transmits on a channel orthogonal to that used by the sources. The instantaneous fading state between every transmit-receive pair in this network is assumed to be known at both the transmitter and receiver. Under an average power constraint at each source and the relay, the sum-rate for the achievable strategy of decode-and-forward (DF) is maximized over all power allocations (policies) at the sources and relay. It is shown that the sum-rate maximizing policy exploits the multiuser fading diversity to reveal the optimality of opportunistic channel use by each user. A geometric interpretation of the optimal power policy is also presented.", ["Orthogonality", "Geometry", "MARC Train"]], ["Minimum Expected Distortion in Gaussian Layered Broadcast Coding with Successive Refinement", "A transmitter without channel state information (CSI) wishes to send a delay-limited Gaussian source over a slowly fading channel. The source is coded in superimposed layers, with each layer successively refining the description in the previous one. The receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. In the limit of a continuum of infinite layers, the optimal power distribution that minimizes the expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. In the optimal power distribution, as SNR increases, the allocation over the higher layers remains unchanged; rather the extra power is allocated towards the lower layers. On the other hand, as the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the power distribution that maximizes expected capacity. While expected distortion can be improved by acquiring CSI at the transmitter (CSIT) or by increasing diversity from the realization of independent fading paths, at high SNR the performance benefit from diversity exceeds that from CSIT, especially when b is large.", ["Channel state information", "Fading", "Linear differential equation", "Transmitter", "Differential equation", "Electric power distribution", "Distortion"]], ["Supervised Feature Selection via Dependence Estimation", "We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.", ["Feature selection", "Hilbert-Schmidt operator", "Supervised learning", "David Hilbert"]], ["A Channel that Heats Up", "Motivated by on-chip communication, a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers. For this channel, an expression for the capacity per unit cost is derived, and it is shown that the expression holds also in the presence of feedback.", ["Weight function", "Feedback", "Communication", "Additive white Gaussian noise"]], ["Exploiting Heavy Tails in Training Times of Multilayer Perceptrons: A Case Study with the UCI Thyroid Disease Database", "The random initialization of weights of a multilayer perceptron makes it possible to model its training process as a Las Vegas algorithm, i.e. a randomized algorithm which stops when some required training error is obtained, and whose execution time is a random variable. This modeling is used to perform a case study on a well-known pattern recognition benchmark: the UCI Thyroid Disease Database. Empirical evidence is presented of the training time probability distribution exhibiting a heavy tail behavior, meaning a big probability mass of long executions. This fact is exploited to reduce the training time cost by applying two simple restart strategies. The first assumes full knowledge of the distribution yielding a 40% cut down in expected time with respect to the training without restarts. The second, assumes null knowledge, yielding a reduction ranging from 9% to 23%.", ["Random variable", "Multilayer perceptron", "Probability distribution", "Pattern recognition", "Las Vegas algorithm", "Perceptron", "Randomness", "Randomized algorithm", "Probability", "Heavy-tailed distribution", "Algorithm"]], ["Random Access Broadcast: Stability and Throughput Analysis", "A wireless network in which packets are broadcast to a group of receivers through use of a random access protocol is considered in this work. The relation to previous work on networks of interacting queues is discussed and subsequently, the stability and throughput regions of the system are analyzed and presented. A simple network of two source nodes and two destination nodes is considered first. The broadcast service process is analyzed assuming a channel that allows for packet capture and multipacket reception. In this small network, the stability and throughput regions are observed to coincide. The same problem for a network with N sources and M destinations is considered next. The channel model is simplified in that multipacket reception is no longer permitted. Bounds on the stability region are developed using the concept of stability rank and the throughput region of the system is compared to the bounds. Our results show that as the number of destination nodes increases, the stability and throughput regions diminish. Additionally, a previous conjecture that the stability and throughput regions coincide for a network of arbitrarily many sources is supported for a broadcast scenario by the results presented in this work.", ["Wireless network", "Wireless", "Node (networking)", "Communications protocol", "Network packet", "Throughput", "Packet capture", "Random access", "Channel (communications)", "Channel model", "Computer network", "Broadcasting"]], ["The Complexity of Simple Stochastic Games", "In this paper we survey the computational time complexity of assorted simple stochastic game problems, and we give an overview of the best known algorithms associated with each problem.", ["Time complexity", "Stochastic", "Algorithm", "Complexity"]], ["Writing on Dirty Paper with Resizing and its Application to Quasi-Static Fading Broadcast Channels", "This paper studies a variant of the classical problem of ``writing on dirty paper'' in which the sum of the input and the interference, or dirt, is multiplied by a random variable that models resizing, known to the decoder but not to the encoder. The achievable rate of Costa's dirty paper coding (DPC) scheme is calculated and compared to the case of the decoder's also knowing the dirt. In the ergodic case, the corresponding rate loss vanishes asymptotically in the limits of both high and low signal-to-noise ratio (SNR), and is small at all finite SNR for typical distributions like Rayleigh, Rician, and Nakagami. In the quasi-static case, the DPC scheme is lossless at all SNR in terms of outage probability. Quasi-static fading broadcast channels (BC) without transmit channel state information (CSI) are investigated as an application of the robustness properties. It is shown that the DPC scheme leads to an outage achievable rate region that strictly dominates that of time division.", ["Signal-to-noise ratio", "Random variable", "Encoder", "Channel state information", "Signal (electronics)", "Probability", "Lossless data compression", "Channel (communications)", "Dirty paper coding", "Ergodicity"]], ["Minimum cost distributed source coding over a network", "This work considers the problem of transmitting multiple compressible sources over a network at minimum cost. The aim is to find the optimal rates at which the sources should be compressed and the network flows using which they should be transmitted so that the cost of the transmission is minimal. We consider networks with capacity constraints and linear cost functions. The problem is complicated by the fact that the description of the feasible rate region of distributed source coding problems typically has a number of constraints that is exponential in the number of sources. This renders general purpose solvers inefficient. We present a framework in which these problems can be solved efficiently by exploiting the structure of the feasible rate regions coupled with dual decomposition and optimization techniques such as the subgradient method and the proximal bundle method.", ["Data compression", "Mathematical optimization"]], ["On Algebraic Decoding of $q$-ary Reed-Muller and Product-Reed-Solomon Codes", "We consider a list decoding algorithm recently proposed by Pellikaan-Wu \\cite{PW2005} for $q$-ary Reed-Muller codes $\\mathcal{RM}_q(\\ell, m, n)$ of length $n \\leq q^m$ when $\\ell \\leq q$. A simple and easily accessible correctness proof is given which shows that this algorithm achieves a relative error-correction radius of $\\tau \\leq (1 - \\sqrt{{\\ell q^{m-1}}/{n}})$. This is an improvement over the proof using one-point Algebraic-Geometric codes given in \\cite{PW2005}. The described algorithm can be adapted to decode Product-Reed-Solomon codes. We then propose a new low complexity recursive algebraic decoding algorithm for Reed-Muller and Product-Reed-Solomon codes. Our algorithm achieves a relative error correction radius of $\\tau \\leq \\prod_{i=1}^m (1 - \\sqrt{k_i/q})$. This technique is then proved to outperform the Pellikaan-Wu method in both complexity and error correction radius over a wide range of code rates.", ["Error detection and correction", "Correctness (computer science)", "Reed-Solomon", "Radius", "Reed-Solomon error correction", "Approximation error"]], ["A High-Throughput Cross-Layer Scheme for Distributed Wireless Ad Hoc Networks", "In wireless ad hoc networks, distributed nodes can collaboratively form an antenna array for long-distance communications to achieve high energy efficiency. In recent work, Ochiai, et al., have shown that such collaborative beamforming can achieve a statistically nice beampattern with a narrow main lobe and low sidelobes. However, the process of collaboration introduces significant delay, since all collaborating nodes need access to the same information. In this paper, a technique that significantly reduces the collaboration overhead is proposed. It consists of two phases. In the first phase, nodes transmit locally in a random access fashion. Collisions, when they occur, are viewed as linear mixtures of the collided packets. In the second phase, a set of cooperating nodes acts as a distributed antenna system and beamform the received analog waveform to one or more faraway destinations. This step requires multiplication of the received analog waveform by a complex number, which is independently computed by each cooperating node, and which enables separation of the collided packets based on their final destination. The scheme requires that each node has global knowledge of the network coordinates. The proposed scheme can achieve high throughput, which in certain cases exceeds one.", ["Complex number", "Beamforming", "Waveform", "Network packet", "Wireless ad hoc network", "Main lobe", "Energy", "Ad hoc", "Throughput", "Antenna (radio)", "Multiplication"]], ["Nature-Inspired Interconnects for Self-Assembled Large-Scale Network-on-Chip Designs", "Future nano-scale electronics built up from an Avogadro number of components needs efficient, highly scalable, and robust means of communication in order to be competitive with traditional silicon approaches. In recent years, the Networks-on-Chip (NoC) paradigm emerged as a promising solution to interconnect challenges in silicon-based electronics. Current NoC architectures are either highly regular or fully customized, both of which represent implausible assumptions for emerging bottom-up self-assembled molecular electronics that are generally assumed to have a high degree of irregularity and imperfection. Here, we pragmatically and experimentally investigate important design trade-offs and properties of an irregular, abstract, yet physically plausible 3D small-world interconnect fabric that is inspired by modern network-on-chip paradigms. We vary the framework's key parameters, such as the connectivity, the number of switch nodes, the distribution of long- versus short-range connections, and measure the network's relevant communication characteristics. We further explore the robustness against link failures and the ability and efficiency to solve a simple toy problem, the synchronization task. The results confirm that (1) computation in irregular assemblies is a promising and disruptive computing paradigm for self-assembled nano-scale electronics and (2) that 3D small-world interconnect fabrics with a power-law decaying distribution of shortcut lengths are physically plausible and have major advantages over local 2D and 3D regular topologies.", ["Molecular electronics", "Paradigm", "Toy", "Communication", "Avogadro constant", "Computing", "Power law", "Computation", "Silicon", "Electronics", "Programming paradigm"]], ["Modern Coding Theory: The Statistical Mechanics and Computer Science Point of View", "These are the notes for a set of lectures delivered by the two authors at the Les Houches Summer School on `Complex Systems' in July 2006. They provide an introduction to the basic concepts in modern (probabilistic) coding theory, highlighting connections with statistical mechanics. We also stress common concepts with other disciplines dealing with similar problems that can be generically referred to as `large graphical models'. While most of the lectures are devoted to the classical channel coding problem over simple memoryless channels, we present a discussion of more complex channel models. We conclude with an overview of the main open challenges in the field.", ["Coding theory", "Statistical mechanics", "Mechanics", "Memorylessness", "Graphical model", "Computer science", "Les Houches", "Statistics", "Forward error correction"]], ["Higher-order theories", "We extend our approach to abstract syntax (with binding constructions) through modules and linearity. First we give a new general definition of arity, yielding the companion notion of signature. Then we obtain a modularity result as requested by Ghani and Uustalu (2003): in our setting, merging two extensions of syntax corresponds to building an amalgamated sum. Finally we define a natural notion of equation concerning a signature and prove the existence of an initial semantics for a so-called representable signature equipped with a set of equations.", ["Arity"]], ["Recommending Related Papers Based on Digital Library Access Records", "An important goal for digital libraries is to enable researchers to more easily explore related work. While citation data is often used as an indicator of relatedness, in this paper we demonstrate that digital access records (e.g. http-server logs) can be used as indicators as well. In particular, we show that measures based on co-access provide better coverage than co-citation, that they are available much sooner, and that they are more accurate for recent papers.", ["Server (computing)"]], ["On Verifying and Engineering the Well-gradedness of a Union-closed Family", "Current techniques for generating a knowledge space, such as QUERY, guarantees that the resulting structure is closed under union, but not that it satisfies wellgradedness, which is one of the defining conditions for a learning space. We give necessary and sufficient conditions on the base of a union-closed set family that ensures that the family is well-graded. We consider two cases, depending on whether or not the family contains the empty set. We also provide algorithms for efficiently testing these conditions, and for augmenting a set family in a minimal way to one that satisfies these conditions.", ["Necessary and sufficient condition", "Closed set", "Empty set", "Engineering"]], ["Optimal Routing for the Gaussian Multiple-Relay Channel with Decode-and-Forward", "In this paper, we study a routing problem on the Gaussian multiple relay channel, in which nodes employ a decode-and-forward coding strategy. We are interested in routes for the information flow through the relays that achieve the highest DF rate. We first construct an algorithm that provably finds optimal DF routes. As the algorithm runs in factorial time in the worst case, we propose a polynomial time heuristic algorithm that finds an optimal route with high probability. We demonstrate that that the optimal (and near optimal) DF routes are good in practice by simulating a distributed DF coding scheme using low density parity check codes with puncturing and incremental redundancy.", ["Metaheuristic", "Polynomial time", "Best, worst and average case", "Low-density parity-check code", "Factorial", "Routing", "Heuristic", "Algorithm", "Polynomial", "Hybrid automatic repeat request", "Parity bit"]], ["Using Access Data for Paper Recommendations on ArXiv.org", "This thesis investigates in the use of access log data as a source of information for identifying related scientific papers. This is done for arXiv.org, the authority for publication of e-prints in several fields of physics. Compared to citation information, access logs have the advantage of being immediately available, without manual or automatic extraction of the citation graph. Because of that, a main focus is on the question, how far user behavior can serve as a replacement for explicit meta-data, which potentially might be expensive or completely unavailable. Therefore, we compare access, content, and citation-based measures of relatedness on different recommendation tasks. As a final result, an online recommendation system has been built that can help scientists to find further relevant literature, without having to search for them actively.", ["Thesis", "Literature", "Recommender system", "Physics", "ArXiv", "Data"]], ["Arbitrary Rate Permutation Modulation for the Gaussian Channel", "In this paper non-group permutation modulated sequences for the Gaussian channel are considered. Without the restriction to group codes rather than subsets of group codes, arbitrary rates are achievable. The code construction utilizes the known optimal group constellations to ensure at least the same performance but exploit the Gray code ordering structure of multiset permutations as a selection criterion at the decoder. The decoder achieves near maximum likelihood performance at low computational cost and low additional memory requirements at the receiver.", ["Gray code", "Multiset", "Additive white Gaussian noise", "Maximum likelihood", "Modulation", "Decoder"]], ["Achievable Rates for Two-Way Wire-Tap Channels", "We consider two-way wire-tap channels, where two users are communicating with each other in the presence of an eavesdropper, who has access to the communications through a multiple-access channel. We find achievable rates for two different scenarios, the Gaussian two-way wire-tap channel, (GTW-WT), and the binary additive two-way wire-tap channel, (BATW-WT). It is shown that the two-way channels inherently provide a unique advantage for wire-tapped scenarios, as the users know their own transmitted signals and in effect help encrypt the other user's messages, similar to a one-time pad. We compare the achievable rates to that of the Gaussian multiple-access wire-tap channel (GMAC-WT) to illustrate this advantage.", ["One-time pad", "Channel access method", "Wire", "Channel (communications)"]], ["Detection of two-sided alternatives in a Brownian motion model", "This work examines the problem of sequential detection of a change in the drift of a Brownian motion in the case of two-sided alternatives. Applications to real life situations in which two-sided changes can occur are discussed. Traditionally, 2-CUSUM stopping rules have been used for this problem due to their asymptotically optimal character as the mean time between false alarms tends to $\\infty$. In particular, attention has focused on 2-CUSUM harmonic mean rules due to the simplicity in calculating their first moments. In this paper, we derive closed-form expressions for the first moment of a general 2-CUSUM stopping rule. We use these expressions to obtain explicit upper and lower bounds for it. Moreover, we derive an expression for the rate of change of this first moment as one of the threshold parameters changes. Based on these expressions we obtain explicit upper and lower bounds to this rate of change. Using these expressions we are able to find the best 2-CUSUM stopping rule with respect to the extended Lorden criterion. In fact, we demonstrate not only the existence but also the uniqueness of the best 2-CUSUM stopping both in the case of a symmetric change and in the case of a non-symmetric case. Furthermore, we discuss the existence of a modification of the 2-CUSUM stopping rule that has a strictly better performance than its classical 2-CUSUM counterpart for small values of the mean time between false alarms. We conclude with a discussion on the open problem of strict optimality in the case of two-sided alternatives.", ["Control chart", "Stopping time", "Brownian motion", "Asymptotically optimal algorithm", "Derivative", "Open problem", "Upper and lower bounds", "Classical mechanics", "Harmonic mean"]], ["Space Time Codes from Permutation Codes", "A new class of space time codes with high performance is presented. The code design utilizes tailor-made permutation codes, which are known to have large minimal distances as spherical codes. A geometric connection between spherical and space time codes has been used to translate them into the final space time codes. Simulations demonstrate that the performance increases with the block lengths, a result that has been conjectured already in previous work. Further, the connection to permutation codes allows for moderate complex en-/decoding algorithms.", ["Permutation", "Spacetime", "Code"]], ["Algorithm for Evaluation of the Interval Power Function of Unconstrained Arguments", "We describe an algorithm for evaluation of the interval extension of the power function of variables x and y given by the expression x^y. Our algorithm reduces the general case to the case of non-negative bases.", ["Non-negative", "Algorithm"]], ["Experimenting with recursive queries in database and logic programming systems", "This paper considers the problem of reasoning on massive amounts of (possibly distributed) data. Presently, existing proposals show some limitations: {\\em (i)} the quantity of data that can be handled contemporarily is limited, due to the fact that reasoning is generally carried out in main-memory; {\\em (ii)} the interaction with external (and independent) DBMSs is not trivial and, in several cases, not allowed at all; {\\em (iii)} the efficiency of present implementations is still not sufficient for their utilization in complex reasoning tasks involving massive amounts of data. This paper provides a contribution in this setting; it presents a new system, called DLV$^{DB}$, which aims to solve these problems. Moreover, the paper reports the results of a thorough experimental analysis we have carried out for comparing our system with several state-of-the-art systems (both logic and databases) on some classical deductive problems; the other tested systems are: LDL++, XSB, Smodels and three top-level commercial DBMSs. DLV$^{DB}$ significantly outperforms even the commercial Database Systems on recursive queries. To appear in Theory and Practice of Logic Programming (TPLP)", ["Logic programming", "Database management system", "Reason", "Deductive reasoning", "Logic"]], ["Computing modular polynomials in quasi-linear time", "We analyse and compare the complexity of several algorithms for computing modular polynomials. We show that an algorithm relying on floating point evaluation of modular functions and on interpolation, which has received little attention in the literature, has a complexity that is essentially (up to logarithmic factors) linear in the size of the computed polynomials. In particular, it obtains the classical modular polynomials $\\Phi_\\ell$ of prime level $\\ell$ in time O (\\ell^3 \\log^4 \\ell \\log \\log \\ell). Besides treating modular polynomials for $\\Gamma^0 (\\ell)$, which are an important ingredient in many algorithms dealing with isogenies of elliptic curves, the algorithm is easily adapted to more general situations. Composite levels are handled just as easily as prime levels, as well as polynomials between a modular function and its transform of prime level, such as the Schl\\\"afli polynomials and their generalisations. Our distributed implementation of the algorithm confirms the theoretical analysis by computing modular equations of record level around 10000 in less than two weeks on ten processors.", ["Modular form", "Floating point", "Computing", "Function (mathematics)", "Polynomial", "Algorithm", "Interpolation", "Linear"]], ["Euclidean Shortest Paths in Simple Cube Curves at a Glance", "This paper reports about the development of two provably correct approximate algorithms which calculate the Euclidean shortest path (ESP) within a given cube-curve with arbitrary accuracy, defined by $\\epsilon >0$, and in time complexity $\\kappa(\\epsilon) \\cdot {\\cal O}(n)$, where $\\kappa(\\epsilon)$ is the length difference between the path used for initialization and the minimum-length path, divided by $\\epsilon$. A run-time diagram also illustrates this linear-time behavior of the implemented ESP algorithm.", ["Correctness (computer science)", "Algorithm", "Cube", "Time complexity", "Euclidean space"]], ["Generalized Stability Condition for Generalized and Doubly-Generalized LDPC Codes", "In this paper, the stability condition for low-density parity-check (LDPC) codes on the binary erasure channel (BEC) is extended to generalized LDPC (GLDPC) codes and doublygeneralized LDPC (D-GLDPC) codes. It is proved that, in both cases, the stability condition only involves the component codes with minimum distance 2. The stability condition for GLDPC codes is always expressed as an upper bound to the decoding threshold. This is not possible for D-GLDPC codes, unless all the generalized variable nodes have minimum distance at least 3. Furthermore, a condition called derivative matching is defined in the paper. This condition is sufficient for a GLDPC or DGLDPC code to achieve the stability condition with equality. If this condition is satisfied, the threshold of D-GLDPC codes (whose generalized variable nodes have all minimum distance at least 3) and GLDPC codes can be expressed in closed form.", ["Binary erasure channel", "Low-density parity-check code"]], ["Characterization of P2P IPTV Traffic: Scaling Analysis", "P2P IPTV applications arise on the Internet and will be massively used in the future. It is expected that P2P IPTV will contribute to increase the overall Internet traffic. In this context, it is important to measure the impact of P2P IPTV on the networks and to characterize this traffic. Dur- ing the 2006 FIFA World Cup, we performed an extensive measurement campaign. We measured network traffic generated by broadcasting soc- cer games by the most popular P2P IPTV applications, namely PPLive, PPStream, SOPCast and TVAnts. From the collected data, we charac- terized the P2P IPTV traffic structure at different time scales by using wavelet based transform method. To the best of our knowledge, this is the first work, which presents a complete multiscale analysis of the P2P IPTV traffic. Our results show that the scaling properties of the TCP traffic present periodic behavior whereas the UDP traffic is stationary and lead to long- range depedency characteristics. For all the applications, the download traffic has different characteristics than the upload traffic. The signaling traffic has a significant impact on the download traffic but it has negligible impact on the upload. Both sides of the traffic and its granularity has to be taken into account to design accurate P2P IPTV traffic models.", ["IPTV", "2006 FIFA World Cup", "FIFA World Cup", "Granularity", "Transmission Control Protocol", "Peer-to-peer", "Wavelet", "Multiscale modeling", "User Datagram Protocol"]], ["Alternative axiomatics and complexity of deliberative STIT theories", "We propose two alternatives to Xu's axiomatization of the Chellas STIT. The first one also provides an alternative axiomatization of the deliberative STIT. The second one starts from the idea that the historic necessity operator can be defined as an abbreviation of operators of agency, and can thus be eliminated from the logic of the Chellas STIT. The second axiomatization also allows us to establish that the problem of deciding the satisfiability of a STIT formula without temporal operators is NP-complete in the single-agent case, and is NEXPTIME-complete in the multiagent case, both for the deliberative and the Chellas' STIT.", ["NP-complete", "Logic", "Boolean satisfiability problem", "NEXPTIME", "Axiomatic system", "Multi-agent system"]], ["Neighbor Discovery in Wireless Networks:A Multiuser-Detection Approach", "We examine the problem of determining which nodes are neighbors of a given one in a wireless network. We consider an unsupervised network operating on a frequency-flat Gaussian channel, where $K+1$ nodes associate their identities to nonorthogonal signatures, transmitted at random times, synchronously, and independently. A number of neighbor-discovery algorithms, based on different optimization criteria, are introduced and analyzed. Numerical results show how reduced-complexity algorithms can achieve a satisfactory performance.", ["Additive white Gaussian noise", "Wireless network"]], ["2D Path Solutions from a Single Layer Excitable CNN Model", "An easily implementable path solution algorithm for 2D spatial problems, based on excitable/programmable characteristics of a specific cellular nonlinear network (CNN) model is presented and numerically investigated. The network is a single layer bioinspired model which was also implemented in CMOS technology. It exhibits excitable characteristics with regionally bistable cells. The related response realizes propagations of trigger autowaves, where the excitable mode can be globally preset and reset. It is shown that, obstacle distributions in 2D space can also be directly mapped onto the coupled cell array in the network. Combining these two features, the network model can serve as the main block in a 2D path computing processor. The related algorithm and configurations are numerically experimented with circuit level parameters and performance estimations are also presented. The simplicity of the model also allows alternative technology and device level implementation, which may become critical in autonomous processor design of related micro or nanoscale robotic applications.", ["2D computer graphics", "Computing", "CNN", "Nonlinear system", "Algorithm", "CMOS", "Cell (biology)", "Technology", "Robotics", "Central processing unit"]], ["Sample size cognizant detection of signals in white noise", "The detection and estimation of signals in noisy, limited data is a problem of interest to many scientific and engineering communities. We present a computationally simple, sample eigenvalue based procedure for estimating the number of high-dimensional signals in white noise when there are relatively few samples. We highlight a fundamental asymptotic limit of sample eigenvalue based detection of weak high-dimensional signals from a limited sample size and discuss its implication for the detection of two closely spaced signals. This motivates our heuristic definition of the 'effective number of identifiable signals.' Numerical simulations are used to demonstrate the consistency of the algorithm with respect to the effective number of signals and the superior performance of the algorithm with respect to Wax and Kailath's \"asymptotically consistent\" MDL based estimator.", ["Computational complexity theory", "Engineering", "Eigenvalues and eigenvectors", "Heuristic", "White noise", "Asymptote", "Sample size determination", "Consistent estimator", "Algorithm"]], ["Coalition Games with Cooperative Transmission: A Cure for the Curse of Boundary Nodes in Selfish Packet-Forwarding Wireless Networks", "In wireless packet-forwarding networks with selfish nodes, applications of a repeated game can induce the nodes to forward each others' packets, so that the network performance can be improved. However, the nodes on the boundary of such networks cannot benefit from this strategy, as the other nodes do not depend on them. This problem is sometimes known as the curse of the boundary nodes. To overcome this problem, an approach based on coalition games is proposed, in which the boundary nodes can use cooperative transmission to help the backbone nodes in the middle of the network. In return, the backbone nodes are willing to forward the boundary nodes' packets. The stability of the coalitions is studied using the concept of a core. Then two types of fairness, namely, the min-max fairness using nucleolus and the average fairness using the Shapley function are investigated. Finally, a protocol is designed using both repeated games and coalition games. Simulation results show how boundary nodes and backbone nodes form coalitions together according to different fairness criteria. The proposed protocol can improve the network connectivity by about 50%, compared with pure repeated game schemes.", ["Network packet", "Repeated game", "Nucleolus", "Wireless", "Wireless network", "Network performance", "Node (networking)", "Simulation", "Computer network"]], ["Straggler Identification in Round-Trip Data Streams via Newton's Identities and Invertible Bloom Filters", "We introduce the straggler identification problem, in which an algorithm must determine the identities of the remaining members of a set after it has had a large number of insertion and deletion operations performed on it, and now has relatively few remaining members. The goal is to do this in o(n) space, where n is the total number of identities. The straggler identification problem has applications, for example, in determining the set of unacknowledged packets in a high-bandwidth multicast data stream. We provide a deterministic solution to the straggler identification problem that uses only O(d log n) bits and is based on a novel application of Newton's identities for symmetric polynomials. This solution can identify any subset of d stragglers from a set of n O(log n)-bit identifiers, assuming that there are no false deletions of identities not already in the set. Indeed, we give a lower bound argument that shows that any small-space deterministic solution to the straggler identification problem cannot be guaranteed to handle false deletions. Nevertheless, we show that there is a simple randomized solution using O(d log n log(1/epsilon)) bits that can maintain a multiset and solve the straggler identification problem, tolerating false deletions, where epsilon>0 is a user-defined parameter bounding the probability of an incorrect response. This randomized solution is based on a new type of Bloom filter, which we call the invertible Bloom filter.", ["Newton's identities", "Bloom filter", "Multiset", "Algorithm", "Probability", "Data stream", "Bit", "Symmetric polynomial", "Multicast", "Upper and lower bounds", "Subset", "Network packet"]], ["Vocabulary growth in collaborative tagging systems", "We analyze a large-scale snapshot of del.icio.us and investigate how the number of different tags in the system grows as a function of a suitably defined notion of time. We study the temporal evolution of the global vocabulary size, i.e. the number of distinct tags in the entire system, as well as the evolution of local vocabularies, that is the growth of the number of distinct tags used in the context of a given resource or user. In both cases, we find power-law behaviors with exponents smaller than one. Surprisingly, the observed growth behaviors are remarkably regular throughout the entire history of the system and across very different resources being bookmarked. Similar sub-linear laws of growth have been observed in written text, and this qualitative universality calls for an explanation and points in the direction of non-trivial cognitive processes in the complex interaction patterns characterizing collaborative tagging.", ["Evolution", "Cognition", "Vocabulary", "Tag (metadata)", "Power law"]], ["Direct Optimization of Ranking Measures", "Web page ranking and collaborative filtering require the optimization of sophisticated performance measures. Current Support Vector approaches are unable to optimize them directly and focus on pairwise comparisons instead. We present a new approach which allows direct optimization of the relevant loss functions. This is achieved via structured estimation in Hilbert spaces. It is most related to Max-Margin-Markov networks optimization of multivariate performance measures. Key to our approach is that during training the ranking problem can be viewed as a linear assignment problem, which can be solved by the Hungarian Marriage algorithm. At test time, a sort operation is sufficient, as our algorithm assigns a relevance score to every (document, query) pair. Experiments show that the our algorithm is fast and that it works very well.", ["Collaborative filtering", "Assignment problem", "Mathematical optimization", "Hilbert space", "Loss function", "Markov random field", "David Hilbert", "Algorithm", "Hungary"]], ["Lifetime Improvement in Wireless Sensor Networks via Collaborative Beamforming and Cooperative Transmission", "Collaborative beamforming (CB) and cooperative transmission (CT) have recently emerged as communication techniques that can make effective use of collaborative/cooperative nodes to create a virtual multiple-input/multiple-output (MIMO) system. Extending the lifetime of networks composed of battery-operated nodes is a key issue in the design and operation of wireless sensor networks. This paper considers the effects on network lifetime of allowing closely located nodes to use CB/CT to reduce the load or even to avoid packet-forwarding requests to nodes that have critical battery life. First, the effectiveness of CB/CT in improving the signal strength at a faraway destination using energy in nearby nodes is studied. Then, the performance improvement obtained by this technique is analyzed for a special 2D disk case. Further, for general networks in which information-generation rates are fixed, a new routing problem is formulated as a linear programming problem, while for other general networks, the cost for routing is dynamically adjusted according to the amount of energy remaining and the effectiveness of CB/CT. From the analysis and the simulation results, it is seen that the proposed method can reduce the payloads of energy-depleting nodes by about 90% in the special case network considered and improve the lifetimes of general networks by about 10%, compared with existing techniques.", ["Beamforming", "Sensor", "MIMO", "Signal strength", "Linear programming", "Wireless", "Energy", "Simulation", "Wireless sensor network", "Order of the Bath", "Communication", "Routing"]], ["General-Purpose Computing on a Semantic Network Substrate", "This article presents a model of general-purpose computing on a semantic network substrate. The concepts presented are applicable to any semantic network representation. However, due to the standards and technological infrastructure devoted to the Semantic Web effort, this article is presented from this point of view. In the proposed model of computing, the application programming interface, the run-time program, and the state of the computing virtual machine are all represented in the Resource Description Framework (RDF). The implementation of the concepts presented provides a practical computing paradigm that leverages the highly-distributed and standardized representational-layer of the Semantic Web.", ["Application programming interface", "Semantic Web", "Virtual machine", "Semantic network", "Resource Description Framework", "Computing", "Paradigm", "Programming paradigm", "Infrastructure", "World Wide Web"]], ["Lifetime Improvement of Wireless Sensor Networks by Collaborative Beamforming and Cooperative Transmission", "Extending network lifetime of battery-operated devices is a key design issue that allows uninterrupted information exchange among distributive nodes in wireless sensor networks. Collaborative beamforming (CB) and cooperative transmission (CT) have recently emerged as new communication techniques that enable and leverage effective resource sharing among collaborative/cooperative nodes. In this paper, we seek to maximize the lifetime of sensor networks by using the new idea that closely located nodes can use CB/CT to reduce the load or even avoid packet forwarding requests to nodes that have critical battery life. First, we study the effectiveness of CB/CT to improve the signal strength at a faraway destination using energy in nearby nodes. Then, a 2D disk case is analyzed to assess the resulting performance improvement. For general networks, if information-generation rates are fixed, the new routing problem is formulated as a linear programming problem; otherwise, the cost for routing is dynamically adjusted according to the amount of energy remaining and the effectiveness of CB/CT. From the analysis and simulation results, it is seen that the proposed schemes can improve the lifetime by about 90% in the 2D disk network and by about 10% in the general networks, compared to existing schemes.", ["Beamforming", "Linear programming", "Wireless", "Sensor", "Energy", "Simulation", "Wireless sensor network", "Signal strength", "Order of the Bath", "Communication", "Routing"]], ["Cooperative Transmission Protocols with High Spectral Efficiency and High Diversity Order Using Multiuser Detection and Network Coding", "Cooperative transmission is an emerging communication technique that takes advantages of the broadcast nature of wireless channels. However, due to low spectral efficiency and the requirement of orthogonal channels, its potential for use in future wireless networks is limited. In this paper, by making use of multiuser detection (MUD) and network coding, cooperative transmission protocols with high spectral efficiency, diversity order, and coding gain are developed. Compared with the traditional cooperative transmission protocols with single-user detection, in which the diversity gain is only for one source user, the proposed MUD cooperative transmission protocols have the merits that the improvement of one user's link can also benefit the other users. In addition, using MUD at the relay provides an environment in which network coding can be employed. The coding gain and high diversity order can be obtained by fully utilizing the link between the relay and the destination. From the analysis and simulation results, it is seen that the proposed protocols achieve higher diversity gain, better asymptotic efficiency, and lower bit error rate, compared to traditional MUD and to existing cooperative transmission protocols.", ["Wireless", "Bit error rate", "Wireless network", "MUD", "Simulation", "Communication", "Forward error correction", "Bit", "Orthogonality", "Diversity scheme", "Spectral efficiency", "Computer network", "Streaming media", "Transmission (telecommunications)"]], ["Diversity-Multiplexing Tradeoff in Selective-Fading MIMO Channels", "We establish the optimal diversity-multiplexing (DM) tradeoff of coherent time, frequency and time-frequency selective-fading MIMO channels and provide a code design criterion for DM-tradeoff optimality. Our results are based on the analysis of the \"Jensen channel\" associated to a given selective-fading MIMO channel. While the original problem seems analytically intractable due to the mutual information being a sum of correlated random variables, the Jensen channel is equivalent to the original channel in the sense of the DM-tradeoff and lends itself nicely to analytical treatment. Finally, as a consequence of our results, we find that the classical rank criterion for space-time code design (in selective-fading MIMO channels) ensures optimality in the sense of the DM-tradeoff.", ["Frequency", "Mutual information", "MIMO", "Random variable", "Multiplexing", "Spacetime"]], ["Estimation Diversity and Energy Efficiency in Distributed Sensing", "Distributed estimation based on measurements from multiple wireless sensors is investigated. It is assumed that a group of sensors observe the same quantity in independent additive observation noises with possibly different variances. The observations are transmitted using amplify-and-forward (analog) transmissions over non-ideal fading wireless channels from the sensors to a fusion center, where they are combined to generate an estimate of the observed quantity. Assuming that the Best Linear Unbiased Estimator (BLUE) is used by the fusion center, the equal-power transmission strategy is first discussed, where the system performance is analyzed by introducing the concept of estimation outage and estimation diversity, and it is shown that there is an achievable diversity gain on the order of the number of sensors. The optimal power allocation strategies are then considered for two cases: minimum distortion under power constraints; and minimum power under distortion constraints. In the first case, it is shown that by turning off bad sensors, i.e., sensors with bad channels and bad observation quality, adaptive power gain can be achieved without sacrificing diversity gain. Here, the adaptive power gain is similar to the array gain achieved in Multiple-Input Single-Output (MISO) multi-antenna systems when channel conditions are known to the transmitter. In the second case, the sum power is minimized under zero-outage estimation distortion constraint, and some related energy efficiency issues in sensor networks are discussed.", ["Sensor", "Energy", "Transmitter", "Wireless", "System analysis", "Antenna (radio)", "Power transmission", "Efficient energy use", "Wireless sensor network", "MIMO", "Transmission (telecommunications)", "Observation", "Diversity scheme"]], ["The Trade-off between Processing Gains of an Impulse Radio UWB System in the Presence of Timing Jitter", "In time hopping impulse radio, $N_f$ pulses of duration $T_c$ are transmitted for each information symbol. This gives rise to two types of processing gain: (i) pulse combining gain, which is a factor $N_f$, and (ii) pulse spreading gain, which is $N_c=T_f/T_c$, where $T_f$ is the mean interval between two subsequent pulses. This paper investigates the trade-off between these two types of processing gain in the presence of timing jitter. First, an additive white Gaussian noise (AWGN) channel is considered and approximate closed form expressions for bit error probability are derived for impulse radio systems with and without pulse-based polarity randomization. Both symbol-synchronous and chip-synchronous scenarios are considered. The effects of multiple-access interference and timing jitter on the selection of optimal system parameters are explained through theoretical analysis. Finally, a multipath scenario is considered and the trade-off between processing gains of a synchronous impulse radio system with pulse-based polarity randomization is analyzed. The effects of the timing jitter, multiple-access interference and inter-frame interference are investigated. Simulation studies support the theoretical results.", ["Bit error rate", "Additive white Gaussian noise", "Jitter", "Simulation", "Probability", "Randomization", "Bit", "Radio", "Gaussian noise"]], ["Bayesian approach to rough set", "This paper proposes an approach to training rough set models using Bayesian framework trained using Markov Chain Monte Carlo (MCMC) method. The prior probabilities are constructed from the prior knowledge that good rough set models have fewer rules. Markov Chain Monte Carlo sampling is conducted through sampling in the rough set granule space and Metropolis algorithm is used as an acceptance criteria. The proposed method is tested to estimate the risk of HIV given demographic data. The results obtained shows that the proposed approach is able to achieve an average accuracy of 58% with the accuracy varying up to 66%. In addition the Bayesian rough set give the probabilities of the estimated HIV status as well as the linguistic rules describing how the demographic parameters drive the risk of HIV.", ["Markov chain Monte Carlo", "HIV", "Demographics", "Monte Carlo method", "Prior probability", "Algorithm", "Metropolis-Hastings algorithm", "Markov process", "Markov chain", "Probability", "Bayes' theorem", "Sampling (statistics)", "Bayesian probability", "Rough set", "Accuracy and precision", "Data"]], ["On sensing capacity of sensor networks for the class of linear observation, fixed SNR models", "In this paper we address the problem of finding the sensing capacity of sensor networks for a class of linear observation models and a fixed SNR regime. Sensing capacity is defined as the maximum number of signal dimensions reliably identified per sensor observation. In this context sparsity of the phenomena is a key feature that determines sensing capacity. Precluding the SNR of the environment the effect of sparsity on the number of measurements required for accurate reconstruction of a sparse phenomena has been widely dealt with under compressed sensing. Nevertheless the development there was motivated from an algorithmic perspective. In this paper our aim is to derive these bounds in an information theoretic set-up and thus provide algorithm independent conditions for reliable reconstruction of sparse signals. In this direction we first generalize the Fano's inequality and provide lower bounds to the probability of error in reconstruction subject to an arbitrary distortion criteria. Using these lower bounds to the probability of error, we derive upper bounds to sensing capacity and show that for fixed SNR regime sensing capacity goes down to zero as sparsity goes down to zero. This means that disproportionately more sensors are required to monitor very sparse events. Our next main contribution is that we show the effect of sensing diversity on sensing capacity, an effect that has not been considered before. Sensing diversity is related to the effective \\emph{coverage} of a sensor with respect to the field. In this direction we show the following results (a) Sensing capacity goes down as sensing diversity per sensor goes down; (b) Random sampling (coverage) of the field by sensors is better than contiguous location sampling (coverage).", ["Compressed sensing", "Sensor", "Information theory", "Wireless sensor network", "Random sampling", "Probability", "Distortion"]], ["An Adaptive Strategy for the Classification of G-Protein Coupled Receptors", "One of the major problems in computational biology is the inability of existing classification models to incorporate expanding and new domain knowledge. This problem of static classification models is addressed in this paper by the introduction of incremental learning for problems in bioinformatics. Many machine learning tools have been applied to this problem using static machine learning structures such as neural networks or support vector machines that are unable to accommodate new information into their existing models. We utilize the fuzzy ARTMAP as an alternate machine learning system that has the ability of incrementally learning new data as it becomes available. The fuzzy ARTMAP is found to be comparable to many of the widespread machine learning systems. The use of an evolutionary strategy in the selection and combination of individual classifiers into an ensemble system, coupled with the incremental learning ability of the fuzzy ARTMAP is proven to be suitable as a pattern classifier. The algorithm presented is tested using data from the G-Coupled Protein Receptors Database and shows good accuracy of 83%. The system presented is also generally applicable, and can be used in problems in genomics and proteomics.", ["Bioinformatics", "Machine learning", "Neural network", "Computational biology", "Domain knowledge", "Protein", "Proteomics", "Receptor (biochemistry)", "Support vector machine", "Genomics"]], ["Polynomial algorithms for protein similarity search for restricted mRNA structures", "In this paper we consider the problem of computing an mRNA sequence of maximal similarity for a given mRNA of secondary structure constraints, introduced by Backofen et al. in [BNS02] denoted as the MRSO problem. The problem is known to be NP-complete for planar associated implied structure graphs of vertex degree at most 3. In [BFHV05] a first polynomial dynamic programming algorithms for MRSO on implied structure graphs with maximum vertex degree 3 of bounded cut-width is shown. We give a simple but more general polynomial dynamic programming solution for the MRSO problem for associated implied structure graphs of bounded clique-width. Our result implies that MRSO is polynomial for graphs of bounded tree-width, co-graphs, $P_4$-sparse graphs, and distance hereditary graphs. Further we conclude that the problem of comparing two solutions for MRSO is hard for the class of problems which can be solved in polynomial time with a number of parallel queries to an oracle in NP.", ["NP-complete", "Dynamic programming", "Graph (mathematics)", "Polynomial time", "Tree decomposition", "Secondary structure", "Protein", "Algorithm", "Parallel computing", "Computing", "Vertex (graph theory)", "Messenger RNA", "Oracle", "Dense graph", "Nearest neighbor search", "Tree (graph theory)", "Polynomial"]], ["Une plate-forme dynamique pour l'\\'evaluation des performances des bases de donn\\'ees \\`a objets", "In object-oriented or object-relational databases such as multimedia databases or most XML databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. However, this has been the way these databases and associated optimisation techniques such as clustering have been evaluated up to now. This paper opens up research regarding this issue by proposing a dynamic object evaluation framework (DOEF). DOEF accomplishes access pattern change by defining configurable styles of change. It is a preliminary prototype that has been designed to be open and fully extensible. Though originally designed for the object-oriented model, it can also be used within the object-relational model with few adaptations. Furthermore, new access pattern change models can be added too. To illustrate the capabilities of DOEF, we conducted two different sets of experiments. In the first set of experiments, we used DOEF to compare the performances of four state of the art dynamic clustering algorithms. The results show that DOEF is effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern. They also led us to conclude that dynamic clustering algorithms can cope with moderate levels of access pattern change, but that performance rapidly degrades to be worse than no clustering when vigorous styles of access pattern change are applied. In the second set of experiments, we used DOEF to compare the performance of two different object stores: Platypus and SHORE. The use of DOEF exposed the poor swapping performance of Platypus.", ["Relational model", "Object-oriented programming", "XML", "Relational database", "Object-relational impedance mismatch", "Mathematical optimization", "Database", "Object (computer science)", "XML database", "Cluster analysis", "Prototype", "Algorithm", "Platypus"]], ["Conception d'un banc d'essais d\\'ecisionnel", "We present in this paper a new benchmark for evaluating the performances of data warehouses. Benchmarking is useful either to system users for comparing the performances of different systems, or to system engineers for testing the effect of various design choices. While the TPC (Transaction Processing Performance Council) standard benchmarks address the first point, they are not tuneable enough to address the second one. Our Data Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses and workloads. DWEB is fully parameterized. However, two levels of parameterization keep it easy to tune. Since DWEB mainly meets engineering benchmarking needs, it is complimentary to the TPC standard benchmarks, and not a competitor. Finally, DWEB is implemented as a Java free software that can be interfaced with most existing relational database management systems.", ["Free software", "Data", "Engineering", "Transaction Processing Performance Council", "Database", "Relational database management system", "Ad hoc", "Data warehouse", "Relational database", "Java (programming language)", "Database management system", "Benchmarking", "Computer software"]], ["Comparing Robustness of Pairwise and Multiclass Neural-Network Systems for Face Recognition", "Noise, corruptions and variations in face images can seriously hurt the performance of face recognition systems. To make such systems robust, multiclass neuralnetwork classifiers capable of learning from noisy data have been suggested. However on large face data sets such systems cannot provide the robustness at a high level. In this paper we explore a pairwise neural-network system as an alternative approach to improving the robustness of face recognition. In our experiments this approach is shown to outperform the multiclass neural-network system in terms of the predictive accuracy on the face images corrupted by noise.", ["Neural network"]], ["Vers l'auto-administration des entrep\\^ots de donn\\'ees", "With the wide development of databases in general and data warehouses in particular, it is important to reduce the tasks that a database administrator must perform manually. The idea of using data mining techniques to extract useful knowledge for administration from the data themselves has existed for some years. However, little research has been achieved. The aim of this study is to search for a way of extracting useful knowledge from stored data to automatically apply performance optimization techniques, and more particularly indexing techniques. We have designed a tool that extracts frequent itemsets from a given workload to compute an index configuration that helps optimizing data access time. The experiments we performed showed that the index configurations generated by our tool allowed performance gains of 15% to 25% on a test database and a test data warehouse.", ["Mathematical optimization", "Data mining", "Database administrator", "Data warehouse", "Database", "Warehouse"]], ["$\\delta$-sequences and Evaluation Codes defined by Plane Valuations at Infinity", "We introduce the concept of $\\delta$-sequence. A $\\delta$-sequence $\\Delta$ generates a well-ordered semigroup $S$ in $\\mathbb{Z}^2$ or $\\mathbb{R}$. We show how to construct (and compute parameters) for the dual code of any evaluation code associated with a weight function defined by $\\Delta$ from the polynomial ring in two indeterminates to a semigroup $S$ as above. We prove that this is a simple procedure which can be understood by considering a particular class of valuations of function fields of surfaces, called plane valuations at infinity. We also give algorithms to construct an unlimited number of $\\delta$-sequences of the different existing types, and so this paper provides the tools to know and use a new large set of codes.", ["Polynomial ring", "Weight function", "Semigroup", "Polynomial", "Function (mathematics)", "Indeterminate (variable)", "River delta"]], ["Simulating spin systems on IANUS, an FPGA-based computer", "We describe the hardwired implementation of algorithms for Monte Carlo simulations of a large class of spin models. We have implemented these algorithms as VHDL codes and we have mapped them onto a dedicated processor based on a large FPGA device. The measured performance on one such processor is comparable to O(100) carefully programmed high-end PCs: it turns out to be even better for some selected spin models. We describe here codes that we are currently executing on the IANUS massively parallel FPGA-based system.", ["Field-programmable gate array", "VHDL", "Computer", "Monte Carlo method", "Central processing unit", "Hardwired control"]], ["On Energy Efficient Hierarchical Cross-Layer Design: Joint Power Control and Routing for Ad Hoc Networks", "In this paper, a hierarchical cross-layer design approach is proposed to increase energy efficiency in ad hoc networks through joint adaptation of nodes' transmitting powers and route selection. The design maintains the advantages of the classic OSI model, while accounting for the cross-coupling between layers, through information sharing. The proposed joint power control and routing algorithm is shown to increase significantly the overall energy efficiency of the network, at the expense of a moderate increase in complexity. Performance enhancement of the joint design using multiuser detection is also investigated, and it is shown that the use of multiuser detection can increase the capacity of the ad hoc network significantly for a given level of energy consumption.", ["OSI model", "Wireless ad hoc network", "Algorithm", "Ad hoc", "Energy", "Accountancy", "Hierarchy"]], ["Capacity of a Class of Modulo-Sum Relay Channels", "This paper characterizes the capacity of a class of modulo additive noise relay channels, in which the relay observes a corrupted version of the noise and has a separate channel to the destination. The capacity is shown to be strictly below the cut-set bound in general and achievable using a quantize-and-forward strategy at the relay. This result confirms a conjecture by Ahlswede and Han about the capacity of channels with rate limited state information at the destination for this particular class of channels.", ["Additive white Gaussian noise", "Modular arithmetic", "Quantization (physics)", "Conjecture"]], ["Rough Sets Computations to Impute Missing Data", "Many techniques for handling missing data have been proposed in the literature. Most of these techniques are overly complex. This paper explores an imputation technique based on rough set computations. In this paper, characteristic relations are introduced to describe incompletely specified decision tables.It is shown that the basic rough set idea of lower and upper approximations for incompletely specified decision tables may be defined in a variety of different ways. Empirical results obtained using real data are given and they provide a valuable and promising insight to the problem of missing data. Missing data were predicted with an accuracy of up to 99%.", ["Missing data", "Literature", "Decision table", "Data", "Rough set"]], ["Sabbath Day Home Automation: \"It's Like Mixing Technology and Religion\"", "We present a qualitative study of 20 American Orthodox Jewish families' use of home automation for religious purposes. These lead users offer insight into real-life, long-term experience with home automation technologies. We discuss how automation was seen by participants to contribute to spiritual experience and how participants oriented to the use of automation as a religious custom. We also discuss the relationship of home automation to family life. We draw design implications for the broader population, including surrender of control as a design resource, home technologies that support long-term goals and lifestyle choices, and respite from technology.", ["Religion", "Judaism", "Orthodox Judaism", "Religious experience", "United States", "Technology", "Shabbat", "Home automation"]], ["Capacity Gain from Two-Transmitter and Two-Receiver Cooperation", "Capacity improvement from transmitter and receiver cooperation is investigated in a two-transmitter, two-receiver network with phase fading and full channel state information available at all terminals. The transmitters cooperate by first exchanging messages over an orthogonal transmitter cooperation channel, then encoding jointly with dirty paper coding. The receivers cooperate by using Wyner-Ziv compress-and-forward over an analogous orthogonal receiver cooperation channel. To account for the cost of cooperation, the allocation of network power and bandwidth among the data and cooperation channels is studied. It is shown that transmitter cooperation outperforms receiver cooperation and improves capacity over non-cooperative transmission under most operating conditions when the cooperation channel is strong. However, a weak cooperation channel limits the transmitter cooperation rate; in this case receiver cooperation is more advantageous. Transmitter-and-receiver cooperation offers sizable additional capacity gain over transmitter-only cooperation at low SNR, whereas at high SNR transmitter cooperation alone captures most of the cooperative capacity improvement.", ["Orthogonality", "Channel state information", "Signal-to-noise ratio", "Transmission (telecommunications)", "Data", "Receiver (radio)", "Channel (communications)", "Phase (waves)", "Fading", "Transmitter", "Forward error correction", "Dirty paper coding", "Data compression"]], ["Lower Bounds on Implementing Robust and Resilient Mediators", "We consider games that have (k,t)-robust equilibria when played with a mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by coalitions of size up to k and deviations by up to $t$ players with unknown utilities. We prove lower bounds that match upper bounds on the ability to implement such mediators using cheap talk (that is, just allowing communication among the players). The bounds depend on (a) the relationship between k, t, and n, the total number of players in the system; (b) whether players know the exact utilities of other players; (c) whether there are broadcast channels or just point-to-point channels; (d) whether cryptography is available; and (e) whether the game has a $k+t)-punishment strategy; that is, a strategy that, if used by all but at most $k+t$ players, guarantees that every player gets a worse outcome than they do with the equilibrium strategy.", ["Mediation", "Cryptography"]], ["Evaluating Personal Archiving Strategies for Internet-based Information", "Internet-based personal digital belongings present different vulnerabilities than locally stored materials. We use responses to a survey of people who have recovered lost websites, in combination with supplementary interviews, to paint a fuller picture of current curatorial strategies and practices. We examine the types of personal, topical, and commercial websites that respondents have lost and the reasons they have lost this potentially valuable material. We further explore what they have tried to recover and how the loss influences their subsequent practices. We found that curation of personal digital materials in online stores bears some striking similarities to the curation of similar materials stored locally in that study participants continue to archive personal assets by relying on a combination of benign neglect, sporadic backups, and unsystematic file replication. However, we have also identified issues specific to Internet-based material: how risk is spread by distributing the files among multiple servers and services; the circular reasoning participants use when they discuss the safety of their digital assets; and the types of online material that are particularly vulnerable to loss. The study reveals ways in which expectations of permanence and notification are violated and situations in which benign neglect has far greater consequences for the long-term fate of important digital assets.", ["Internet", "Vulnerability (computing)"]], ["The Long Term Fate of Our Digital Belongings: Toward a Service Model for Personal Archives", "We conducted a preliminary field study to understand the current state of personal digital archiving in practice. Our aim is to design a service for the long-term storage, preservation, and access of digital belongings by examining how personal archiving needs intersect with existing and emerging archiving technologies, best practices, and policies. Our findings not only confirmed that experienced home computer users are creating, receiving, and finding an increasing number of digital belongings, but also that they have already lost irreplaceable digital artifacts such as photos, creative efforts, and records. Although participants reported strategies such as backup and file replication for digital safekeeping, they were seldom able to implement them consistently. Four central archiving themes emerged from the data: (1) people find it difficult to evaluate the worth of accumulated materials; (2) personal storage is highly distributed both on- and offline; (3) people are experiencing magnified curatorial problems associated with managing files in the aggregate, creating appropriate metadata, and migrating materials to maintainable formats; and (4) facilities for long-term access are not supported by the current desktop metaphor. Four environmental factors further complicate archiving in consumer settings: the pervasive influence of malware; consumer reliance on ad hoc IT providers; an accretion of minor system and registry inconsistencies; and strong consumer beliefs about the incorruptibility of digital forms, the reliability of digital technologies, and the social vulnerability of networked storage.", ["Malware", "Desktop metaphor", "Metadata", "Metaphor", "Ad hoc", "Information technology", "Document management system", "Best practice", "Consumer", "Computer", "Home computer"]], ["An Automated Evaluation Metric for Chinese Text Entry", "In this paper, we propose an automated evaluation metric for text entry. We also consider possible improvements to existing text entry evaluation metrics, such as the minimum string distance error rate, keystrokes per character, cost per correction, and a unified approach proposed by MacKenzie, so they can accommodate the special characteristics of Chinese text. Current methods lack an integrated concern about both typing speed and accuracy for Chinese text entry evaluation. Our goal is to remove the bias that arises due to human factors. First, we propose a new metric, called the correction penalty (P), based on Fitts' law and Hick's law. Next, we transform it into the approximate amortized cost (AAC) of information theory. An analysis of the AAC of Chinese text input methods with different context lengths is also presented.", ["Hick's law", "Fitts's law", "Information theory", "Human factors"]], ["On the Development of Text Input Method - Lessons Learned", "Intelligent Input Methods (IM) are essential for making text entries in many East Asian scripts, but their application to other languages has not been fully explored. This paper discusses how such tools can contribute to the development of computer processing of other oriental languages. We propose a design philosophy that regards IM as a text service platform, and treats the study of IM as a cross disciplinary subject from the perspectives of software engineering, human-computer interaction (HCI), and natural language processing (NLP). We discuss these three perspectives and indicate a number of possible future research directions.", ["Software engineering", "Languages of Asia", "Natural language processing", "Engineering", "Human-computer interaction", "Philosophy", "Natural language", "Input method", "Computer", "East Asia", "Race and ethnicity in the United States Census"]], ["Periodicity of certain piecewise affine planar maps", "We determine periodic and aperiodic points of certain piecewise affine maps in the Euclidean plane. Using these maps, we prove for $\\lambda\\in\\{\\frac{\\pm1\\pm\\sqrt5}2,\\pm\\sqrt2,\\pm\\sqrt3\\}$ that all integer sequences $(a_k)_{k\\in\\mathbb Z}$ satisfying $0\\le a_{k-1}+\\lambda a_k+a_{k+1}<1$ are periodic.", ["Euclidean geometry", "Piecewise", "Plane (geometry)", "Periodic function", "Euclidean space", "Integer"]], ["The Complexity of Weighted Boolean #CSP", "This paper gives a dichotomy theorem for the complexity of computing the partition function of an instance of a weighted Boolean constraint satisfaction problem. The problem is parameterised by a finite set F of non-negative functions that may be used to assign weights to the configurations (feasible solutions) of a problem instance. Classical constraint satisfaction problems correspond to the special case of 0,1-valued functions. We show that the partition function, i.e. the sum of the weights of all configurations, can be computed in polynomial time if either (1) every function in F is of ``product type'', or (2) every function in F is ``pure affine''. For every other fixed set F, computing the partition function is FP^{#P}-complete.", ["Constraint satisfaction problem", "Polynomial", "Polynomial time", "Complexity", "Constraint satisfaction", "Dichotomy", "Computing", "Finite set", "Function (mathematics)", "Constraint (mathematics)", "Non-negative"]], ["Network statistics on early English Syntax: Structural criteria", "This paper includes a reflection on the role of networks in the study of English language acquisition, as well as a collection of practical criteria to annotate free-speech corpora from children utterances. At the theoretical level, the main claim of this paper is that syntactic networks should be interpreted as the outcome of the use of the syntactic machinery. Thus, the intrinsic features of such machinery are not accessible directly from (known) network properties. Rather, what one can see are the global patterns of its use and, thus, a global view of the power and organization of the underlying grammar. Taking a look into more practical issues, the paper examines how to build a net from the projection of syntactic relations. Recall that, as opposed to adult grammars, early-child language has not a well-defined concept of structure. To overcome such difficulty, we develop a set of systematic criteria assuming constituency hierarchy and a grammar based on lexico-thematic relations. At the end, what we obtain is a well defined corpora annotation that enables us i) to perform statistics on the size of structures and ii) to build a network from syntactic relations over which we can perform the standard measures of complexity. We also provide a detailed example.", ["Statistics", "Annotation", "Language acquisition", "Freedom of speech", "Hierarchy", "Thematic relation", "Language", "Grammar"]], ["Distributed Algorithms for Spectrum Allocation, Power Control, Routing, and Congestion Control in Wireless Networks", "We develop distributed algorithms to allocate resources in multi-hop wireless networks with the aim of minimizing total cost. In order to observe the fundamental duplexing constraint that co-located transmitters and receivers cannot operate simultaneously on the same frequency band, we first devise a spectrum allocation scheme that divides the whole spectrum into multiple sub-bands and activates conflict-free links on each sub-band. We show that the minimum number of required sub-bands grows asymptotically at a logarithmic rate with the chromatic number of network connectivity graph. A simple distributed and asynchronous algorithm is developed to feasibly activate links on the available sub-bands. Given a feasible spectrum allocation, we then design node-based distributed algorithms for optimally controlling the transmission powers on active links for each sub-band, jointly with traffic routes and user input rates in response to channel states and traffic demands. We show that under specified conditions, the algorithms asymptotically converge to the optimal operating point.", ["Frequency", "Chromatic number", "Distributed algorithms", "Routing", "Wireless", "Connectivity (graph theory)", "Network congestion", "Mesh networking", "Duplex (telecommunications)", "Wireless network", "Distributed computing", "Biasing", "Frequency allocation", "Electromagnetic spectrum"]], ["Avoiding Rotated Bitboards with Direct Lookup", "This paper describes an approach for obtaining direct access to the attacked squares of sliding pieces without resorting to rotated bitboards. The technique involves creating four hash tables using the built in hash arrays from an interpreted, high level language. The rank, file, and diagonal occupancy are first isolated by masking the desired portion of the board. The attacked squares are then directly retrieved from the hash tables. Maintaining incrementally updated rotated bitboards becomes unnecessary as does all the updating, mapping and shifting required to access the attacked squares. Finally, rotated bitboard move generation speed is compared with that of the direct hash table lookup method.", ["Hash table", "Bitboard", "High-level programming language", "Array data structure", "Hash function", "Lookup table"]], ["Stochastic Optimization Algorithms", "When looking for a solution, deterministic methods have the enormous advantage that they do find global optima. Unfortunately, they are very CPU-intensive, and are useless on untractable NP-hard problems that would require thousands of years for cutting-edge computers to explore. In order to get a result, one needs to revert to stochastic algorithms, that sample the search space without exploring it thoroughly. Such algorithms can find very good results, without any guarantee that the global optimum has been reached; but there is often no other choice than using them. This chapter is a short introduction to the main methods used in stochastic optimization.", ["NP-hard", "Stochastic optimization", "Global optimum", "Algorithm", "Central processing unit", "Stochastic", "Algorithmic composition", "Mathematical optimization"]], ["Minimizing Unsatisfaction in Colourful Neighbourhoods", "Colouring sparse graphs under various restrictions is a theoretical problem of significant practical relevance. Here we consider the problem of maximizing the number of different colours available at the nodes and their neighbourhoods, given a predetermined number of colours. In the analytical framework of a tree approximation, carried out at both zero and finite temperatures, solutions obtained by population dynamics give rise to estimates of the threshold connectivity for the incomplete to complete transition, which are consistent with those of existing algorithms. The nature of the transition as well as the validity of the tree approximation are investigated.", ["Population dynamics", "Finite set", "Graph (mathematics)", "Population"]], ["A Game-Theoretic Approach to Energy-Efficient Modulation in CDMA Networks with Delay Constraints", "A game-theoretic framework is used to study the effect of constellation size on the energy efficiency of wireless networks for M-QAM modulation. A non-cooperative game is proposed in which each user seeks to choose its transmit power (and possibly transmit symbol rate) as well as the constellation size in order to maximize its own utility while satisfying its delay quality-of-service (QoS) constraint. The utility function used here measures the number of reliable bits transmitted per joule of energy consumed, and is particularly suitable for energy-constrained networks. The best-response strategies and Nash equilibrium solution for the proposed game are derived. It is shown that in order to maximize its utility (in bits per joule), a user must choose the lowest constellation size that can accommodate the user's delay constraint. Using this framework, the tradeoffs among energy efficiency, delay, throughput and constellation size are also studied and quantified. The effect of trellis-coded modulation on energy efficiency is also discussed.", ["Joule", "Nash equilibrium", "Constellation", "Modulation", "Symbol rate", "Perfect competition", "Quadrature amplitude modulation", "Trellis modulation", "Energy", "Wireless network", "Non-cooperative game", "Game theory", "Wireless", "Code division multiple access", "Quality of service", "Efficient energy use", "Throughput", "Utility"]], ["Energy-Efficient Resource Allocation in Wireless Networks with Quality-of-Service Constraints", "A game-theoretic model is proposed to study the cross-layer problem of joint power and rate control with quality of service (QoS) constraints in multiple-access networks. In the proposed game, each user seeks to choose its transmit power and rate in a distributed manner in order to maximize its own utility while satisfying its QoS requirements. The user's QoS constraints are specified in terms of the average source rate and an upper bound on the average delay where the delay includes both transmission and queuing delays. The utility function considered here measures energy efficiency and is particularly suitable for wireless networks with energy constraints. The Nash equilibrium solution for the proposed non-cooperative game is derived and a closed-form expression for the utility achieved at equilibrium is obtained. It is shown that the QoS requirements of a user translate into a \"size\" for the user which is an indication of the amount of network resources consumed by the user. Using this competitive multiuser framework, the tradeoffs among throughput, delay, network capacity and energy efficiency are studied. In addition, analytical expressions are given for users' delay profiles and the delay performance of the users at Nash equilibrium is quantified.", ["Non-cooperative game", "Nash equilibrium", "Closed-form expression", "Wireless network", "Wireless", "Perfect competition", "Cooperative game", "Queuing delay", "Game theory", "Energy", "Quality of service", "Channel access method", "Utility", "Throughput"]], ["A Unified Approach to Energy-Efficient Power Control in Large CDMA Systems", "A unified approach to energy-efficient power control is proposed for code-division multiple access (CDMA) networks. The approach is applicable to a large family of multiuser receivers including the matched filter, the decorrelator, the linear minimum mean-square error (MMSE) receiver, and the (nonlinear) optimal detectors. It exploits the linear relationship that has been shown to exist between the transmit power and the output signal-to-interference-plus-noise ratio (SIR) in the large-system limit. It is shown that, for this family of receivers, when users seek to selfishly maximize their own energy efficiency, the Nash equilibrium is SIR-balanced. In addition, a unified power control (UPC) algorithm for reaching the Nash equilibrium is proposed. The algorithm adjusts the user's transmit powers by iteratively computing the large-system multiuser efficiency, which is independent of instantaneous spreading sequences. The convergence of the algorithm is proved for the matched filter, the decorrelator, and the MMSE receiver, and is demonstrated by means of simulation for an optimal detector. Moreover, the performance of the algorithm in finite-size systems is studied and compared with that of a conventional power control scheme, in which user powers depend on the instantaneous spreading sequences.", ["Minimum mean square error", "Nash equilibrium", "Matched filter", "Energy", "Signal (electronics)", "Algorithm", "Nonlinear system", "Simulation", "Channel access method", "Computing", "Code division multiple access", "Linear"]], ["A Note on Ontology and Ordinary Language", "We argue for a compositional semantics grounded in a strongly typed ontology that reflects our commonsense view of the world and the way we talk about it. Assuming such a structure we show that the semantics of various natural language phenomena may become nearly trivial.", ["Ontology", "Strong typing", "Semantics", "Natural language"]], ["An algorithm for clock synchronization with the gradient property in sensor networks", "We introduce a distributed algorithm for clock synchronization in sensor networks. Our algorithm assumes that nodes in the network only know their immediate neighborhoods and an upper bound on the network's diameter. Clock-synchronization messages are only sent as part of the communication, assumed reasonably frequent, that already takes place among nodes. The algorithm has the gradient property of [2], achieving an O(1) worst-case skew between the logical clocks of neighbors. As in the case of [3,8], the algorithm's actions are such that no constant lower bound exists on the rate at which logical clocks progress in time, and for this reason the lower bound of [2,5] that forbids constant skew between neighbors does not apply.", ["Synchronization", "Sensor", "Distributed algorithms", "Communication", "Upper and lower bounds", "Clock synchronization", "Algorithm"]], ["Acyclic Preference Systems in P2P Networks", "In this work we study preference systems natural for the Peer-to-Peer paradigm. Most of them fall in three categories: global, symmetric and complementary. All these systems share an acyclicity property. As a consequence, they admit a stable (or Pareto efficient) configuration, where no participant can collaborate with better partners than their current ones. We analyze the representation of the such preference systems and show that any acyclic system can be represented with a symmetric mark matrix. This gives a method to merge acyclic preference systems and retain the acyclicity. We also consider such properties of the corresponding collaboration graph, as clustering coefficient and diameter. In particular, studying the example of preferences based on real latency measurements, we observe that its stable configuration is a small-world graph.", ["Pareto efficiency", "Clustering coefficient", "Peer-to-peer", "Matrix (mathematics)", "Paradigm", "Graph (mathematics)", "Cluster analysis"]], ["Ensemble Learning for Free with Evolutionary Algorithms ?", "Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-line) or incrementally along evolution (On-line). Experiments on a set of benchmark problems show that Off-line outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.", ["Fitness function", "Evolution", "Boosting", "Coevolution", "Evolutionary computation", "Evolutionary algorithm", "Hypothesis", "Machine learning", "Statistical classification"]], ["The Complexity of Model Checking Higher-Order Fixpoint Logic", "Higher-Order Fixpoint Logic (HFL) is a hybrid of the simply typed \\lambda-calculus and the modal \\lambda-calculus. This makes it a highly expressive temporal logic that is capable of expressing various interesting correctness properties of programs that are not expressible in the modal \\lambda-calculus. This paper provides complexity results for its model checking problem. In particular we consider those fragments of HFL built by using only types of bounded order k and arity m. We establish k-fold exponential time completeness for model checking each such fragment. For the upper bound we use fixpoint elimination to obtain reachability games that are singly-exponential in the size of the formula and k-fold exponential in the size of the underlying transition system. These games can be solved in deterministic linear time. As a simple consequence, we obtain an exponential time upper bound on the expression complexity of each such fragment. The lower bound is established by a reduction from the word problem for alternating (k-1)-fold exponential space bounded Turing Machines. Since there are fixed machines of that type whose word problems are already hard with respect to k-fold exponential time, we obtain, as a corollary, k-fold exponential time completeness for the data complexity of our fragments of HFL, provided m exceeds 3. This also yields a hierarchy result in expressive power.", ["Temporal logic", "Word problem for groups", "Upper and lower bounds", "Arity", "Calculus", "Linear time", "Model checking", "Lambda calculus", "Time complexity", "State transition system", "Fixed point (mathematics)", "Expressive power", "EXPSPACE", "Complexity", "Turing machine", "Hierarchy", "Completeness", "Corollary"]], ["Diversity of MIMO Multihop Relay Channels - Part I: Amplify-and-Forward", "In this two-part paper, we consider the multiantenna multihop relay channels in which the source signal arrives at the destination through N independent relaying hops in series. The main concern of this work is to design relaying strategies that utilize efficiently the relays in such a way that the diversity is maximized. In part I, we focus on the amplify-and-forward (AF) strategy with which the relays simply scale the received signal and retransmit it. More specifically, we characterize the diversity-multiplexing tradeoff (DMT) of the AF scheme in a general multihop channel with arbitrary number of antennas and arbitrary number of hops. The DMT is in closed-form expression as a function of the number of antennas at each node. First, we provide some basic results on the DMT of the general Rayleigh product channels. It turns out that these results have very simple and intuitive interpretation. Then, the results are applied to the AF multihop channels which is shown to be equivalent to the Rayleigh product channel, in the DMT sense. Finally, the project-and-forward (PF) scheme, a variant of the AF scheme, is proposed. We show that the PF scheme has the same DMT as the AF scheme, while the PF can have significant power gain over the AF scheme in some cases. In part II, we will derive the upper bound on the diversity of the multihop channels and show that it can be achieved by partitioning the multihop channel into AF subchannels.", ["Closed-form expression", "Orthogonal frequency-division multiplexing", "MIMO"]], ["Critical phenomena in complex networks", "The combination of the compactness of networks, featuring small diameters, and their complex architectures results in a variety of critical effects dramatically different from those in cooperative systems on lattices. In the last few years, researchers have made important steps toward understanding the qualitatively new critical phenomena in complex networks. We review the results, concepts, and methods of this rapidly developing field. Here we mostly consider two closely related classes of these critical phenomena, namely structural phase transitions in the network architectures and transitions in cooperative models on networks as substrates. We also discuss systems where a network and interacting agents on it influence each other. We overview a wide range of critical phenomena in equilibrium and growing networks including the birth of the giant connected component, percolation, k-core percolation, phenomena near epidemic thresholds, condensation transitions, critical phenomena in spin models placed on networks, synchronization, and self-organized criticality effects in interacting systems on networks. We also discuss strong finite size effects in these systems and highlight open problems and perspectives.", ["Self-organized criticality", "Connected space", "Critical phenomena", "Percolation", "Compact space", "Complex network", "Enzyme substrate (biology)", "Degeneracy (graph theory)", "Self-organization", "Phase transition", "Condensation"]], ["Checking Equivalence of Quantum Circuits and States", "Quantum computing promises exponential speed-ups for important simulation and optimization problems. It also poses new CAD problems that are similar to, but more challenging, than the related problems in classical (non-quantum) CAD, such as determining if two states or circuits are functionally equivalent. While differences in classical states are easy to detect, quantum states, which are represented by complex-valued vectors, exhibit subtle differences leading to several notions of equivalence. This provides flexibility in optimizing quantum circuits, but leads to difficult new equivalence-checking issues for simulation and synthesis. We identify several different equivalence-checking problems and present algorithms for practical benchmarks, including quantum communication and search circuits, which are shown to be very fast and robust for hundreds of qubits.", ["Quantum computer", "Mathematical optimization", "Exponential growth", "Computer-aided design", "Simulation", "Quantum information science", "Qubit", "Complex number"]], ["Can the Internet cope with stress?", "When will the Internet become aware of itself? In this note the problem is approached by asking an alternative question: Can the Internet cope with stress? By extrapolating the psychological difference between coping and defense mechanisms a distributed software experiment is outlined which could reject the hypothesis that the Internet is not a conscious entity.", ["Defence mechanism", "Extrapolation"]], ["Joint Detection and Identification of an Unobservable Change in the Distribution of a Random Sequence", "This paper examines the joint problem of detection and identification of a sudden and unobservable change in the probability distribution function (pdf) of a sequence of independent and identically distributed (i.i.d.) random variables to one of finitely many alternative pdf's. The objective is quick detection of the change and accurate inference of the ensuing pdf. Following a Bayesian approach, a new sequential decision strategy for this problem is revealed and is proven optimal. Geometrical properties of this strategy are demonstrated via numerical examples.", ["Probability distribution", "Random sequence", "Independent and identically distributed random variables", "Decision theory", "Inference", "Independence (probability theory)", "Random variable", "Variable (mathematics)", "Probability", "Randomness", "Probability distribution function", "Probability density function", "Bayesian probability", "Function (mathematics)"]], ["Reliable Memories Built from Unreliable Components Based on Expander Graphs", "In this paper, memories built from components subject to transient faults are considered. A fault-tolerant memory architecture based on low-density parity-check codes is proposed and the existence of reliable memories for the adversarial failure model is proved. The proof relies on the expansion property of the underlying Tanner graph of the code. An equivalence between the Taylor-Kuznetsov (TK) scheme and Gallager B algorithm is established and the results are extended to the independent failure model. It is also shown that the proposed memory architecture has lower redundancy compared to the TK scheme. The results are illustrated with specific numerical examples.", ["Low-density parity-check code", "Fault-tolerant design", "Algorithm", "Architecture", "Redundancy (engineering)"]], ["Constructions of q-Ary Constant-Weight Codes", "This paper introduces a new combinatorial construction for q-ary constant-weight codes which yields several families of optimal codes and asymptotically optimal codes. The construction reveals intimate connection between q-ary constant-weight codes and sets of pairwise disjoint combinatorial designs of various types.", ["Combinatorics", "Asymptotically optimal algorithm", "Disjoint sets", "Asymptote"]], ["An efficient centralized binary multicast network coding algorithm for any cyclic network", "We give an algorithm for finding network encoding and decoding equations for error-free multicasting networks with multiple sources and sinks. The algorithm given is efficient (polynomial complexity) and works on any kind of network (acyclic, link cyclic, flow cyclic, or even in the presence of knots). The key idea will be the appropriate use of the delay (both natural and additional) during the encoding. The resulting code will always work with finite delay with binary encoding coefficients.", ["Polynomial time", "Binary numeral system", "Multicast", "Algorithm", "Polynomial", "Code", "Finite set"]], ["About the domino problem in the hyperbolic plane, a new solution: complement", "In this paper, we complete the construction of paper arXiv:cs.CG/0701096v2. Together with the proof contained in arXiv:cs.CG/0701096v2, this paper definitely proves that the general problem of tiling the hyperbolic plane with {\\it \\`a la} Wang tiles is undecidable.", ["Hyperbolic geometry", "Plane (geometry)", "Wang tile", "ArXiv", "Undecidable problem"]], ["An Energy Efficiency Perspective on Training for Fading Channels", "In this paper, the bit energy requirements of training-based transmission over block Rayleigh fading channels are studied. Pilot signals are employed to obtain the minimum mean-square-error (MMSE) estimate of the channel fading coefficients. Energy efficiency is analyzed in the worst case scenario where the channel estimate is assumed to be perfect and the error in the estimate is considered as another source of additive Gaussian noise. It is shown that bit energy requirement grows without bound as the snr goes to zero, and the minimum bit energy is achieved at a nonzero snr value below which one should not operate. The effect of the block length on both the minimum bit energy and the snr value at which the minimum is achieved is investigated. Flash training schemes are analyzed and shown to improve the energy efficiency in the low-snr regime. Energy efficiency analysis is also carried out when peak power constraints are imposed on pilot signals.", ["Rayleigh fading", "Gaussian noise", "Efficient energy use", "Energy"]], ["On the Low-SNR Capacity of Phase-Shift Keying with Hard-Decision Detection", "The low-snr capacity of M-ary PSK transmission over both the additive white Gaussian noise (AWGN) and fading channels is analyzed when hard-decision detection is employed at the receiver. Closed-form expressions for the first and second derivatives of the capacity at zero snr are obtained. The spectral-efficiency/bit-energy tradeoff in the low-snr regime is analyzed by finding the wideband slope and the bit energy required at zero spectral efficiency. Practical design guidelines are drawn from the information-theoretic analysis. The fading channel analysis is conducted for both coherent and noncoherent cases, and the performance penalty in the low-power regime for not knowing the channel is identified.", ["Fading", "Additive white Gaussian noise", "Phase-shift keying", "Wideband", "Signal-to-noise ratio", "Bit", "Information theory", "Energy", "Low-power broadcasting", "Noise", "Spectral efficiency", "Channel (communications)", "Gaussian noise", "Receiver (radio)"]], ["Training Optimization for Gauss-Markov Rayleigh Fading Channels", "In this paper, pilot-assisted transmission over Gauss-Markov Rayleigh fading channels is considered. A simple scenario, where a single pilot signal is transmitted every T symbols and T-1 data symbols are transmitted in between the pilots, is studied. First, it is assumed that binary phase-shift keying (BPSK) modulation is employed at the transmitter. With this assumption, the training period, and data and training power allocation are jointly optimized by maximizing an achievable rate expression. Achievable rates and energy-per-bit requirements are computed using the optimal training parameters. Secondly, a capacity lower bound is obtained by considering the error in the estimate as another source of additive Gaussian noise, and the training parameters are optimized by maximizing this lower bound.", ["Rayleigh fading", "Phase-shift keying", "BPSK", "Modulation", "Data", "Signal (electronics)", "Carl Friedrich Gauss", "Additive white Gaussian noise", "Gaussian noise", "Energy", "Pilot signal", "Bit", "John William Strutt, 3rd Baron Rayleigh", "Transmitter"]], ["Performance Analysis for Multichannel Reception of OOFSK Signaling", "In this paper, the error performance of on-off frequency shift keying (OOFSK) modulation over fading channels is analyzed when the receiver is equipped with multiple antennas. The analysis is conducted in two cases: the coherent scenario where the fading is perfectly known at the receiver, and the noncoherent scenario where neither the receiver nor the transmitter knows the fading coefficients. For both cases, the maximum a posteriori probability (MAP) detection rule is derived and analytical probability of error expressions are obtained. The effect of fading correlation among the receiver antennas is also studied. Simulation results indicate that for sufficiently low duty cycle values, lower probability of error values with respect to FSK signaling are achieved. Equivalently, when compared to FSK modulation, OOFSK with low duty cycle requires less energy to achieve the same probability of error, which renders this modulation a more energy efficient transmission technique.", ["Energy", "Frequency", "A priori and a posteriori", "Duty cycle", "Simulation", "Frequency-shift keying", "Correlation and dependence", "Modulation", "Maximum a posteriori estimation"]], ["Error Probability Analysis of Peaky Signaling over Fading Channels", "In this paper, the performance of signaling strategies with high peak-to-average power ratio is analyzed in both coherent and noncoherent fading channels. Two recently proposed modulation schemes, namely on-off binary phase-shift keying and on-off quaternary phase-shift keying, are considered. For these modulation formats, the optimal decision rules used at the detector are identified and analytical expressions for the error probabilities are obtained. Numerical techniques are employed to compute the error probabilities. It is concluded that increasing the peakedness of the signals results in reduced error rates for a given power level and hence improve the energy efficiency.", ["Energy", "Decision theory", "Modulation", "Optimal decision", "Phase-shift keying"]], ["Comparison of Discrete and Continuous Wavelet Transforms", "In this paper we outline several points of view on the interplay between discrete and continuous wavelet transforms; stressing both pure and applied aspects of both. We outline some new links between the two transform technologies based on the theory of representations of generators and relations. By this we mean a finite system of generators which are represented by operators in Hilbert space. We further outline how these representations yield sub-band filter banks for signal and image processing algorithms.", ["Hilbert space", "Finite set", "Presentation of a group", "Image processing", "Continuous function", "Wavelet", "David Hilbert"]], ["Oblivious Transfer based on Key Exchange", "Key-exchange protocols have been overlooked as a possible means for implementing oblivious transfer (OT). In this paper we present a protocol for mutual exchange of secrets, 1-out-of-2 OT and coin flipping similar to Diffie-Hellman protocol using the idea of obliviously exchanging encryption keys. Since, Diffie-Hellman scheme is widely used, our protocol may provide a useful alternative to the conventional methods for implementation of oblivious transfer and a useful primitive in building larger cryptographic schemes.", ["Oblivious transfer", "Key (cryptography)", "Coin flipping", "Encryption", "Diffie-Hellman", "Whitfield Diffie", "Cryptography"]], ["Fault Classification in Cylinders Using Multilayer Perceptrons, Support Vector Machines and Guassian Mixture Models", "Gaussian mixture models (GMM) and support vector machines (SVM) are introduced to classify faults in a population of cylindrical shells. The proposed procedures are tested on a population of 20 cylindrical shells and their performance is compared to the procedure, which uses multi-layer perceptrons (MLP). The modal properties extracted from vibration data are used to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM produces 94% classification accuracy while the MLP produces 88% classification rates.", ["Support vector machine", "Cylinder (geometry)", "Perceptron"]], ["The Parameter-Less Self-Organizing Map algorithm", "The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a learning rate and annealing schemes for learning rate and neighbourhood size. We discuss the relative performance of the PLSOM and the SOM and demonstrate some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally we discuss some example applications of the PLSOM and present a proof of ordering under certain limited conditions.", ["Self-organizing map", "Neural network", "Algorithm", "Annealing (metallurgy)"]], ["Using Images to create a Hierarchical Grid Spatial Index", "This paper presents a hybrid approach to spatial indexing of two dimensional data. It sheds new light on the age old problem by thinking of the traditional algorithms as working with images. Inspiration is drawn from an analogous situation that is found in machine and human vision. Image processing techniques are used to assist in the spatial indexing of the data. A fixed grid approach is used and bins with too many records are sub-divided hierarchically. Search queries are pre-computed for bins that do not contain any data records. This has the effect of dividing the search space up into non rectangular regions which are based on the spatial properties of the data. The bucketing quad tree can be considered as an image with a resolution of two by two for each layer. The results show that this method performs better than the quad tree if there are more divisions per layer. This confirms our suspicions that the algorithm works better if it gets to look at the data with higher resolution images. An elegant class structure is developed where the implementation of concrete spatial indexes for a particular data type merely relies on rendering the data onto an image.", ["Image processing", "Quadtree", "Data type", "Hierarchy", "Data", "Algorithm"]], ["Riemannian level-set methods for tensor-valued data", "We present a novel approach for the derivation of PDE modeling curvature-driven flows for matrix-valued data. This approach is based on the Riemannian geometry of the manifold of Symmetric Positive Definite Matrices Pos(n).", ["Riemannian geometry", "Tensor", "Partial differential equation", "Curvature", "Manifold", "Geometry", "Matrix (mathematics)", "Riemannian manifold"]], ["Power Allocation for Discrete-Input Non-Ergodic Block-Fading Channels", "We consider power allocation algorithms for fixed-rate transmission over Nakagami-m non-ergodic block-fading channels with perfect transmitter and receiver channel state information and discrete input signal constellations under both short- and long-term power constraints. Optimal power allocation schemes are shown to be direct applications of previous results in the literature. We show that the SNR exponent of the optimal short-term scheme is given by the Singleton bound. We also illustrate the significant gains available by employing long-term power constraints. Due to the nature of the expressions involved, the complexity of optimal schemes may be prohibitive for system implementation. We propose simple sub-optimal power allocation schemes whose outage probability performance is very close to the minimum outage probability obtained by optimal schemes.", ["Channel state information", "Ergodicity", "Signal-to-noise ratio", "Singleton bound", "Exponentiation", "Probability", "Transmitter"]], ["More Efficient Algorithms and Analyses for Unequal Letter Cost Prefix-Free Coding", "There is a large literature devoted to the problem of finding an optimal (min-cost) prefix-free code with an unequal letter-cost encoding alphabet of size. While there is no known polynomial time algorithm for solving it optimally there are many good heuristics that all provide additive errors to optimal. The additive error in these algorithms usually depends linearly upon the largest encoding letter size. This paper was motivated by the problem of finding optimal codes when the encoding alphabet is infinite. Because the largest letter cost is infinite, the previous analyses could give infinite error bounds. We provide a new algorithm that works with infinite encoding alphabets. When restricted to the finite alphabet case, our algorithm often provides better error bounds than the best previous ones known.", ["Heuristic", "Polynomial time", "Polynomial", "Prefix code", "Finite set", "Algorithm", "Literature", "Alphabet", "Prefix", "Letter (paper size)"]], ["The Complexity of Games on Higher Order Pushdown Automata", "We prove an n-EXPTIME lower bound for the problem of deciding the winner in a reachability game on Higher Order Pushdown Automata (HPDA) of level n. This bound matches the known upper bound for parity games on HPDA. As a consequence the mu-calculus model checking over graphs given by n-HPDA is n-EXPTIME complete.", ["EXPTIME", "Reachability", "Model checking", "Calculus", "Complexity", "Graph (mathematics)", "Upper and lower bounds"]], ["Dynamic Clustering in Object-Oriented Databases: An Advocacy for Simplicity", "We present in this paper three dynamic clustering techniques for Object-Oriented Databases (OODBs). The first two, Dynamic, Statistical & Tunable Clustering (DSTC) and StatClust, exploit both comprehensive usage statistics and the inter-object reference graph. They are quite elaborate. However, they are also complex to implement and induce a high overhead. The third clustering technique, called Detection & Reclustering of Objects (DRO), is based on the same principles, but is much simpler to implement. These three clustering algorithm have been implemented in the Texas persistent object store and compared in terms of clustering efficiency (i.e., overall performance increase) and overhead using the Object Clustering Benchmark (OCB). The results obtained showed that DRO induced a lighter overhead while still achieving better overall performance.", ["Statistics", "Object-oriented programming", "Database", "Algorithm", "Cluster analysis", "Texas"]], ["Inverse-free Berlekamp-Massey-Sakata Algorithm and Small Decoders for Algebraic-Geometric Codes", "This paper proposes a novel algorithm for finding error-locators of algebraic-geometric codes that can eliminate the division-calculations of finite fields from the Berlekamp-Massey-Sakata algorithm. This inverse-free algorithm provides full performance in correcting a certain class of errors, generic errors, which includes most errors, and can decode codes on algebraic curves without the determination of unknown syndromes. Moreover, we propose three different kinds of architectures that our algorithm can be applied to, and we represent the control operation of shift-registers and switches at each clock-timing with numerical simulations. We estimate the performance in comparison of the total running time and the numbers of multipliers and shift-registers in three architectures with those of the conventional ones for codes on algebraic curves.", ["Algebraic geometry", "Algebraic number", "Algebraic curve", "Finite field", "Finite set", "Numerical analysis", "Geometry", "Algorithm", "Division (mathematics)"]], ["WDM and Directed Star Arboricity", "A digraph is $m$-labelled if every arc is labelled by an integer in $\\{1, \\dots,m\\}$. Motivated by wavelength assignment for multicasts in optical networks, we introduce and study $n$-fibre colourings of labelled digraphs. These are colourings of the arcs of $D$ such that at each vertex $v$, and for each colour $\\alpha$, $in(v,\\alpha)+out(v,\\alpha)\\leq n$ with $in(v,\\alpha)$ the number of arcs coloured $\\alpha$ entering $v$ and $out(v,\\alpha)$ the number of labels $l$ such that there is at least one arc of label $l$ leaving $v$ and coloured with $\\alpha$. The problem is to find the minimum number of colours $\\lambda_n(D)$ such that the $m$-labelled digraph $D$ has an $n$-fibre colouring. In the particular case when $D$ is $1$-labelled, $\\lambda_1(D)$ is called the directed star arboricity of $D$, and is denoted by $dst(D)$. We first show that $dst(D)\\leq 2\\Delta^-(D)+1$, and conjecture that if $\\Delta^-(D)\\geq 2$, then $dst(D)\\leq 2\\Delta^-(D)$. We also prove that for a subcubic digraph $D$, then $dst(D)\\leq 3$, and that if $\\Delta^+(D), \\Delta^-(D)\\leq 2$, then $dst(D)\\leq 4$. Finally, we study $\\lambda_n(m,k)=\\max\\{\\lambda_n(D) \\tq D \\mbox{is $m$-labelled} \\et \\Delta^-(D)\\leq k\\}$. We show that if $m\\geq n$, then $\\ds \\left\\lceil\\frac{m}{n}\\left\\lceil \\frac{k}{n}\\right\\rceil + \\frac{k}{n} \\right\\rceil\\leq \\lambda_n(m,k) \\leq\\left\\lceil\\frac{m}{n}\\left\\lceil \\frac{k}{n}\\right\\rceil + \\frac{k}{n} \\right\\rceil + C \\frac{m^2\\log k}{n}$ for some constant $C$. We conjecture that the lower bound should be the right value of $\\lambda_n(m,k)$.", ["Wavelength", "Digraph (orthography)", "Mbox", "Integer", "Wavelength-division multiplexing", "Star", "Fiber", "Lambda", "Upper and lower bounds", "Conjecture"]], ["Optimal Delay-Throughput Trade-offs in Mobile Ad-Hoc Networks: Hybrid Random Walk and One-Dimensional Mobility Models", "Optimal delay-throughput trade-offs for two-dimensional i.i.d mobility models have been established in [23], where we showed that the optimal trade-offs can be achieved using rate-less codes when the required delay guarantees are sufficient large. In this paper, we extend the results to other mobility models including two-dimensional hybrid random walk model, one-dimensional i.i.d. mobility model and one-dimensional hybrid random walk model. We consider both fast mobiles and slow mobiles, and establish the optimal delay-throughput trade-offs under some conditions. Joint coding-scheduling algorithms are also proposed to achieve the optimal trade-offs.", ["Random walk", "Mobile ad hoc network", "Throughput", "Random walk hypothesis", "Mobility model", "Hybrid (biology)", "Scheduling algorithm"]], ["Algorithms for laying points optimally on a plane and a circle", "Two averaging algorithms are considered which are intended for choosing an optimal plane and an optimal circle approximating a group of points in three-dimensional Euclidean space.", ["Euclidean space", "Plane (geometry)", "Three-dimensional space", "Circle"]], ["Edges and Switches, Tunnels and Bridges", "Edge casing is a well-known method to improve the readability of drawings of non-planar graphs. A cased drawing orders the edges of each edge crossing and interrupts the lower edge in an appropriate neighborhood of the crossing. Certain orders will lead to a more readable drawing than others. We formulate several optimization criteria that try to capture the concept of a \"good\" cased drawing. Further, we address the algorithmic question of how to turn a given drawing into an optimal cased drawing. For many of the resulting optimization problems, we either find polynomial time algorithms or NP-hardness results.", ["Graph drawing", "Polynomial time", "Graph (mathematics)", "Planar graph", "Hardness of approximation", "Algorithm", "NP (complexity)", "NP-hard", "Mathematical optimization", "Polynomial"]], ["Undirected Graphs of Entanglement Two", "Entanglement is a complexity measure of directed graphs that origins in fixed point theory. This measure has shown its use in designing efficient algorithms to verify logical properties of transition systems. We are interested in the problem of deciding whether a graph has entanglement at most k. As this measure is defined by means of games, game theoretic ideas naturally lead to design polynomial algorithms that, for fixed k, decide the problem. Known characterizations of directed graphs of entanglement at most 1 lead, for k = 1, to design even faster algorithms. In this paper we present an explicit characterization of undirected graphs of entanglement at most 2. With such a characterization at hand, we devise a linear time algorithm to decide whether an undirected graph has this property.", ["Decision problem", "Polynomial", "Undirected graph", "Computational complexity theory", "Graph (mathematics)", "Algorithm", "Fixed point (mathematics)", "Fixed point theorem", "Linear time", "Blum axioms", "Game theory", "State transition system", "Complexity"]], ["Frugal Colouring of Graphs", "A $k$-frugal colouring of a graph $G$ is a proper colouring of the vertices of $G$ such that no colour appears more than $k$ times in the neighbourhood of a vertex. This type of colouring was introduced by Hind, Molloy and Reed in 1997. In this paper, we study the frugal chromatic number of planar graphs, planar graphs with large girth, and outerplanar graphs, and relate this parameter with several well-studied colourings, such as colouring of the square, cyclic colouring, and $L(p,q)$-labelling. We also study frugal edge-colourings of multigraphs.", ["Chromatic number", "Planar graph", "Graph (mathematics)", "Vertex (graph theory)"]], ["Encoding for the Blackwell Channel with Reinforced Belief Propagation", "A key idea in coding for the broadcast channel (BC) is binning, in which the transmitter encode information by selecting a codeword from an appropriate bin (the messages are thus the bin indexes). This selection is normally done by solving an appropriate (possibly difficult) combinatorial problem. Recently it has been shown that binning for the Blackwell channel --a particular BC-- can be done by iterative schemes based on Survey Propagation (SP). This method uses decimation for SP and suffers a complexity of O(n^2). In this paper we propose a new variation of the Belief Propagation (BP) algorithm, named Reinforced BP algorithm, that turns BP into a solver. Our simulations show that this new algorithm has complexity O(n log n). Using this new algorithm together with a non-linear coding scheme, we can efficiently achieve rates close to the border of the capacity region of the Blackwell channel.", ["Linear code", "Combinatorics", "Algorithm", "BP", "Iteration", "Transmitter", "Social Democratic Party of Switzerland"]], ["Batch Processor Sharing with Hyper-Exponential Service Time", "We study Batch Processor-Sharing (BPS) queuing model with hyper-exponential service time distribution and Poisson batch arrival process. One of the main goals to study BPS is the possibility of its application in size-based scheduling, which is used in differentiation between Short and Long flows in the Internet. In the case of hyper-exponential service time distribution we find an analytical expression of the expected conditional response time for the BPS queue. We show, that the expected conditional response time is a concave function of the service time. We apply the received results to the Two Level Processor-Sharing (TLPS) model with hyper-exponential service time distribution and find the expression of the expected response time for the TLPS model. TLPS scheduling discipline can be applied to size-based differentiation in TCP/IP networks and Web server request handling.", ["Transmission Control Protocol", "Internet Protocol Suite", "Web server", "Scheduling (computing)", "Concave function", "Internet", "Server (computing)", "World Wide Web"]], ["Multiresolution Approximation of Polygonal Curves in Linear Complexity", "We propose a new algorithm to the problem of polygonal curve approximation based on a multiresolution approach. This algorithm is suboptimal but still maintains some optimality between successive levels of resolution using dynamic programming. We show theoretically and experimentally that this algorithm has a linear complexity in time and space. We experimentally compare the outcomes of our algorithm to the optimal \"full search\" dynamic programming solution and finally to classical merge and split approaches. The experimental evaluations confirm the theoretical derivations and show that the proposed approach evaluated on 2D coastal maps either show a lower time complexity or provide polygonal approximations closer to the input discrete curves.", ["Polygonal chain", "Approximation", "2D computer graphics", "Dynamic programming", "Curve", "Algorithm", "Complexity", "Classical mechanics"]], ["VOODB: A Generic Discrete-Event Random Simulation Model to Evaluate the Performances of OODBs", "Performance of object-oriented database systems (OODBs) is still an issue to both designers and users nowadays. The aim of this paper is to propose a generic discrete-event random simulation model, called VOODB, in order to evaluate the performances of OODBs in general, and the performances of optimization methods like clustering in particular. Such optimization methods undoubtedly improve the performances of OODBs. Yet, they also always induce some kind of overhead for the system. Therefore, it is important to evaluate their exact impact on the overall performances. VOODB has been designed as a generic discrete-event random simulation model by putting to use a modelling approach, and has been validated by simulating the behavior of the O2 OODB and the Texas persistent object store. Since our final objective is to compare object clustering algorithms, some experiments have also been conducted on the DSTC clustering technique, which is implemented in Texas. To validate VOODB, performance results obtained by simulation for a given experiment have been compared to the results obtained by benchmarking the real systems in the same conditions. Benchmarking and simulation performance evaluations have been observed to be consistent, so it appears that simulation can be a reliable approach to evaluate the performances of OODBs.", ["Conceptual model", "Discrete event simulation", "Database management system", "Object database", "Mathematical optimization", "Object-oriented programming", "Algorithm", "Database", "Computer simulation", "Simulation", "Cluster analysis", "Experiment"]], ["OCB: A Generic Benchmark to Evaluate the Performances of Object-Oriented Database Systems", "We present in this paper a generic object-oriented benchmark (the Object Clustering Benchmark) that has been designed to evaluate the performances of clustering policies in object-oriented databases. OCB is generic because its sample database may be customized to fit the databases introduced by the main existing benchmarks (e.g., OO1). OCB's current form is clustering-oriented because of its clustering-oriented workload, but it can be easily adapted to other purposes. Lastly, OCB's code is compact and easily portable. OCB has been implemented in a real system (Texas, running on a Sun workstation), in order to test a specific clustering policy called DSTC. A few results concerning this test are presented.", ["Sun Microsystems", "Object-oriented programming", "Object database", "Database", "Workstation", "Texas"]], ["Performance Evaluation for Clustering Algorithms in Object-Oriented Database Systems", "It is widely acknowledged that good object clustering is critical to the performance of object-oriented databases. However, object clustering always involves some kind of overhead for the system. The aim of this paper is to propose a modelling methodology in order to evaluate the performances of different clustering policies. This methodology has been used to compare the performances of three clustering algorithms found in the literature (Cactis, CK and ORION) that we considered representative of the current research in the field of object clustering. The actual performance evaluation was performed using simulation. Simulation experiments we performed showed that the Cactis algorithm is better than the ORION algorithm and that the CK algorithm totally outperforms both other algorithms in terms of response time and clustering overhead.", ["Cluster analysis", "Object database", "Literature", "Object-oriented programming", "Methodology", "Database"]], ["The Design of Efficiently-Encodable Rate-Compatible LDPC Codes", "We present a new class of irregular low-density parity-check (LDPC) codes for moderate block lengths (up to a few thousand bits) that are well-suited for rate-compatible puncturing. The proposed codes show good performance under puncturing over a wide range of rates and are suitable for usage in incremental redundancy hybrid-automatic repeat request (ARQ) systems. In addition, these codes are linear-time encodable with simple shift-register circuits. For a block length of 1200 bits the codes outperform optimized irregular LDPC codes and extended irregular repeat-accumulate (eIRA) codes for all puncturing rates 0.6~0.9 (base code performance is almost the same) and are particularly good at high puncturing rates where good puncturing performance has been previously difficult to achieve.", ["Automatic repeat request", "Low-density parity-check code", "Character encoding"]], ["Succinct Indexable Dictionaries with Applications to Encoding $k$-ary Trees, Prefix Sums and Multisets", "We consider the {\\it indexable dictionary} problem, which consists of storing a set $S \\subseteq \\{0,...,m-1\\}$ for some integer $m$, while supporting the operations of $\\Rank(x)$, which returns the number of elements in $S$ that are less than $x$ if $x \\in S$, and -1 otherwise; and $\\Select(i)$ which returns the $i$-th smallest element in $S$. We give a data structure that supports both operations in O(1) time on the RAM model and requires ${\\cal B}(n,m) + o(n) + O(\\lg \\lg m)$ bits to store a set of size $n$, where ${\\cal B}(n,m) = \\ceil{\\lg {m \\choose n}}$ is the minimum number of bits required to store any $n$-element subset from a universe of size $m$. Previous dictionaries taking this space only supported (yes/no) membership queries in O(1) time. In the cell probe model we can remove the $O(\\lg \\lg m)$ additive term in the space bound, answering a question raised by Fich and Miltersen, and Pagh. We present extensions and applications of our indexable dictionary data structure, including: An information-theoretically optimal representation of a $k$-ary cardinal tree that supports standard operations in constant time, A representation of a multiset of size $n$ from $\\{0,...,m-1\\}$ in ${\\cal B}(n,m+n) + o(n)$ bits that supports (appropriate generalizations of) $\\Rank$ and $\\Select$ operations in constant time, and A representation of a sequence of $n$ non-negative integers summing up to $m$ in ${\\cal B}(n,m+n) + o(n)$ bits that supports prefix sum queries in constant time.", ["Multiset", "Data structure", "Random-access memory", "Integer", "Non-negative", "Prefix sum", "Information theory", "Indexing (motion)", "Prefix", "Floor and ceiling functions", "Subset", "Dictionary", "Data", "Constant time"]], ["Iterative Rounding for the Closest String Problem", "The closest string problem is an NP-hard problem, whose task is to find a string that minimizes maximum Hamming distance to a given set of strings. This can be reduced to an integer program (IP). However, to date, there exists no known polynomial-time algorithm for IP. In 2004, Meneses et al. introduced a branch-and-bound (B & B) method for solving the IP problem. Their algorithm is not always efficient and has the exponential time complexity. In the paper, we attempt to solve efficiently the IP problem by a greedy iterative rounding technique. The proposed algorithm is polynomial time and much faster than the existing B & B IP for the CSP. If the number of strings is limited to 3, the algorithm is provably at most 1 away from the optimum. The empirical results show that in many cases we can find an exact solution. Even though we fail to find an exact solution, the solution found is very close to exact solution.", ["Hamming distance", "NP-hard", "Time complexity", "Polynomial time", "Integer", "Algorithm", "Branch and bound", "String (computer science)", "Rounding", "Empirical", "Polynomial", "Iteration"]], ["Rate Bounds for MIMO Relay Channels", "This paper considers the multi-input multi-output (MIMO) relay channel where multiple antennas are employed by each terminal. Compared to single-input single-output (SISO) relay channels, MIMO relay channels introduce additional degrees of freedom, making the design and analysis of optimal cooperative strategies more complex. In this paper, a partial cooperation strategy that combines transmit-side message splitting and block-Markov encoding is presented. Lower bounds on capacity that improve on a previously proposed non-cooperative lower bound are derived for Gaussian MIMO relay channels.", ["Degrees of freedom (statistics)", "MIMO", "Upper and lower bounds"]], ["Clustering Co-occurrence of Maximal Frequent Patterns in Streams", "One way of getting a better view of data is using frequent patterns. In this paper frequent patterns are subsets that occur a minimal number of times in a stream of itemsets. However, the discovery of frequent patterns in streams has always been problematic. Because streams are potentially endless it is in principle impossible to say if a pattern is often occurring or not. Furthermore the number of patterns can be huge and a good overview of the structure of the stream is lost quickly. The proposed approach will use clustering to facilitate the analysis of the structure of the stream. A clustering on the co-occurrence of patterns will give the user an improved view on the structure of the stream. Some patterns might occur so much together that they should form a combined pattern. In this way the patterns in the clustering will be the largest frequent patterns: maximal frequent patterns. Our approach to decide if patterns occur often together will be based on a method of clustering when only the distance between pairs is known. The number of maximal frequent patterns is much smaller and combined with clustering methods these patterns provide a good view on the structure of the stream.", ["Cluster analysis", "Co-occurrence"]], ["Clustering with Lattices in the Analysis of Graph Patterns", "Mining frequent subgraphs is an area of research where we have a given set of graphs (each graph can be seen as a transaction), and we search for (connected) subgraphs contained in many of these graphs. In this work we will discuss techniques used in our framework Lattice2SAR for mining and analysing frequent subgraph data and their corresponding lattice information. Lattice information is provided by the graph mining algorithm gSpan; it contains all supergraph-subgraph relations of the frequent subgraph patterns -- and their supports. Lattice2SAR is in particular used in the analysis of frequent graph patterns where the graphs are molecules and the frequent subgraphs are fragments. In the analysis of fragments one is interested in the molecules where patterns occur. This data can be very extensive and in this paper we focus on a technique of making it better available by using the lattice information in our clustering. Now we can reduce the number of times the highly compressed occurrence data needs to be accessed by the user. The user does not have to browse all the occurrence data in search of patterns occurring in the same molecules. Instead one can directly see which frequent subgraphs are of interest.", ["Subgraph", "Algorithm", "Cluster analysis", "Molecule", "Mining", "Structure mining"]], ["NodeTrix: Hybrid Representation for Analyzing Social Networks", "The need to visualize large social networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available. Unfortunately, the visualizations in existing systems do not satisfactorily answer the basic dilemma of being readable both for the global structure of the network and also for detailed analysis of local communities. To address this problem, we present NodeTrix, a hybrid representation for networks that combines the advantages of two traditional representations: node-link diagrams are used to show the global structure of a network, while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities. A key contribution is a set of interaction techniques. These allow analysts to create a NodeTrix visualization by dragging selections from either a node-link or a matrix, flexibly manipulate the NodeTrix representation to explore the dataset, and create meaningful summary visualizations of their findings. Finally, we present a case study applying NodeTrix to the analysis of the InfoVis 2004 coauthorship dataset to illustrate the capabilities of NodeTrix as both an exploration tool and an effective means of communicating results.", ["Social network", "Scientific visualization", "Interaction technique", "Adjacency matrix", "Hardware", "Matrix (mathematics)", "Visualization (computer graphics)"]], ["Risk Assessment Algorithms Based On Recursive Neural Networks", "The assessment of highly-risky situations at road intersections have been recently revealed as an important research topic within the context of the automotive industry. In this paper we shall introduce a novel approach to compute risk functions by using a combination of a highly non-linear processing model in conjunction with a powerful information encoding procedure. Specifically, the elements of information either static or dynamic that appear in a road intersection scene are encoded by using directed positional acyclic labeled graphs. The risk assessment problem is then reformulated in terms of an inductive learning task carried out by a recursive neural network. Recursive neural networks are connectionist models capable of solving supervised and non-supervised learning problems represented by directed ordered acyclic graphs. The potential of this novel approach is demonstrated through well predefined scenarios. The major difference of our approach compared to others is expressed by the fact of learning the structure of the risk. Furthermore, the combination of a rich information encoding procedure with a generalized model of dynamical recurrent networks permit us, as we shall demonstrate, a sophisticated processing of information that we believe as being a first step for building future advanced intersection safety systems", ["Neural network", "Connectionism", "Graph (mathematics)", "Supervised learning", "Information processing", "Code", "Algorithm", "Artificial neural network", "Inductive reasoning", "Directed acyclic graph", "Recursion"]], ["Privacy - an Issue for eLearning? A Trend Analysis Reflecting the Attitude of European eLearning Users", "Availing services provided via the Internet became a widely accepted means in organising one's life. Beside others, eLearning goes with this trend as well. But, while employing Internet service makes life more convenient, at the same time, it raises risks with respect to the protection of the users' privacy. This paper analyses the attitudes of eLearning users towards their privacy by, initially, pointing out terminology and legal issues connected with privacy. Further, the concept and implementation as well as a result analysis of a conducted study is presented, which explores the problem area from different perspectives. The paper will show that eLearning users indeed care for the protection of their personal information when using eLearning services. However, their attitudes and behaviour slightly differ. In conclusion, we provide first approaches of assisting possibilities for users how to resolve the difference of requirements and their actual activities with respect to privacy protection.", ["Attitude (psychology)", "Privacy", "Internet", "E-learning", "Paper", "Privacy law"]], ["Moving Walkways, Escalators, and Elevators", "We study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. This elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. The travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful. We give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.", ["Teleportation in fiction", "Escalator", "Geometry", "Geometric modeling"]], ["Learning to Bluff", "The act of bluffing confounds game designers to this day. The very nature of bluffing is even open for debate, adding further complication to the process of creating intelligent virtual players that can bluff, and hence play, realistically. Through the use of intelligent, learning agents, and carefully designed agent outlooks, an agent can in fact learn to predict its opponents reactions based not only on its own cards, but on the actions of those around it. With this wider scope of understanding, an agent can in learn to bluff its opponents, with the action representing not an illogical action, as bluffing is often viewed, but rather as an act of maximising returns through an effective statistical optimisation. By using a tee dee lambda learning algorithm to continuously adapt neural network agent intelligence, agents have been shown to be able to learn to bluff without outside prompting, and even to learn to call each others bluffs in free, competitive play.", ["Neural network"]], ["Soft constraint abstraction based on semiring homomorphism", "The semiring-based constraint satisfaction problems (semiring CSPs), proposed by Bistarelli, Montanari and Rossi \\cite{BMR97}, is a very general framework of soft constraints. In this paper we propose an abstraction scheme for soft constraints that uses semiring homomorphism. To find optimal solutions of the concrete problem, the idea is, first working in the abstract problem and finding its optimal solutions, then using them to solve the concrete problem. In particular, we show that a mapping preserves optimal solutions if and only if it is an order-reflecting semiring homomorphism. Moreover, for a semiring homomorphism $\\alpha$ and a problem $P$ over $S$, if $t$ is optimal in $\\alpha(P)$, then there is an optimal solution $\\bar{t}$ of $P$ such that $\\bar{t}$ has the same value as $t$ in $\\alpha(P)$.", ["Semiring", "Constraint satisfaction"]], ["The Optimization of a Novel Prismatic Drive", "The design of a mechanical transmission taking into account the transmitted forces is reported in this paper. This transmission is based on Slide-o-Cam, a cam mechanism with multiple rollers mounted on a common translating follower. The design of Slide-o-Cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. This transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. The pressure angle is a relevant performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. To assess the transmission capability of the mechanism, the Hertz formula is introduced to calculate the stresses on the rollers and on the cams. The final transmission is intended to replace the current ball-screws in the Orthoglide, a three-DOF parallel robot for the production of translational motions, currently under development for machining applications at Ecole Centrale de Nantes.", ["Machining", "Robot", "Friction", "Cam", "Mathematical optimization", "Pinion", "Transmission (mechanics)", "Pressure"]], ["MIMO detection employing Markov Chain Monte Carlo", "We propose a soft-output detection scheme for Multiple-Input-Multiple-Output (MIMO) systems. The detector employs Markov Chain Monte Carlo method to compute bit reliabilities from the signals received and is thus suited for coded MIMO systems. It offers a good trade-off between achievable performance and algorithmic complexity.", ["Monte Carlo method", "MIMO", "Markov chain Monte Carlo", "Computational complexity theory", "Algorithm", "Trade-off", "Markov chain"]], ["Approximate textual retrieval", "An approximate textual retrieval algorithm for searching sources with high levels of defects is presented. It considers splitting the words in a query into two overlapping segments and subsequently building composite regular expressions from interlacing subsets of the segments. This procedure reduces the probability of missed occurrences due to source defects, yet diminishes the retrieval of irrelevant, non-contextual occurrences.", ["Regular expression", "Algorithm"]], ["Equivalence of LP Relaxation and Max-Product for Weighted Matching in General Graphs", "Max-product belief propagation is a local, iterative algorithm to find the mode/MAP estimate of a probability distribution. While it has been successfully employed in a wide variety of applications, there are relatively few theoretical guarantees of convergence and correctness for general loopy graphs that may have many short cycles. Of these, even fewer provide exact ``necessary and sufficient'' characterizations. In this paper we investigate the problem of using max-product to find the maximum weight matching in an arbitrary graph with edge weights. This is done by first constructing a probability distribution whose mode corresponds to the optimal matching, and then running max-product. Weighted matching can also be posed as an integer program, for which there is an LP relaxation. This relaxation is not always tight. In this paper we show that \\begin{enumerate} \\item If the LP relaxation is tight, then max-product always converges, and that too to the correct answer. \\item If the LP relaxation is loose, then max-product does not converge. \\end{enumerate} This provides an exact, data-dependent characterization of max-product performance, and a precise connection to LP relaxation, which is a well-studied optimization technique. Also, since LP relaxation is known to be tight for bipartite graphs, our results generalize other recent results on using max-product to find weighted matchings in bipartite graphs.", ["Matching (graph theory)", "Probability distribution", "Graph (mathematics)", "Algorithm", "Mathematical optimization", "Belief propagation", "Integer", "Probability", "Necessary and sufficient condition", "Iterative method"]], ["Bayesian Approach to Neuro-Rough Models", "This paper proposes a neuro-rough model based on multi-layered perceptron and rough set. The neuro-rough model is then tested on modelling the risk of HIV from demographic data. The model is formulated using Bayesian framework and trained using Monte Carlo method and Metropolis criterion. When the model was tested to estimate the risk of HIV infection given the demographic data it was found to give the accuracy of 62%. The proposed model is able to combine the accuracy of the Bayesian MLP model and the transparency of Bayesian rough set model.", ["Monte Carlo method", "Demographics", "HIV", "Metropolis-Hastings algorithm", "Perceptron", "Bayesian probability", "Infection", "Bayes' theorem"]], ["Medical Image Segmentation and Localization using Deformable Templates", "This paper presents deformable templates as a tool for segmentation and localization of biological structures in medical images. Structures are represented by a prototype template, combined with a parametric warp mapping used to deform the original shape. The localization procedure is achieved using a multi-stage, multi-resolution algorithm de-signed to reduce computational complexity and time. The algorithm initially identifies regions in the image most likely to contain the desired objects and then examines these regions at progressively increasing resolutions. The final stage of the algorithm involves warping the prototype template to match the localized objects. The algorithm is presented along with the results of four example applications using MRI, x-ray and ultrasound images.", ["Computational complexity theory", "Ultrasound", "Magnetic resonance imaging", "Prototype", "X-ray", "Algorithm"]], ["Non-cooperative games for spreading code optimization, power control and receiver design in wireless data networks", "This paper focuses on the issue of energy efficiency in wireless data networks through a game theoretic approach. The case considered is that in which each user is allowed to vary its transmit power, spreading code, and uplink receiver in order to maximize its own utility, which is here defined as the ratio of data throughput to transmit power. In particular, the case in which linear multiuser detectors are employed at the receiver is treated first, and, then, the more challenging case in which non-linear decision feedback multiuser receivers are adopted is addressed. It is shown that, for both receivers, the problem at hand of utility maximization can be regarded as a non-cooperative game, and it is proved that a unique Nash equilibrium point exists. Simulation results show that significant performance gains can be obtained through both non-linear processing and spreading code optimization; in particular, for systems with a number of users not larger than the processing gain, remarkable gains come from spreading code optimization, while, for overloaded systems, the largest gainscome from the use of non-linear processing. In every case, however, the non-cooperative games proposed here are shown to outperform competing alternatives.", ["Simulation", "Program optimization", "Nash equilibrium", "Utility", "Non-cooperative game", "Feedback", "Wireless", "Energy", "Uplink", "Cooperative game", "Game theory", "Linear", "Equilibrium point", "Throughput", "Computer network"]], ["Overview of the Netsukuku network", "Netsukuku is a P2P network system designed to handle a large number of nodes with minimal CPU and memory resources. It can be easily used to build a worldwide distributed, anonymous and not controlled network, separated from the Internet, without the support of any servers, ISPs or authority controls. In this document, we give a generic and non technical description of the Netsukuku network, emphasizing its main ideas and features.", ["Netsukuku", "Central processing unit", "Peer-to-peer", "Internet", "Node (networking)", "Computer network"]], ["Quantum Shortest Path Netsukuku", "This document describes the QSPN, the routing discovery algorithm used by Netsukuku. Through a deductive analysis the main proprieties of the QSPN are shown. Moreover, a second version of the algorithm, is presented.", ["Netsukuku", "Deductive reasoning", "Algorithm"]], ["The Netsukuku network topology", "In this document, we describe the fractal structure of the Netsukuku topology. Moreover, we show how it is possible to use the QSPN v2 on the high levels of the fractal.", ["Netsukuku", "Topology", "Fractal", "Network topology"]], ["ANDNA: the distributed hostname management system of Netsukuku", "We present the Abnormal Netsukuku Domain Name Anarchy system. ANDNA is the distributed, non hierarchical and decentralised system of hostname management used in the Netsukuku network.", ["Netsukuku", "Hierarchy", "Hostname"]], ["Enhancement of Noisy Planar Nuclear Medicine Images using Mean Field Annealing", "Nuclear medicine (NM) images inherently suffer from large amounts of noise and blur. The purpose of this research is to reduce the noise and blur while maintaining image integrity for improved diagnosis. The proposed solution is to increase image quality after the standard pre- and post-processing undertaken by a gamma camera system. Mean Field Annealing (MFA) is the image processing technique used in this research. It is a computational iterative technique that makes use of the Point Spread Function (PSF) and the noise associated with the NM image. MFA is applied to NM images with the objective of reducing noise while not compromising edge integrity. Using a sharpening filter as a post-processing technique (after MFA) yields image enhancement of planar NM images.", ["Point spread function", "Nuclear medicine", "Gamma camera", "Medicine", "Image processing", "Iterative method", "Camera", "Iteration", "Image editing", "Plane (geometry)", "Filter (signal processing)"]], ["The Multiobjective Optimization of a Prismatic Drive", "The multiobjective optimization of Slide-o-Cam is reported in this paper. Slide-o-Cam is a cam mechanism with multiple rollers mounted on a common translating follower. This transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. A Pareto frontier is obtained by means of multiobjective optimization. This optimization is based on three objective functions: (i) the pressure angle, which is a suitable performance index for the transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame; (ii) the Hertz pressure used to evaluate the stresses produced on the contact surface between cam and roller; and (iii) the size of the mechanism, characterized by the number of cams and their width.", ["Friction", "Cam", "Multi-objective optimization", "Pinion", "Mathematical optimization"]], ["Game-Theoretic Power Control in Impulse Radio UWB Wireless Networks", "In this paper, a game-theoretic model for studying power control for wireless data networks in frequency-selective multipath environments is analyzed. The uplink of an impulse-radio ultrawideband system is considered. The effects of self-interference and multiple-access interference on the performance of Rake receivers are investigated for synchronous systems. Focusing on energy efficiency, a noncooperative game is proposed in which users in the network are allowed to choose their transmit powers to maximize their own utilities, and the Nash equilibrium for the proposed game is derived. It is shown that, due to the frequency selective multipath, the noncooperative solution is achieved at different signal-to-interference-plus-noise ratios, respectively of the channel realization. A large-system analysis is performed to derive explicit expressions for the achieved utilities. The Pareto-optimal (cooperative) solution is also discussed and compared with the noncooperative approach.", ["Nash equilibrium", "Frequency", "Wireless", "Non-cooperative game", "Game theory", "Multipath propagation", "Fading", "Ultra-wideband", "Wireless network", "Channel access method", "Computer network", "Uplink", "System analysis", "Radio", "Data", "Energy", "Power control", "Pareto efficiency"]], ["Satisfiability Parsimoniously Reduces to the Tantrix(TM) Rotation Puzzle Problem", "Holzer and Holzer (Discrete Applied Mathematics 144(3):345--358, 2004) proved that the Tantrix(TM) rotation puzzle problem is NP-complete. They also showed that for infinite rotation puzzles, this problem becomes undecidable. We study the counting version and the unique version of this problem. We prove that the satisfiability problem parsimoniously reduces to the Tantrix(TM) rotation puzzle problem. In particular, this reduction preserves the uniqueness of the solution, which implies that the unique Tantrix(TM) rotation puzzle problem is as hard as the unique satisfiability problem, and so is DP-complete under polynomial-time randomized reductions, where DP is the second level of the boolean hierarchy over NP.", ["NP-complete", "Polynomial", "NP (complexity)", "Undecidable problem", "Occam's razor", "Turing machine", "Time complexity", "Mathematics"]], ["Variable-Rate Distributed Source Coding in the Presence of Byzantine Sensors", "The distributed source coding problem is considered when the sensors, or encoders, are under Byzantine attack; that is, an unknown number of sensors have been reprogrammed by a malicious intruder to undermine the reconstruction at the fusion center. Three different forms of the problem are considered. The first is a variable-rate setup, in which the decoder adaptively chooses the rates at which the sensors transmit. An explicit characterization of the variable-rate minimum achievable sum rate is stated, given by the maximum entropy over the set of distributions indistinguishable from the true source distribution by the decoder. In addition, two forms of the fixed-rate problem are considered, one with deterministic coding and one with randomized coding. The achievable rate regions are given for both these problems, with a larger region achievable using randomized coding, though both are suboptimal compared to variable-rate coding.", ["Data compression", "Entropy", "Sensor", "Decoder", "Encoder"]], ["Computing Minimal Polynomials of Matrices", "We present and analyse a Monte-Carlo algorithm to compute the minimal polynomial of an $n\\times n$ matrix over a finite field that requires $O(n^3)$ field operations and O(n) random vectors, and is well suited for successful practical implementation. The algorithm, and its complexity analysis, use standard algorithms for polynomial and matrix operations. We compare features of the algorithm with several other algorithms in the literature. In addition we present a deterministic verification procedure which is similarly efficient in most cases but has a worst-case complexity of $O(n^4)$. Finally, we report the results of practical experiments with an implementation of our algorithms in comparison with the current algorithms in the {\\sf GAP} library.", ["Finite field", "Analysis of algorithms", "Polynomial", "Matrix (mathematics)", "Algorithm", "Worst-case complexity", "Computing", "Finite set", "Monte Carlo method"]], ["Performance Comparison of Energy-Efficient Power Control for CDMA and Multiuser UWB Networks", "This paper studies the performance of a wireless data network using energy-efficient power control techniques when different multiple access schemes, namely direct-sequence code division multiple access (DS-CDMA) and impulse-radio ultrawideband (IR-UWB), are considered. Due to the large bandwidth of the system, the multipath channel is assumed to be frequency-selective. By making use of noncooperative game-theoretic models and large-system analysis tools, explicit expressions for the achieved utilities at the Nash equilibrium are derived in terms of the network parameters. A measure of the loss of DS-CDMA with respect to IR-UWB is proposed, which proves substantial equivalence between the two schemes. Simulation results are provided to validate the analysis.", ["Ultra-wideband", "Code division multiple access", "Radio", "Non-cooperative game", "Game theory", "Nash equilibrium", "System analysis", "Frequency", "Simulation", "Energy", "Power control", "Bandwidth (signal processing)", "Data", "Multipath propagation", "Network analysis (electrical circuits)"]], ["An Independent Evaluation of Subspace Face Recognition Algorithms", "This paper explores a comparative study of both the linear and kernel implementations of three of the most popular Appearance-based Face Recognition projection classes, these being the methodologies of Principal Component Analysis, Linear Discriminant Analysis and Independent Component Analysis. The experimental procedure provides a platform of equal working conditions and examines the ten algorithms in the categories of expression, illumination, occlusion and temporal delay. The results are then evaluated based on a sequential combination of assessment tools that facilitate both intuitive and statistical decisiveness among the intra and interclass comparisons. The best categorical algorithms are then incorporated into a hybrid methodology, where the advantageous effects of fusion strategies are considered.", ["Linear discriminant analysis", "Principal component analysis", "Independent component analysis"]], ["On Isotropic Sets of Points in the Plane. Application to the Design of Robot Archirectures", "Various performance indices are used for the design of serial manipulators. One method of optimization relies on the condition number of the Jacobian matrix. The minimization of the condition number leads, under certain conditions, to isotropic configurations, for which the roundoff-error amplification is lowest. In this paper, the isotropy conditions, introduced elsewhere, are the motivation behind the introduction of isotropic sets of points. By connecting together these points, we define families of isotropic manipulators. This paper is devoted to planar manipulators, the concepts being currently extended to their spatial counterparts. Furthermore, only manipulators with revolute joints are considered here.", ["Mathematical optimization", "Condition number", "Jacobian matrix and determinant", "Isotropy", "Motivation"]], ["The Kinematic Analysis of a Symmetrical Three-Degree-of-Freedom Planar Parallel Manipulator", "Presented in this paper is the kinematic analysis of a symmetrical three-degree-of-freedom planar parallel manipulator. In opposite to serial manipulators, parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. This property produces more complicated kinematic models but allows more flexibility in trajectory planning. To take into account this property, the notion of aspects, i.e. the maximal singularity-free domains, was introduced, based on the notion of working modes, which makes it possible to separate the inverse kinematic solutions. The aim of this paper is to show that a non-singular assembly-mode changing trajectory exist for a symmetrical planar parallel manipulator, with equilateral base and platform triangle.", ["Symmetry", "Inverse kinematics", "Singular point of an algebraic variety", "Equilateral polygon", "Trajectory", "Kinematics", "Plane (geometry)"]], ["Uniqueness Domains in the Workspace of Parallel Manipulators", "This work investigates new kinematic features of parallel manipulators. It is well known that parallel manipulators admit generally several direct kinematic solutions for a given set of input joint values. The aim of this paper is to characterize the uniqueness domains in the workspace of parallel manipulators, as well as their image in the joint space. The study focuses on the most usual case of parallel manipulators with only one inverse kinematic solution. The notion of aspect introduced for serial manipulators in [Borrel 86] is redefined for such parallel manipulators. Then, it is shown that it is possible to link several solutions to the forward kinematic problem without meeting a singularity, thus meaning that the aspects are not uniqueness domains. An additional set of surfaces, namely the characteristic surfaces, are characterized which divide the workspace into basic regions and yield new uniqueness domains. This study is illustrated all along the paper with a 3-RPR planar parallel manipulator. An octree model of spaces is used to compute the joint space, the workspace and all other newly defined sets.", ["Octree", "Inverse kinematics"]], ["The Kinematic design of a 3-dof Hybrid Manipulator", "This paper focuses on the kinematic properties of a new three-degree-of-freedom hybrid manipulator. This manipulator is obtained by adding in series to a five-bar planar mechanism (similar to the one studied by Bajpai and Roth) a third revolute passing through the line of centers of the two actuated revolute joints of the above linkage. The resulting architecture is hybrid in that it has both serial and parallel links. Fully-parallel manipulators are known for the existence of particularly undesirable singularities (referred to as parallel singularities) where control is lost [4] and [6]. On the other hand, due to their cantilever type of kinematic arrangement, fully serial manipulators suffer from a lack of stiffness and from relatively large positioning errors. The hybrid manipulator studied is intrinsically stiffer and more accurate. Furthermore, since all actuators are located on the first axis, the inertial effects are considerably reduced. In addition, it is shown that the special kinematic structure of our manipulator has the potential of avoiding parallel singularities by a suitable choice of the \"working mode\", thus leading to larger workspaces. The influence of the different structural dimensions (e.g. the link lengths) on the kinematic and mechanical properties are analysed in view of the optimal design of such hybrid manipulators.", ["Stiffness", "Degrees of freedom (mechanics)", "Cantilever", "Kinematics", "Inertial frame of reference", "Inertia", "Actuator", "Architecture", "Hybrid (biology)"]], ["Definition sets for the Direct Kinematics of Parallel Manipulators", "The aim of this paper is to characterize the uniqueness domains in the workspace of parallel manipulators, as well as their image in the joint space. The notion of aspect introduced for serial manipulators in [Borrel 86] is redefined for such parallel manipulators. Then, it is shown that it is possible to link several solutions to the direct kinematic problem without meeting a singularity, thus meaning that the aspects are not uniqueness domains. Additional surfaces are characterized in the workspace which yield new uniqueness domains. An octree model of spaces is used to compute the joint space, the workspace and all other newly defined sets. This study is illustrated all along the paper with a 3-RPR planar parallel manipulator.", ["Kinematics", "Octree"]], ["Improved Analysis of Kannan's Shortest Lattice Vector Algorithm", "The security of lattice-based cryptosystems such as NTRU, GGH and Ajtai-Dwork essentially relies upon the intractability of computing a shortest non-zero lattice vector and a closest lattice vector to a given target vector in high dimensions. The best algorithms for these tasks are due to Kannan, and, though remarkably simple, their complexity estimates have not been improved since more than twenty years. Kannan's algorithm for solving the shortest vector problem is in particular crucial in Schnorr's celebrated block reduction algorithm, on which are based the best known attacks against the lattice-based encryption schemes mentioned above. Understanding precisely Kannan's algorithm is of prime importance for providing meaningful key-sizes. In this paper we improve the complexity analyses of Kannan's algorithms and discuss the possibility of improving the underlying enumeration strategy.", ["Lattice problem", "Algorithm", "Computing", "Encryption", "NTRUEncrypt"]], ["Artificial Neural Networks and Support Vector Machines for Water Demand Time Series Forecasting", "Water plays a pivotal role in many physical processes, and most importantly in sustaining human life, animal life and plant life. Water supply entities therefore have the responsibility to supply clean and safe water at the rate required by the consumer. It is therefore necessary to implement mechanisms and systems that can be employed to predict both short-term and long-term water demands. The increasingly growing field of computational intelligence techniques has been proposed as an efficient tool in the modelling of dynamic phenomena. The primary objective of this paper is to compare the efficiency of two computational intelligence techniques in water demand forecasting. The techniques under comparison are the Artificial Neural Networks (ANNs) and the Support Vector Machines (SVMs). In this study it was observed that the ANNs perform better than the SVMs. This performance is measured against the generalisation ability of the two.", ["Demand forecasting", "Support vector machine", "Plant", "Water supply", "Computational intelligence", "Drinking water", "Time series", "Animal"]], ["A New Three-DOF Parallel Mechanism: Milling Machine Applications", "This paper describes a new parallel kinematic architecture for machining applications, namely, the orthoglide. This machine features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The main interest of the orthoglide is that it takes benefit from the advantages of the popular PPP serial machines (regular Cartesian workspace shape and uniform performances) as well as from the parallel kinematic arrangement of the links (less inertia and better dynamic performances), which makes the orthoglide well suited to high-speed machining applications. Possible extension of the orthoglide to 5-axis machining is also investigated.", ["Orthogonality", "Cartesian coordinate system", "Machining", "Architecture"]], ["Cellular Systems with Full-Duplex Amplify-and-Forward Relaying and Cooperative Base-Stations", "In this paper the benefits provided by multi-cell processing of signals transmitted by mobile terminals which are received via dedicated relay terminals (RTs) are assessed. Unlike previous works, each RT is assumed here to be capable of full-duplex operation and receives the transmission of adjacent relay terminals. Focusing on intra-cell TDMA and non-fading channels, a simplified uplink cellular model introduced by Wyner is considered. This framework facilitates analytical derivation of the per-cell sum-rate of multi-cell and conventional single-cell receivers. In particular, the analysis is based on the observation that the signal received at the base stations can be interpreted as the outcome of a two-dimensional linear time invariant system. Numerical results are provided as well in order to provide further insight into the performance benefits of multi-cell processing with relaying.", ["Duplex (telecommunications)", "Receiver (radio)", "Uplink", "Time division multiple access", "Transmission (telecommunications)", "Full-duplex", "LTI system theory", "Mobile phone", "Base station", "Signal (electronics)"]], ["Tracking User Attention in Collaborative Tagging Communities", "Collaborative tagging has recently attracted the attention of both industry and academia due to the popularity of content-sharing systems such as CiteULike, del.icio.us, and Flickr. These systems give users the opportunity to add data items and to attach their own metadata (or tags) to stored data. The result is an effective content management tool for individual users. Recent studies, however, suggest that, as tagging communities grow, the added content and the metadata become harder to manage due to an ease in content diversity. Thus, mechanisms that cope with increase of diversity are fundamental to improve the scalability and usability of collaborative tagging systems. This paper analyzes whether usage patterns can be harnessed to improve navigability in a growing knowledge space. To this end, it presents a characterization of two collaborative tagging communities that target scientific literature: CiteULike and Bibsonomy. We explore three main directions: First, we analyze the tagging activity distribution across the user population. Second, we define new metrics for similarity in user interest and use these metrics to uncover the structure of the tagging communities we study. The structure we uncover suggests a clear segmentation of interests into a large number of individuals with unique preferences and a core set of users with interspersed interests. Finally, we offer preliminary results that demonstrate that the interest-based structure of the tagging community can be used to facilitate content usage as communities scale.", ["CiteULike", "Folksonomy", "Flickr", "Usability", "Content management", "Delicious (website)", "Academia", "Scalability", "Literature", "Metadata"]], ["Recognizing Partial Cubes in Quadratic Time", "We show how to test whether a graph with n vertices and m edges is a partial cube, and if so how to find a distance-preserving embedding of the graph into a hypercube, in the near-optimal time bound O(n^2), improving previous O(nm)-time solutions.", ["Hypercube", "Cube", "Graph (mathematics)", "Embedding", "Time complexity"]], ["Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs with Missing Values", "An ensemble based approach for dealing with missing data, without predicting or imputing the missing values is proposed. This technique is suitable for online operations of neural networks and as a result, is used for online condition monitoring. The proposed technique is tested in both classification and regression problems. An ensemble of Fuzzy-ARTMAPs is used for classification whereas an ensemble of multi-layer perceptrons is used for the regression problem. Results obtained using this ensemble-based technique are compared to those obtained using a combination of auto-associative neural networks and genetic algorithms and findings show that this method can perform up to 9% better in regression problems. Another advantage of the proposed technique is that it eliminates the need for finding the best estimate of the data, and hence, saves time.", ["Neural network", "Genetic algorithm", "Perceptron", "Fuzzy logic", "Data", "Nervous system", "Regression analysis", "Condition monitoring"]], ["Optimal Cache-Oblivious Mesh Layouts", "A mesh is a graph that divides physical space into regularly-shaped regions. Meshes computations form the basis of many applications, e.g. finite-element methods, image rendering, and collision detection. In one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. The performance of a mesh update depends on the layout of the mesh in memory. This paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. Such a memory layout is called cache-oblivious. Formally, for a $d$-dimensional mesh $G$, block size $B$, and cache size $M$ (where $M=\\Omega(B^d)$), the mesh update of $G$ uses $O(1+|G|/B)$ memory transfers. The paper also shows how the mesh-update performance degrades for smaller caches, where $M=o(B^d)$. The paper then gives two algorithms for finding cache-oblivious mesh layouts. The first layout algorithm runs in time $O(|G|\\log^2|G|)$ both in expectation and with high probability on a RAM. It uses $O(1+|G|\\log^2(|G|/M)/B)$ memory transfers in expectation and $O(1+(|G|/B)(\\log^2(|G|/M) + \\log|G|))$ memory transfers with high probability in the cache-oblivious and disk-access machine (DAM) models. The layout is obtained by finding a fully balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree. The second algorithm runs faster by almost a $\\log|G|/\\log\\log|G|$ factor in all three memory models, both in expectation and with high probability. The layout obtained by finding a relax-balanced decomposition tree of $G$ and then performing an in-order traversal of the leaves of the tree.", ["Force-based algorithms (graph drawing)", "Random-access memory", "Cache-oblivious algorithm", "Algorithm", "Asymptotically optimal algorithm", "Natural logarithm", "Finite element method", "Collision detection", "Probability", "Cache", "Big O notation"]], ["Strategies for the Design of a Slide-o-Cam Transmission", "The optimization of the pressure angle in a cam-follower transmission is reported in this paper. This transmission is based on Slide-o-Cam, a cam mechanism with multiple rollers mounted on a common translating follower. The design of Slide-o-Cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. This transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. The pressure angle is a suitable performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. Two alternative design strategies are studied, namely, (i) increase the number of lobes on each cam or (ii) increase the number of cams. This device is intended to replace the current ball-screws in Orthoglide, a three-DOF parallel robot for the production of translational motions, currently under development at Ecole Centrale de Nantes for machining applications.", ["Friction", "Robot", "Machining", "Cam", "Mathematical optimization", "Pinion"]], ["Regions of Feasible Point-to-Point Trajectories in the Cartesian Workspace of Fully-Parallel Manipulators", "The goal of this paper is to define the n-connected regions in the Cartesian workspace of fully-parallel manipulators, i.e. the maximal regions where it is possible to execute point-to-point motions. The manipulators considered in this study may have multiple direct and inverse kinematic solutions. The N-connected regions are characterized by projection, onto the Cartesian workspace, of the connected components of the reachable configuration space defined in the Cartesian product of the Cartesian space by the joint space. Generalized octree models are used for the construction of all spaces. This study is illustrated with a simple planar fully-parallel manipulator.", ["N-connected", "Cartesian product", "Configuration space", "Inverse kinematics", "Kinematics", "Octree", "Plane (geometry)", "Cartesian coordinate system"]], ["The Design of Parallel Kinematic Machine Tools Using Kinetostatic Performance Criteria", "Most industrial machine tools have a serial kinematic architecture, which means that each axis has to carry the following one, including its actuators and joints. High Speed Machining highlights some drawbacks of such architectures: heavy moving parts require from the machine structure high stiffness to limit bending problems that lower the machine accuracy, and limit the dynamic performances of the feed axes. That is why PKMs attract more and more researchers and companies, because they are claimed to offer several advantages over their serial counterparts, like high structural rigidity and high dynamic capacities. Indeed, the parallel kinematic arrangement of the links provides higher stiffness and lower moving masses that reduce inertia effects. Thus, PKMs have better dynamic performances. However, the design of a parallel kinematic machine tool (PKMT) is a hard task that requires further research studies before wide industrial use can be expected. Many criteria need to be taken into account in the design of a PKMT. We pay special attention to the description of kinetostatic criteria that rely on the conditioning of the Jacobian matrix of the mechanism. The organisation of this paper is as follows: next section introduces general remarks about PKMs, then is explained why PKMs can be interesting alternative machine tool designs. Then are presented existing PKMTs. An application to the design of a small-scale machine tool prototype developed at IRCCyN is presented at the end of this paper.", ["Machining", "Machine tool", "Actuator", "Jacobian matrix and determinant", "Inertia", "PK machine gun", "Prototype", "Matrix (mathematics)", "Stiffness", "Architecture", "Kinematics"]], ["Mining Patterns with a Balanced Interval", "In many applications it will be useful to know those patterns that occur with a balanced interval, e.g., a certain combination of phone numbers are called almost every Friday or a group of products are sold a lot on Tuesday and Thursday. In previous work we proposed a new measure of support (the number of occurrences of a pattern in a dataset), where we count the number of times a pattern occurs (nearly) in the middle between two other occurrences. If the number of non-occurrences between two occurrences of a pattern stays almost the same then we call the pattern balanced. It was noticed that some very frequent patterns obviously also occur with a balanced interval, meaning in every transaction. However more interesting patterns might occur, e.g., every three transactions. Here we discuss a solution using standard deviation and average. Furthermore we propose a simpler approach for pruning patterns with a balanced interval, making estimating the pruning threshold more intuitive.", ["Standard deviation", "Mining", "Pruning"]], ["S\\'eparation des Solutions aux Mod\\`eles G\\'eom\\'etriques Direct et Inverse pour les Manipulateurs Pleinement Parall\\`eles", "This article provides a formalism making it possible to manage the solutions of the direct and inverse kinematic models of the fully parallel manipulators. We introduce the concept of working modes to separate the solutions from the opposite geometrical model. Then, we define, for each working mode, the aspects of these manipulators. To separate the solutions from the direct kinematics model, we introduce the concept of characteristic surfaces. Then, we define the uniqueness domains, as being the greatest domains of the workspace in which there is unicity of solutions. The principal applications of this work are the design, the trajectory planning.", ["Geometry", "Trajectory", "Inverse kinematics"]], ["On the Kinetostatic Optimization of Revolute-Coupled Planar Manipulators", "Proposed in this paper is a kinetostatic performance index for the optimum dimensioning of planar manipulators of the serial type. The index is based on the concept of distance of the underlying Jacobian matrix to a given isotropic matrix that is used as a reference model for purposes of performance evaluation. Applications of the index fall in the realm of design, but control applications are outlined. The paper focuses on planar manipulators, the basic concepts being currently extended to their three-dimensional counterparts.", ["Mathematical optimization", "Jacobian matrix and determinant"]], ["Achievable Rates and Optimal Resource Allocation for Imperfectly-Known Fading Relay Channels", "In this paper, achievable rates of imperfectly-known fading relay channels are studied. It is assumed that communication starts with the network training phase in which the receivers estimate the fading coefficients of their respective channels. In the data transmission phase, amplify-and-forward and decode-and-forward relaying schemes are considered, and the corresponding achievable rate expressions are obtained. The achievable rate expressions are then employed to identify the optimal resource allocation strategies.", ["Receiver (radio)", "Data", "Data transmission", "Transmission (telecommunications)", "Communication", "Amplifier"]], ["Ordering Finite-State Markov Channels by Mutual Information", "In previous work, an ordering result was given for the symbolwise probability of error using general Markov channels, under iterative decoding of LDPC codes. In this paper, the ordering result is extended to mutual information, under the assumption of an iid input distribution. For certain channels, in which the capacity-achieving input distribution is iid, this allows ordering of the channels by capacity. The complexity of analyzing general Markov channels is mitigated by this ordering, since it is possible to immediately determine that a wide class of channels, with different numbers of states, has a smaller mutual information than a given channel.", ["Independent and identically distributed random variables", "Mutual information", "Iteration", "Probability", "Probability distribution", "Low-density parity-check code"]], ["IDF revisited: A simple new derivation within the Robertson-Sp\\\"arck Jones probabilistic model", "There have been a number of prior attempts to theoretically justify the effectiveness of the inverse document frequency (IDF). Those that take as their starting point Robertson and Sparck Jones's probabilistic model are based on strong or complex assumptions. We show that a more intuitively plausible assumption suffices. Moreover, the new assumption, while conceptually very simple, provides a solution to an estimation problem that had been deemed intractable by Robertson and Walker (1997).", ["Statistical model", "Frequency", "Probability"]], ["Multiple Antenna Secure Broadcast over Wireless Networks", "In wireless data networks, communication is particularly susceptible to eavesdropping due to its broadcast nature. Security and privacy systems have become critical for wireless providers and enterprise networks. This paper considers the problem of secret communication over the Gaussian broadcast channel, where a multi-antenna transmitter sends independent confidential messages to two users with perfect secrecy. That is, each user would like to obtain its own message reliably and confidentially. First, a computable Sato-type outer bound on the secrecy capacity region is provided for a multi-antenna broadcast channel with confidential messages. Next, a dirty-paper secure coding scheme and its simplified version are described. For each case, the corresponding achievable rate region is derived under the perfect secrecy requirement. Finally, two numerical examples demonstrate that the Sato-type outer bound is consistent with the boundary of the simplified dirty-paper coding secrecy rate region.", ["Privacy", "Communication", "Computer network", "Wireless", "MIMO", "Eavesdropping", "Antenna (radio)"]], ["Symbol Error Rates of Maximum-Likelihood Detector: Convex/Concave Behavior and Applications", "Convexity/concavity properties of symbol error rates (SER) of the maximum likelihood detector operating in the AWGN channel (non-fading and fading) are studied. Generic conditions are identified under which the SER is a convex/concave function of the SNR. Universal bounds for the SER 1st and 2nd derivatives are obtained, which hold for arbitrary constellations and are tight for some of them. Applications of the results are discussed, which include optimum power allocation in spatial multiplexing systems, optimum power/time sharing to decrease or increase (jamming problem) error rate, and implication for fading channels.", ["Concave function", "Additive white Gaussian noise", "Spatial multiplexing", "Multiplexing", "Maximum likelihood"]], ["Artificial Intelligence for Conflict Management", "Militarised conflict is one of the risks that have a significant impact on society. Militarised Interstate Dispute (MID) is defined as an outcome of interstate interactions, which result on either peace or conflict. Effective prediction of the possibility of conflict between states is an important decision support tool for policy makers. In a previous research, neural networks (NNs) have been implemented to predict the MID. Support Vector Machines (SVMs) have proven to be very good prediction techniques and are introduced for the prediction of MIDs in this study and compared to neural networks. The results show that SVMs predict MID better than NNs while NNs give more consistent and easy to interpret sensitivity analysis than SVMs.", ["Artificial intelligence", "Mentioned in Despatches", "Sensitivity analysis", "Decision support system", "Support vector machine", "Conflict resolution", "Neural network"]], ["Control of Complex Systems Using Bayesian Networks and Genetic Algorithm", "A method based on Bayesian neural networks and genetic algorithm is proposed to control the fermentation process. The relationship between input and output variables is modelled using Bayesian neural network that is trained using hybrid Monte Carlo method. A feedback loop based on genetic algorithm is used to change input variables so that the output variables are as close to the desired target as possible without the loss of confidence level on the prediction that the neural network gives. The proposed procedure is found to reduce the distance between the desired target and measured outputs significantly.", ["Genetic algorithm", "Monte Carlo method", "Neural network", "Confidence interval", "Bayesian network", "Fermentation (biochemistry)", "Feedback", "Genetics", "Prediction"]], ["Kinematic Calibration of the Orthoglide-Type Mechanisms", "The paper proposes a novel calibration approach for the Orthoglide-type mechanisms based on observations of the manipulator leg parallelism during motions between the prespecified test postures. It employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. They are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the TCP along the Cartesian axes. Using the measured differences, the developed algorithm estimates the joint offsets that are treated as the most essential parameters to be adjusted. The sensitivity of the measurement methods and the calibration accuracy are also studied. Experimental results are presented that demonstrate validity of the proposed calibration technique.", ["Cartesian coordinate system", "Measurement", "Calibration", "Transmission Control Protocol", "Magnetism", "Kinematics", "Algorithm"]], ["The Design of a Novel Prismatic Drive for a Three-DOF Parallel-Kinematics Machine", "The design of a novel prismatic drive is reported in this paper. This transmission is based on Slide-O-Cam, a cam mechanism with multiple rollers mounted on a common translating follower. The design of Slide-O-Cam was reported elsewhere. This drive thus provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. Such properties can be used to design new transmissions for parallel-kinematics machines. In this paper, this transmission is optimized to replace ball-screws in Orthoglide, a three-DOF parallel robot optimized for machining applications.", ["Kinematics", "Cam", "Machining", "Friction", "Pinion"]], ["Calibration of quasi-isotropic parallel kinematic Machines: Orthoglide", "The paper proposes a novel approach for the geometrical model calibration of quasi-isotropic parallel kinematic mechanisms of the Orthoglide family. It is based on the observations of the manipulator leg parallelism during motions between the specific test postures and employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. They are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the TCP along the Cartesian axes. Using the measured differences, the developed algorithm estimates the joint offsets and the leg lengths that are treated as the most essential parameters. Validity of the proposed calibration technique is confirmed by the experimental results.", ["Kinematics", "Magnetism", "Algorithm", "Isotropy", "Validity", "Calibration", "Transmission Control Protocol", "Cartesian coordinate system"]], ["Rate Adaptation for Cognitive Radio under Interference from Primary Spectrum User", "A cognitive radio can operate as a secondary system in a given spectrum. This operation should use limited power in order not to disturb the communication by primary spectrum user. Under such conditions, in this paper we investigate how to maximize the spectral efficiency in the secondary system. A secondary receiver observes a multiple access channel of two users, the secondary and the primary transmitter, respectively. We show that, for spectrally-efficient operation, the secondary system should apply Opportunistic Interference Cancellation (OIC). With OIC, the secondary system decodes the primary signal when such an opportunity is created by the primary rate and the power received from the primary system. For such an operation, we derive the achievable data rate in the secondary system. When the primary signal is decodable, we devise a method, based on superposition coding, by which the secondary system can achieve the maximal possible rate. Finally, we investigate the power allocation in the secondary system when multiple channels are used. We show that the optimal power allocation with OIC can be achieved through intercepted water-filling instead of the conventional water-filling. The results show a significant gain for the rate achieved through an opportunistic interference cancellation.", ["Cognitive radio", "Receiver (radio)", "Radio", "Transmitter", "Communication", "Spectral efficiency", "Channel access method", "Electromagnetic spectrum", "Channel (communications)", "Primary Rate Interface", "Data"]], ["Evolving Symbolic Controllers", "The idea of symbolic controllers tries to bridge the gap between the top-down manual design of the controller architecture, as advocated in Brooks' subsumption architecture, and the bottom-up designer-free approach that is now standard within the Evolutionary Robotics community. The designer provides a set of elementary behavior, and evolution is given the goal of assembling them to solve complex tasks. Two experiments are presented, demonstrating the efficiency and showing the recursiveness of this approach. In particular, the sensitivity with respect to the proposed elementary behaviors, and the robustness w.r.t. generalization of the resulting controllers are studied in detail.", ["Subsumption architecture", "Evolution", "Architecture", "Robotics"]], ["Design of a 3 Axis Parallel Machine Tool for High Speed Machining: The Orthoglide", "The Orthoglide project aims at designing a new 3-axis machine tool for High Speed Machining. Basis kinematics is a 3 degree-of-freedom translational parallel mechanism. This basis was submitted to isotropic and manipulability constraints that allowed the optmization of its kinematic architecture and legs architecture. Thus, several leg morphologies are convenient for the chosen mechanism. We explain the process that led us to the choice we made for the Orthoglide. A static study is presented to show how singular configurations of the legs can cause stiffness problems.", ["Isotropy", "Stiffness", "Machine tool", "Kinematics", "Architecture"]], ["The Isoconditioning Loci of Planar Three-DOF Parallel Manipulators", "The subject of this paper is a special class of parallel manipulators. First, we analyze a family of three-degree-of-freedom manipulators. Two Jacobian matrices appear in the kinematic relations between the joint-rate and the Cartesian-velocity vectors, which are called the \"inverse kinematics\" and the \"direct kinematics\" matrices. The singular configurations of these matrices are studied. The isotropic configurations are then studied based on the characteristic length of this manipulator. The isoconditioning loci of all Jacobian matrices are computed to define a global performance index to compare the different working modes.", ["Inverse kinematics", "Jacobian matrix and determinant", "Matrix (mathematics)", "Cartesian coordinate system", "Isotropy", "Velocity", "Plane (geometry)", "Euclidean vector", "Invertible matrix", "Inverse function", "Kinematics"]], ["A Novel method for the design of 2-DOF Parallel mechanisms for machining applications", "Parallel Kinematic Mechanisms (PKM) are interesting alternative designs for machine tools. A design method based on velocity amplification factors analysis is presented in this paper. The comparative study of two simple two-degree-of-freedom PKM dedicated to machining applications is led through this method: the common desired properties are the largest square Cartesian workspace for given kinetostatic performances. The orientation and position of the Cartesian workspace are chosen to avoid singularities and to produce the best ratio between Cartesian workspace size and mechanism size. The machine size of each resulting design is used as a comparative criterion.", ["Machining", "Machine tool", "Degrees of freedom (mechanics)", "Velocity", "Cartesian coordinate system"]], ["Design of a Three-Axis Isotropic Parallel Manipulator for Machining Applications: The Orthoglide", "The orthoglide is a 3-DOF parallel mechanism designed at IRCCyN for machining applications. It features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The orthoglide has been designed as function of a prescribed Cartesian workspace with prescribed kinetostatic performances. The interesting features of the orthoglide are a regular Cartesian workspace shape, uniform performances in all directions and good compactness. A small-scale prototype of the orthoglide under development is presented at the end of this paper.", ["Machining", "Isotropy", "Compact space", "Orthogonality"]], ["Workspace Analysis of the Orthoglide using Interval Analysis", "This paper addresses the workspace analysis of the orthoglide, a 3-DOF parallel mechanism designed for machining applications. This machine features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The workspace analysis is conducted on the bases of prescribed kinetostatic performances. The interesting features of the orthoglide are a regular Cartesian workspace shape, uniform performances in all directions and good compactness. Interval analysis based methods for computing the dextrous workspace and the largest cube enclosed in this workspace are presented.", ["Orthogonality", "Machining", "Computing", "Compact space"]], ["P\\'eriph\\'eriques haptiques et simulation d'objets, de robots et de mannequins dans un environnement de CAO-Robotique : eM-Virtual Desktop", "This paper presents the development of a new software in order to manage objects, robots and mannequins in using the possibilities given by the haptic feedback of the Phantom desktop devices. The haptic device provides 6 positional degree of freedom sensing but three degrees force feedback. This software called eM-Virtual Desktop is integrated in the Tecnomatix's solution called eM-Workplace. The eM-Workplace provides powerful solutions for planning and designing of complex assembly facilities, lines and workplaces. In the digital mockup context, the haptic interfaces can be used to reduce the development cycle of products. Three different loops are used to manage the graphic, the collision detection and the haptic feedback according to theirs own frequencies. The developed software is currently tested in industrial context by a European automotive constructor.", ["Automotive industry", "Mockup", "Haptic technology", "Frequency", "Computer software", "Feedback"]], ["Predicting the Presence of Internet Worms using Novelty Detection", "Internet worms cause billions of dollars in damage yearly, affecting millions of users worldwide. For countermeasures to be deployed timeously, it is necessary to use an automated system to detect the spread of a worm. This paper discusses a method of determining the presence of a worm, based on routing information currently available from Internet routers. An autoencoder, which is a specialized type of neural network, was used to detect anomalies in normal routing behavior. The autoencoder was trained using information from a single router, and was able to detect both global instability caused by worms as well as localized routing instability.", ["Router (computing)", "Neural network", "Worm", "Router"]], ["Robust Multi-Cellular Developmental Design", "This paper introduces a continuous model for Multi-cellular Developmental Design. The cells are fixed on a 2D grid and exchange \"chemicals\" with their neighbors during the growth process. The quantity of chemicals that a cell produces, as well as the differentiation value of the cell in the phenotype, are controlled by a Neural Network (the genotype) that takes as inputs the chemicals produced by the neighboring cells at the previous time step. In the proposed model, the number of iterations of the growth process is not pre-determined, but emerges during evolution: only organisms for which the growth process stabilizes give a phenotype (the stable state), others are declared nonviable. The optimization of the controller is done using the NEAT algorithm, that optimizes both the topology and the weights of the Neural Networks. Though each cell only receives local information from its neighbors, the experimental results of the proposed approach on the 'flags' problems (the phenotype must match a given 2D pattern) are almost as good as those of a direct regression approach using the same model with global information. Moreover, the resulting multi-cellular organisms exhibit almost perfect self-healing characteristics.", ["Mathematical optimization", "Neural network", "Multicellular organism", "Algorithm", "Topology", "Cell (biology)", "Phenotype", "Near-Earth Asteroid Tracking", "Genotype", "Evolution", "2D computer graphics", "Regression analysis", "Chemical substance"]], ["Diversity-Multiplexing Tradeoff via Asymptotic Analysis of Large MIMO Systems", "Diversity-multiplexing tradeoff (DMT) presents a compact framework to compare various MIMO systems and channels in terms of the two main advantages they provide (i.e. high data rate and/or low error rate). This tradeoff was characterized asymptotically (SNR-> infinity) for i.i.d. Rayleigh fading channel by Zheng and Tse [1]. The asymptotic DMT overestimates the finite-SNR one [2]. In this paper, using the recent results on the asymptotic (in the number of antennas) outage capacity distribution, we derive and analyze the finite-SNR DMT for a broad class of channels (not necessarily Rayleigh fading). Based on this, we give the convergence conditions for the asymptotic DMT to be approached by the finite-SNR one. The multiplexing gain definition is shown to affect critically the convergence point: when the multiplexing gain is defined via the mean (ergodic) capacity, the convergence takes place at realistic SNR values. Furthermore, in this case the diversity gain can also be used to estimate the outage probability with reasonable accuracy. The multiplexing gain definition via the high-SNR asymptote of the mean capacity (as in [1]) results in very slow convergence for moderate to large systems (as 1/ln(SNR)^2) and, hence, the asymptotic DMT cannot be used at realistic SNR values. For this definition, the high-SNR threshold increases exponentially in the number of antennas and in the multiplexing gain. For correlated keyhole channel, the diversity gain is shown to decrease with correlation and power imbalance of the channel. While the SNR-asymptotic DMT of Zheng and Tse does not capture this effect, the size-asymptotic DMT does.", ["Asymptote", "Rayleigh fading", "Correlation and dependence", "Multiplexing", "Probability", "Infinity", "Natural logarithm", "MIMO", "Exponential growth", "Signal-to-noise ratio"]], ["On Optimum Power Allocation for the V-BLAST", "A unified analytical framework for optimum power allocation in the unordered V-BLAST algorithm and its comparative performance analysis are presented. Compact closed-form approximations for the optimum power allocation are derived, based on average total and block error rates. The choice of the criterion has little impact on the power allocation and, overall, the optimum strategy is to allocate more power to lower step transmitters and less to higher ones. High-SNR approximations for optimized average block and total error rates are given. The SNR gain of optimization is rigorously defined and studied using analytical tools, including lower and upper bounds, high and low SNR approximations. The gain is upper bounded by the number of transmitters, for any modulation format and type of fading channel. While the average optimization is less complex than the instantaneous one, its performance is almost as good at high SNR. A measure of robustness of the optimized algorithm is introduced and evaluated. The optimized algorithm is shown to be robust to perturbations in individual and total transmit powers. Based on the algorithm robustness, a pre-set power allocation is suggested as a low-complexity alternative to the other optimization strategies, which exhibits only a minor loss in performance over the practical SNR range.", ["Modulation", "BLAST", "Fading", "Profiling (computer programming)", "Algorithm", "Mathematical optimization"]], ["The Optimal Design of Three Degree-of-Freedom Parallel Mechanisms for Machining Applications", "The subject of this paper is the optimal design of a parallel mechanism intended for three-axis machining applications. Parallel mechanisms are interesting alternative designs in this context but most of them are designed for three- or six-axis machining applications. In the last case, the position and the orientation of the tool are coupled and the shape of the workspace is complex. The aim of this paper is to use a simple parallel mechanism with two-degree-of-freedom (dof) for translational motions and to add one leg to have one-dof rotational motion. The kinematics and singular configurations are studied as well as an optimization method. The three-degree-of-freedom mechanisms analyzed in this paper can be extended to four-axis machines by adding a fourth axis in series with the first two.", ["Mathematical optimization", "Kinematics", "Machining", "Rotation around a fixed axis"]], ["Classification of one family of 3R positioning Manipulators", "The aim of this paper is to classify one family of 3R serial positioning manipulators. This categorization is based on the number of cusp points of quaternary, binary, generic and non-generic manipulators. It was found three subsets of manipulators with 0, 2 or 4 cusp points and one homotopy class for generic quaternary manipulators. This classification allows us to define the design parameters for which the manipulator is cuspidal or not, i.e., for which the manipulator can or cannot change posture without meeting a singularity, respectively.", ["Homotopy", "Quaternary"]], ["Degree Optimization and Stability Condition for the Min-Sum Decoder", "The min-sum (MS) algorithm is arguably the second most fundamental algorithm in the realm of message passing due to its optimality (for a tree code) with respect to the {\\em block error} probability \\cite{Wiberg}. There also seems to be a fundamental relationship of MS decoding with the linear programming decoder \\cite{Koetter}. Despite its importance, its fundamental properties have not nearly been studied as well as those of the sum-product (also known as BP) algorithm. We address two questions related to the MS rule. First, we characterize the stability condition under MS decoding. It turns out to be essentially the same condition as under BP decoding. Second, we perform a degree distribution optimization. Contrary to the case of BP decoding, under MS decoding the thresholds of the best degree distributions for standard irregular LDPC ensembles are significantly bounded away from the Shannon threshold. More precisely, on the AWGN channel, for the best codes that we find, the gap to capacity is 1dB for a rate 0.3 code and it is 0.4dB when the rate is 0.9 (the gap decreases monotonically as we increase the rate). We also used the optimization procedure to design codes for modified MS algorithm where the output of the check node is scaled by a constant $1/\\alpha$. For $\\alpha = 1.25$, we observed that the gap to capacity was lesser for the modified MS algorithm when compared with the MS algorithm. However, it was still quite large, varying from 0.75 dB to 0.2 dB for rates between 0.3 and 0.9. We conclude by posing what we consider to be the most important open questions related to the MS algorithm.", ["Mathematical optimization", "Linear programming", "Decibel", "BP", "Probability", "Additive white Gaussian noise", "Low-density parity-check code", "Decoder", "Degree distribution", "Algorithm", "Message passing", "Monotonic function"]], ["An Approximation Algorithm for Shortest Descending Paths", "A path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t. No efficient algorithm is known to find a shortest descending path (SDP) from s to t in a polyhedral terrain. We give a simple approximation algorithm that solves the SDP problem on general terrains. Our algorithm discretizes the terrain with O(n^2 X / e) Steiner points so that after an O(n^2 X / e * log(n X /e))-time preprocessing phase for a given vertex s, we can determine a (1+e)-approximate SDP from s to any point v in O(n) time if v is either a vertex of the terrain or a Steiner point, and in O(n X /e) time otherwise. Here n is the size of the terrain, and X is a parameter of the geometry of the terrain.", ["Steiner tree problem", "Monotone polygon", "Approximation algorithm", "Preprocessor", "Approximation"]], ["Logic Column 18: Alternative Logics: A Book Review", "This article discusses two books on the topic of alternative logics in science: \"Deviant Logic\", by Susan Haack, and \"Alternative Logics: Do Sciences Need Them?\", edited by Paul Weingartner.", ["Susan Haack", "Logic"]], ["Matroid Pathwidth and Code Trellis Complexity", "We relate the notion of matroid pathwidth to the minimum trellis state-complexity (which we term trellis-width) of a linear code, and to the pathwidth of a graph. By reducing from the problem of computing the pathwidth of a graph, we show that the problem of determining the pathwidth of a representable matroid is NP-hard. Consequently, the problem of computing the trellis-width of a linear code is also NP-hard. For a finite field $\\F$, we also consider the class of $\\F$-representable matroids of pathwidth at most $w$, and correspondingly, the family of linear codes over $\\F$ with trellis-width at most $w$. These are easily seen to be minor-closed. Since these matroids (and codes) have branchwidth at most $w$, a result of Geelen and Whittle shows that such matroids (and the corresponding codes) are characterized by finitely many excluded minors. We provide the complete list of excluded minors for $w=1$, and give a partial list for $w=2$.", ["NP-hard", "Matroid", "Finite field", "Linear code", "Path decomposition", "Branch-decomposition", "Graph (mathematics)", "Computing", "Complexity"]], ["Machine and Component Residual Life Estimation through the Application of Neural Networks", "This paper concerns the use of neural networks for predicting the residual life of machines and components. In addition, the advantage of using condition-monitoring data to enhance the predictive capability of these neural networks was also investigated. A number of neural network variations were trained and tested with the data of two different reliability-related datasets. The first dataset represents the renewal case where the failed unit is repaired and restored to a good-as-new condition. Data was collected in the laboratory by subjecting a series of similar test pieces to fatigue loading with a hydraulic actuator. The average prediction error of the various neural networks being compared varied from 431 to 841 seconds on this dataset, where test pieces had a characteristic life of 8,971 seconds. The second dataset was collected from a group of pumps used to circulate a water and magnetite solution within a plant. The data therefore originated from a repaired system affected by reliability degradation. When optimized, the multi-layer perceptron neural networks trained with the Levenberg-Marquardt algorithm and the general regression neural network produced a sum-of-squares error within 11.1% of each other. The potential for using neural networks for residual life prediction and the advantage of incorporating condition-based data into the model were proven for both examples.", ["Hydraulics", "Neural network", "Squared deviations", "Plant", "Perceptron", "Magnetite", "Laboratory", "Actuator", "Algorithm", "Multilayer perceptron", "Nervous system"]], ["The Orthoglide: Kinematics and Workspace Analysis", "The paper addresses kinematic and geometrical aspects of the Orthoglide, a three-DOF parallel mechanism. This machine consists of three fixed linear joints, which are mounted orthogonally, three identical legs and a mobile platform, which moves in the Cartesian x-y-z space with fixed orientation. New solutions to solve inverse/direct kinematics are proposed and a detailed workspace analysis is performed taking into account specific joint limit constraints.", ["Kinematics", "Orthogonality", "Geometry", "Cartesian coordinate system", "Orientation (vector space)"]], ["Subjective Evaluation of Forms in an Immersive Environment", "User's perception of product, by essence subjective, is a major topic in marketing and industrial design. Many methods, based on users' tests, are used so as to characterise this perception. We are interested in three main methods: multidimensional scaling, semantic differential method, and preference mapping. These methods are used to built a perceptual space, in order to position the new product, to specify requirements by the study of user's preferences, to evaluate some product attributes, related in particular to style (aesthetic). These early stages of the design are primordial for a good orientation of the project. In parallel, virtual reality tools and interfaces are more and more efficient for suggesting to the user complex feelings, and creating in this way various levels of perceptions. In this article, we present on an example the use of multidimensional scaling, semantic differential method and preference mapping for the subjective assessment of virtual products. These products, which geometrical form is variable, are defined with a CAD model and are proposed to the user with a spacemouse and stereoscopic glasses. Advantages and limitations of such evaluation is next discussed..", ["Semantics", "Perception", "Stereoscopy", "Industrial design", "Virtual reality", "Subjectivity", "Geometry", "Multidimensional scaling", "Semantic differential", "Glasses", "Aesthetics", "Computer-aided design"]], ["Realistic Rendering of Kinetostatic Indices of Mechanisms", "The work presented in this paper is related to the use of a haptic device in an environment of robotic simulation. Such device introduces a new approach to feel and to understand the boundaries of the workspace of mechanisms as well as its kinetostatic properties. Indeed, these concepts are abstract and thus often difficult to understand for the end-users. To catch his attention, we propose to amplify the problems of the mechanisms in order to help him to take the good decisions.", ["Simulation", "Robotics", "Natural environment"]], ["A New Concept of Modular Parallel Mechanism for Machining Applications", "The subject of this paper is the design of a new concept of modular parallel mechanisms for three, four or five-axis machining applications. Most parallel mechanisms are designed for three- or six-axis machining applications. In the last case, the position and the orientation of the tool are coupled and the shape of the workspace is complex. The aim of this paper is to use a simple parallel mechanism with two-degree-of-freedom (dof) for translation motions and to add one or two legs to add one or two-dofs for rotation motions. The kinematics and singular configurations are studied for each mechanism.", ["Rotation", "Kinematics", "Machining"]], ["A Workspace based Classification of 3R Orthogonal Manipulators", "A classification of a family of 3-revolute (3R) positioning manipulators is established. This classification is based on the topology of their workspace. The workspace is characterized in a half-cross section by the singular curves of the manipulator. The workspace topology is defined by the number of cusps and nodes that appear on these singular curves. The design parameters space is shown to be partitioned into nine subspaces of distinct workspace topologies. Each separating surface is given as an explicit expression in the DH-parameters.", ["Topology"]], ["Singularity Surfaces and Maximal Singularity-Free Boxes in the Joint Space of Planar 3-RPR Parallel Manipulators", "In this paper, a method to compute joint space singularity surfaces of 3-RPR planar parallel manipulators is first presented. Then, a procedure to determine maximal joint space singularity-free boxes is introduced. Numerical examples are given in order to illustrate graphically the results. This study is of high interest for planning trajectories in the joint space of 3-RPR parallel manipulators and for manipulators design as it may constitute a tool for choosing appropriate joint limits and thus for sizing the link lengths of the manipulator.", ["Trajectory"]], ["Kinematics analysis of the parallel module of the VERNE machine", "The paper derives the inverse and forward kinematic equations of a spatial three-degree-of-freedom parallel mechanism, which is the parallel module of a hybrid serial-parallel 5-axis machine tool. This parallel mechanism consists of a moving platform that is connected to a fixed base by three non-identical legs. Each leg is made up of one prismatic and two pair spherical joint, which are connected in a way that the combined effects of the three legs lead to an over-constrained mechanism with complex motion. This motion is defined as a simultaneous combination of rotation and translation.", ["Kinematics", "Rotation", "Ball joint", "Machine tool", "Sphere", "Space"]], ["Does P=NP?", "This paper has been withdrawn Abstract: This paper has been withdrawn by the author due to the publication.", ["P versus NP problem", "Author"]], ["An Algorithm for Computing Cusp Points in the Joint Space of 3-RPR Parallel Manipulators", "This paper presents an algorithm for detecting and computing the cusp points in the joint space of 3-RPR planar parallel manipulators. In manipulator kinematics, cusp points are special points, which appear on the singular curves of the manipulators. The nonsingular change of assembly mode of 3-RPR parallel manipulators was shown to be associated with the existence of cusp points. At each of these points, three direct kinematic solutions coincide. In the literature, a condition for the existence of three coincident direct kinematic solutions was established, but has never been exploited, because the algebra involved was too complicated to be solved. The algorithm presented in this paper solves this equation and detects all the cusp points in the joint space of these manipulators.", ["Invertible matrix", "Kinematics", "Computing", "Literature", "Algebra", "Algorithm"]], ["Typer la d\\'e-s\\'erialisation sans s\\'erialiser les types", "In this paper, we propose a way of assigning static type information to unmarshalling functions and we describe a verification technique for unmarshalled data that preserves the execution safety provided by static type checking. This technique, whose correctness is proven, relies on singleton types whose values are transmitted to unmarshalling routines at runtime, and on an efficient checking algorithm able to deal with sharing and cycles.", ["Serialization", "Algorithm", "Type system", "Function (mathematics)"]], ["DWEB: A Data Warehouse Engineering Benchmark", "Data warehouse architectural choices and optimization techniques are critical to decision support query performance. To facilitate these choices, the performance of the designed data warehouse must be assessed. This is usually done with the help of benchmarks, which can either help system users comparing the performances of different systems, or help system engineers testing the effect of various design choices. While the TPC standard decision support benchmarks address the first point, they are not tuneable enough to address the second one and fail to model different data warehouse schemas. By contrast, our Data Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses and workloads. DWEB is fully parameterized to fulfill data warehouse design needs. However, two levels of parameterization keep it relatively easy to tune. Finally, DWEB is implemented as a Java free software that can be interfaced with most existing relational database management systems. A sample usage of DWEB is also provided in this paper.", ["Data warehouse", "Free software", "Ad hoc", "Mathematical optimization", "Relational database", "Java (programming language)", "Database", "Engineering", "Warehouse", "Database management system"]], ["DOEF: A Dynamic Object Evaluation Framework", "In object-oriented or object-relational databases such as multimedia databases or most XML databases, access patterns are not static, i.e., applications do not always access the same objects in the same order repeatedly. However, this has been the way these databases and associated optimisation techniques like clustering have been evaluated up to now. This paper opens up research regarding this issue by proposing a dynamic object evaluation framework (DOEF) that accomplishes access pattern change by defining configurable styles of change. This preliminary prototype has been designed to be open and fully extensible. To illustrate the capabilities of DOEF, we used it to compare the performances of four state of the art dynamic clustering algorithms. The results show that DOEF is indeed effective at determining the adaptability of each dynamic clustering algorithm to changes in access pattern.", ["XML", "Object-oriented programming", "XML database", "Database", "Relational database", "Algorithm", "Mathematical optimization", "Object (computer science)", "Prototype"]], ["Decision tree modeling with relational views", "Data mining is a useful decision support technique that can be used to discover production rules in warehouses or corporate data. Data mining research has made much effort to apply various mining algorithms efficiently on large databases. However, a serious problem in their practical application is the long processing time of such algorithms. Nowadays, one of the key challenges is to integrate data mining methods within the framework of traditional database systems. Indeed, such implementations can take advantage of the efficiency provided by SQL engines. In this paper, we propose an integrating approach for decision trees within a classical database system. In other words, we try to discover knowledge from relational databases, in the form of production rules, via a procedure embedding SQL queries. The obtained decision tree is defined by successive, related relational views. Each view corresponds to a given population in the underlying decision tree. We selected the classical Induction Decision Tree (ID3) algorithm to build the decision tree. To prove that our implementation of ID3 works properly, we successfully compared the output of our procedure with the output of an existing and validated data mining software, SIPINA. Furthermore, since our approach is tuneable, it can be generalized to any other similar decision tree-based method.", ["Data mining", "Decision tree", "Relational database", "Database management system", "Database", "Database system", "ID3", "SQL", "Algorithm", "Information retrieval", "Data", "Knowledge", "Decision tree learning"]], ["Warehousing Web Data", "In a data warehousing process, mastering the data preparation phase allows substantial gains in terms of time and performance when performing multidimensional analysis or using data mining algorithms. Furthermore, a data warehouse can require external data. The web is a prevalent data source in this context. In this paper, we propose a modeling process for integrating diverse and heterogeneous (so-called multiform) data into a unified format. Furthermore, the very schema definition provides first-rate metadata in our data warehousing context. At the conceptual level, a complex object is represented in UML. Our logical model is an XML schema that can be described with a DTD or the XML-Schema language. Eventually, we have designed a Java prototype that transforms our multiform input data into XML documents representing our physical model. Then, the XML documents we obtain are mapped into a relational database we view as an ODS (Operational Data Storage), whose content will have to be re-modeled in a multidimensional way to allow its storage in a star schema-based warehouse and, later, its analysis.", ["Star schema", "Data mining", "XML", "Homogeneity and heterogeneity", "Metadata", "XML schema", "Unified Modeling Language", "Relational database", "Logical data model", "Document Type Definition", "Data warehouse", "Prototype", "Conceptual model", "Database", "Object (computer science)", "Java (programming language)", "Data"]], ["Web data modeling for integration in data warehouses", "In a data warehousing process, the data preparation phase is crucial. Mastering this phase allows substantial gains in terms of time and performance when performing a multidimensional analysis or using data mining algorithms. Furthermore, a data warehouse can require external data. The web is a prevalent data source in this context, but the data broadcasted on this medium are very heterogeneous. We propose in this paper a UML conceptual model for a complex object representing a superclass of any useful data source (databases, plain texts, HTML and XML documents, images, sounds, video clips...). The translation into a logical model is achieved with XML, which helps integrating all these diverse, heterogeneous data into a unified format, and whose schema definition provides first-rate metadata in our data warehousing context. Moreover, we benefit from XML's flexibility, extensibility and from the richness of the semi-structured data model, but we are still able to later map XML documents into a database if more structuring is needed.", ["Data mining", "Data modeling", "Homogeneity and heterogeneity", "XML", "Metadata", "Extensibility", "HTML", "Unified Modeling Language", "Semi-structured data", "Database", "Data warehouse", "Data model", "Logical data model", "Conceptual model", "Data", "Algorithm"]], ["Mixing the Objective Caml and C# Programming Models in the .Net Framework", "We present a new code generator, called O'Jacare.net, to inter-operate between C# and Objective Caml through their object models. O'Jacare.net defines a basic IDL (Interface Definition Language) that describes classes and interfaces in order to communicate between Objective Caml and C#. O'Jacare.net generates all needed wrapper classes and takes advantage of static type checking in both worlds. Although the IDL intersects these two object models, O'Jacare.net allows to combine features from both.", ["Objective Caml", "Interoperability", "Type system", ".NET Framework", "Interface description language", "C (programming language)", "Type checking", "Interface (computing)", "Computer programming", "Object (computer science)", "Caml"]], ["Actin - Technical Report", "The Boolean satisfiability problem (SAT) can be solved efficiently with variants of the DPLL algorithm. For industrial SAT problems, DPLL with conflict analysis dependent dynamic decision heuristics has proved to be particularly efficient, e.g. in Chaff. In this work, algorithms that initialize the variable activity values in the solver MiniSAT v1.14 by analyzing the CNF are evolved using genetic programming (GP), with the goal to reduce the total number of conflicts of the search and the solving time. The effect of using initial activities other than zero is examined by initializing with random numbers. The possibility of countering the detrimental effects of reordering the CNF with improved initialization is investigated. The best result found (with validation testing on further problems) was used in the solver Actin, which was submitted to SAT-Race 2006.", ["DPLL algorithm", "Boolean satisfiability problem", "Genetic programming", "Conjunctive normal form", "Algorithm", "Heuristic", "Actin", "Satisfiability", "SAT", "Genetics"]], ["A note on module-composed graphs", "In this paper we consider module-composed graphs, i.e. graphs which can be defined by a sequence of one-vertex insertions v_1,...,v_n, such that the neighbourhood of vertex v_i, 2<= i<= n, forms a module (a homogeneous set) of the graph defined by vertices v_1,..., v_{i-1}. We show that module-composed graphs are HHDS-free and thus homogeneously orderable, weakly chordal, and perfect. Every bipartite distance hereditary graph, every (co-2C_4,P_4)-free graph and thus every trivially perfect graph is module-composed. We give an O(|V_G|(|V_G|+|E_G|)) time algorithm to decide whether a given graph G is module-composed and construct a corresponding module-sequence. For the case of bipartite graphs, module-composed graphs are exactly distance hereditary graphs, which implies simple linear time algorithms for their recognition and construction of a corresponding module-sequence.", ["Trivially perfect graph", "Graph (mathematics)", "Algorithm", "Perfect graph", "Linear time", "Vertex (geometry)", "Sequence"]], ["Unfolding Manhattan Towers", "We provide an algorithm for unfolding the surface of any orthogonal polyhedron that falls into a particular shape class we call Manhattan Towers, to a nonoverlapping planar orthogonal polygon. The algorithm cuts along edges of a 4x5x1 refinement of the vertex grid.", ["Rectilinear polygon", "Orthogonality", "Algorithm", "Polygon", "Polyhedron"]], ["Wireless Networking to Support Data and Voice Communication Using Spread Spectrum Technology in The Physical Layer", "Wireless networking is rapidly growing and becomes an inexpensive technology which allows multiple users to simultaneously access the network and the internet while roaming about the campus. In the present work, the software development of a wireless LAN(WLAN) is highlighted. This WLAN utilizes direct sequence spread spectrum (DSSS) technology at 902MHz RF carrier frequency in its physical layer. Cost effective installation and antijaming property of spread spectrum technology are the major advantages of this work.", ["Wireless network", "Wireless LAN", "Local area network", "Wireless", "Computer software", "Direct-sequence spread spectrum", "Physical Layer", "Spread spectrum", "Radio frequency", "Software development", "Computer network", "Carrier wave", "Hertz", "Communication"]], ["HMM Speaker Identification Using Linear and Non-linear Merging Techniques", "Speaker identification is a powerful, non-invasive and in-expensive biometric technique. The recognition accuracy, however, deteriorates when noise levels affect a specific band of frequency. In this paper, we present a sub-band based speaker identification that intends to improve the live testing performance. Each frequency sub-band is processed and classified independently. We also compare the linear and non-linear merging techniques for the sub-bands recognizer. Support vector machines and Gaussian Mixture models are the non-linear merging techniques that are investigated. Results showed that the sub-band based method used with linear merging techniques enormously improved the performance of the speaker identification over the performance of wide-band recognizers when tested live. A live testing improvement of 9.78% was achieved", ["Support vector machine", "Biometrics", "Nonlinear system", "Linear", "Frequency"]], ["A Class of LDPC Erasure Distributions with Closed-Form Threshold Expression", "In this paper, a family of low-density parity-check (LDPC) degree distributions, whose decoding threshold on the binary erasure channel (BEC) admits a simple closed form, is presented. These degree distributions are a subset of the check regular distributions (i.e. all the check nodes have the same degree), and are referred to as $p$-positive distributions. It is given proof that the threshold for a $p$-positive distribution is simply expressed by $[\\lambda'(0)\\rho'(1)]^{-1}$. Besides this closed form threshold expression, the $p$-positive distributions exhibit three additional properties. First, for given code rate, check degree and maximum variable degree, they are in some cases characterized by a threshold which is extremely close to that of the best known check regular distributions, under the same set of constraints. Second, the threshold optimization problem within the $p$-positive class can be solved in some cases with analytic methods, without using any numerical optimization tool. Third, these distributions can achieve the BEC capacity. The last property is shown by proving that the well-known binomial degree distributions belong to the $p$-positive family.", ["Binary erasure channel", "Code rate", "Mathematical optimization", "Optimization problem", "Erasure", "Subset", "Low-density parity-check code"]], ["Non-Computability of Consciousness", "With the great success in simulating many intelligent behaviors using computing devices, there has been an ongoing debate whether all conscious activities are computational processes. In this paper, the answer to this question is shown to be no. A certain phenomenon of consciousness is demonstrated to be fully represented as a computational process using a quantum computer. Based on the computability criterion discussed with Turing machines, the model constructed is shown to necessarily involve a non-computable element. The concept that this is solely a quantum effect and does not work for a classical case is also discussed.", ["Quantum computer", "Computation", "Computability", "Computer", "Turing machine", "Quantum mechanics", "Alan Turing"]], ["Principal Component Analysis and Automatic Relevance Determination in Damage Identification", "This paper compares two neural network input selection schemes, the Principal Component Analysis (PCA) and the Automatic Relevance Determination (ARD) based on Mac-Kay's evidence framework. The PCA takes all the input data and projects it onto a lower dimension space, thereby reduc-ing the dimension of the input space. This input reduction method often results with parameters that have significant influence on the dynamics of the data being diluted by those that do not influence the dynamics of the data. The ARD selects the most relevant input parameters and discards those that do not contribute significantly to the dynamics of the data being modelled. The ARD sometimes results with important input parameters being discarded thereby compromising the dynamics of the data. The PCA and ARD methods are implemented together with a Multi-Layer-Perceptron (MLP) network for fault identification in structures and the performance of the two methods is as-sessed. It is observed that ARD and PCA give similar accu-racy levels when used as input-selection schemes. There-fore, the choice of input-selection scheme is dependent on the nature of the data being processed.", ["Perceptron", "Principal component analysis", "Neural network", "ARD (broadcaster)", "Dimension"]], ["Using artificial intelligence for data reduction in mechanical engineering", "In this paper artificial neural networks and support vector machines are used to reduce the amount of vibration data that is required to estimate the Time Domain Average of a gear vibration signal. Two models for estimating the time domain average of a gear vibration signal are proposed. The models are tested on data from an accelerated gear life test rig. Experimental results indicate that the required data for calculating the Time Domain Average of a gear vibration signal can be reduced by up to 75% when the proposed models are implemented.", ["Artificial intelligence", "Time domain", "Neural network", "Mechanical engineering", "Artificial neural network", "Support vector machine"]], ["Evolutionary Optimisation Methods for Template Based Image Registration", "This paper investigates the use of evolutionary optimisation techniques to register a template with a scene image. An error function is created to measure the correspondence of the template to the image. The problem presented here is to optimise the horizontal, vertical and scaling parameters that register the template with the scene. The Genetic Algorithm, Simulated Annealing and Particle Swarm Optimisations are compared to a Nelder-Mead Simplex optimisation with starting points chosen in a pre-processing stage. The paper investigates the precision and accuracy of each method and shows that all four methods perform favourably for image registration. SA is the most precise, GA is the most accurate. PSO is a good mix of both and the Simplex method returns local minima the most. A pre-processing stage should be investigated for the evolutionary methods in order to improve performance. Discrete versions of the optimisation methods should be investigated to further improve computational performance.", ["Simplex algorithm", "Image registration", "Mathematical optimization", "Simulated annealing", "Error function", "Maxima and minima", "Accuracy and precision", "Genetic algorithm", "Algorithm"]], ["Option Pricing Using Bayesian Neural Networks", "Options have provided a field of much study because of the complexity involved in pricing them. The Black-Scholes equations were developed to price options but they are only valid for European styled options. There is added complexity when trying to price American styled options and this is why the use of neural networks has been proposed. Neural Networks are able to predict outcomes based on past data. The inputs to the networks here are stock volatility, strike price and time to maturity with the output of the network being the call option price. There are two techniques for Bayesian neural networks used. One is Automatic Relevance Determination (for Gaussian Approximation) and one is a Hybrid Monte Carlo method, both used with Multi-Layer Perceptrons.", ["Monte Carlo method", "Option style", "Black-Scholes", "Neural network", "Perceptron", "Pricing", "Artificial neural network", "Strike price", "Normal distribution", "Volatility (finance)", "Option (finance)", "Approximation", "Bayesian probability", "Call option"]], ["Capacity of Underspread Noncoherent WSSUS Fading Channels under Peak Signal Constraints", "We characterize the capacity of the general class of noncoherent underspread wide-sense stationary uncorrelated scattering (WSSUS) time-frequency-selective Rayleigh fading channels, under peak constraints in time and frequency and in time only. Capacity upper and lower bounds are found which are explicit in the channel's scattering function and allow to identify the capacity-maximizing bandwidth for a given scattering function and a given peak-to-average power ratio.", ["Stationary process", "Rayleigh fading", "Scattering", "Crest factor", "Frequency"]], ["A Tighter Analysis of Setcover Greedy Algorithm for Test Set", "Setcover greedy algorithm is a natural approximation algorithm for test set problem. This paper gives a precise and tighter analysis of performance guarantee of this algorithm. The author improves the performance guarantee $2\\ln n$ which derives from set cover problem to $1.1354\\ln n$ by applying the potential function technique. In addition, the author gives a nontrivial lower bound $1.0004609\\ln n$ of performance guarantee of this algorithm. This lower bound, together with the matching bound of information content heuristic, confirms the fact information content heuristic is slightly better than setcover greedy algorithm in worst case.", ["Approximation algorithm", "Set cover problem", "Greedy algorithm", "Algorithm", "Upper and lower bounds", "Self-information", "Heuristic"]], ["Scalability and Optimisation of a Committee of Agents Using Genetic Algorithm", "A population of committees of agents that learn by using neural networks is implemented to simulate the stock market. Each committee of agents, which is regarded as a player in a game, is optimised by continually adapting the architecture of the agents using genetic algorithms. The committees of agents buy and sell stocks by following this procedure: (1) obtain the current price of stocks; (2) predict the future price of stocks; (3) and for a given price trade until all the players are mutually satisfied. The trading of stocks is conducted by following these rules: (1) if a player expects an increase in price then it tries to buy the stock; (2) else if it expects a drop in the price, it sells the stock; (3)and the order in which a player participates in the game is random. The proposed procedure is implemented to simulate trading of three stocks, namely, the Dow Jones, the Nasdaq and the S&P 500. A linear relationship between the number of players and agents versus the computational time to run the complete simulation is observed. It is also found that no player has a monopolistic advantage.", ["Genetics", "Time complexity", "Dow Jones & Company", "Neural network", "Simulation", "Genetic algorithm", "Scalability", "Mathematical optimization", "NASDAQ", "Stocks", "Monopoly", "Architecture", "Stock market", "Nervous system", "Stock"]], ["Finite Element Model Updating Using Response Surface Method", "This paper proposes the response surface method for finite element model updating. The response surface method is implemented by approximating the finite element model surface response equation by a multi-layer perceptron. The updated parameters of the finite element model were calculated using genetic algorithm by optimizing the surface response equation. The proposed method was compared to the existing methods that use simulated annealing or genetic algorithm together with a full finite element model for finite element model updating. The proposed method was tested on an unsymmetri-cal H-shaped structure. It was observed that the proposed method gave the updated natural frequen-cies and mode shapes that were of the same order of accuracy as those given by simulated annealing and genetic algorithm. Furthermore, it was observed that the response surface method achieved these results at a computational speed that was more than 2.5 times as fast as the genetic algorithm and a full finite element model and 24 times faster than the simulated annealing.", ["Genetic algorithm", "Genetics", "Perceptron", "Algorithm", "Simulated annealing", "Annealing (metallurgy)", "Multilayer perceptron"]], ["Dynamic Model Updating Using Particle Swarm Optimization Method", "This paper proposes the use of particle swarm optimization method (PSO) for finite element (FE) model updating. The PSO method is compared to the existing methods that use simulated annealing (SA) or genetic algorithms (GA) for FE model for model updating. The proposed method is tested on an unsymmetrical H-shaped structure. It is observed that the proposed method gives updated natural frequencies the most accurate and followed by those given by an updated model that was obtained using the GA and a full FE model. It is also observed that the proposed method gives updated mode shapes that are best correlated to the measured ones, followed by those given by an updated model that was obtained using the SA and a full FE model. Furthermore, it is observed that the PSO achieves this accuracy at a computational speed that is faster than that by the GA and a full FE model which is faster than the SA and a full FE model.", ["Particle swarm optimization", "Simulated annealing", "Genetics", "Normal mode", "Frequency", "Genetic algorithm", "Finite element method", "Algorithm"]], ["Modeling and Controlling Interstate Conflict", "Bayesian neural networks were used to model the relationship between input parameters, Democracy, Allies, Contingency, Distance, Capability, Dependency and Major Power, and the output parameter which is either peace or conflict. The automatic relevance determination was used to rank the importance of input variables. Control theory approach was used to identify input variables that would give a peaceful outcome. It was found that using all four controllable variables Democracy, Allies, Capability and Dependency; or using only Dependency or only Capabilities avoids all the predicted conflicts.", ["Control theory", "Neural network", "Mathematical model", "Scientific modelling", "Nervous system"]], ["Energy-Efficient Resource Allocation in Wireless Networks: An Overview of Game-Theoretic Approaches", "An overview of game-theoretic approaches to energy-efficient resource allocation in wireless networks is presented. Focusing on multiple-access networks, it is demonstrated that game theory can be used as an effective tool to study resource allocation in wireless networks with quality-of-service (QoS) constraints. A family of non-cooperative (distributed) games is presented in which each user seeks to choose a strategy that maximizes its own utility while satisfying its QoS requirements. The utility function considered here measures the number of reliable bits that are transmitted per joule of energy consumed and, hence, is particulary suitable for energy-constrained networks. The actions available to each user in trying to maximize its own utility are at least the choice of the transmit power and, depending on the situation, the user may also be able to choose its transmission rate, modulation, packet size, multiuser receiver, multi-antenna processing algorithm, or carrier allocation strategy. The best-response strategy and Nash equilibrium for each game is presented. Using this game-theoretic framework, the effects of power control, rate control, modulation, temporal and spatial signal processing, carrier allocation strategy and delay QoS constraints on energy efficiency and network capacity are quantified.", ["Joule", "Wireless network", "Wireless", "Nash equilibrium", "Energy", "Antenna (radio)", "Signal processing", "Game theory", "Modulation", "Resource allocation", "MIMO", "Quality of service", "Algorithm", "Channel access method", "Efficient energy use", "Utility", "Power control"]], ["A Game-Theoretic Approach to Energy-Efficient Modulation in CDMA Networks with Delay QoS Constraints", "A game-theoretic framework is used to study the effect of constellation size on the energy efficiency of wireless networks for M-QAM modulation. A non-cooperative game is proposed in which each user seeks to choose its transmit power (and possibly transmit symbol rate) as well as the constellation size in order to maximize its own utility while satisfying its delay quality-of-service (QoS) constraint. The utility function used here measures the number of reliable bits transmitted per joule of energy consumed, and is particularly suitable for energy-constrained networks. The best-response strategies and Nash equilibrium solution for the proposed game are derived. It is shown that in order to maximize its utility (in bits per joule), a user must choose the lowest constellation size that can accommodate the user's delay constraint. This strategy is different from one that would maximize spectral efficiency. Using this framework, the tradeoffs among energy efficiency, delay, throughput and constellation size are also studied and quantified. In addition, the effect of trellis-coded modulation on energy efficiency is discussed.", ["Joule", "Constellation", "Modulation", "Quality of service", "Quadrature amplitude modulation", "Wireless", "Symbol rate", "Spectral efficiency", "Trellis modulation", "Energy", "Wireless network", "Game theory", "Nash equilibrium", "Code division multiple access", "Cooperative game", "Non-cooperative game", "Utility", "Throughput", "Efficient energy use"]], ["Random Linear Network Coding: A free cipher?", "We consider the level of information security provided by random linear network coding in network scenarios in which all nodes comply with the communication protocols yet are assumed to be potential eavesdroppers (i.e. \"nice but curious\"). For this setup, which differs from wiretapping scenarios considered previously, we develop a natural algebraic security criterion, and prove several of its key properties. A preliminary analysis of the impact of network topology on the overall network coding security, in particular for complete directed acyclic graphs, is also included.", ["Graph (mathematics)", "Network topology", "Topology", "Cipher", "Information security", "Communication", "Directed acyclic graph", "Telephone tapping"]], ["Scheduling Dags under Uncertainty", "This paper introduces a parallel scheduling problem where a directed acyclic graph modeling $t$ tasks and their dependencies needs to be executed on $n$ unreliable workers. Worker $i$ executes task $j$ correctly with probability $p_{i,j}$. The goal is to find a regimen $\\Sigma$, that dictates how workers get assigned to tasks (possibly in parallel and redundantly) throughout execution, so as to minimize the expected completion time. This fundamental parallel scheduling problem arises in grid computing and project management fields, and has several applications. We show a polynomial time algorithm for the problem restricted to the case when dag width is at most a constant and the number of workers is also at most a constant. These two restrictions may appear to be too severe. However, they are fundamentally required. Specifically, we demonstrate that the problem is NP-hard with constant number of workers when dag width can grow, and is also NP-hard with constant dag width when the number of workers can grow. When both dag width and the number of workers are unconstrained, then the problem is inapproximable within factor less than 5/4, unless P=NP.", ["NP-hard", "Directed acyclic graph", "Computing", "P versus NP problem", "Grid computing", "Graph (mathematics)", "Polynomial time", "Algorithm", "Project management", "Tree (graph theory)", "Polynomial"]], ["Ontology-Supported and Ontology-Driven Conceptual Navigation on the World Wide Web", "This paper presents the principles of ontology-supported and ontology-driven conceptual navigation. Conceptual navigation realizes the independence between resources and links to facilitate interoperability and reusability. An engine builds dynamic links, assembles resources under an argumentative scheme and allows optimization with a possible constraint, such as the user's available time. Among several strategies, two are discussed in detail with examples of applications. On the one hand, conceptual specifications for linking and assembling are embedded in the resource meta-description with the support of the ontology of the domain to facilitate meta-communication. Resources are like agents looking for conceptual acquaintances with intention. On the other hand, the domain ontology and an argumentative ontology drive the linking and assembling strategies.", ["Interoperability", "Mathematical optimization", "Ontology", "World Wide Web", "Navigation"]], ["A Technical Report On Grid Benchmarking using ATLAS V.O", "Grids include heterogeneous resources, which are based on different hardware and software architectures or components. In correspondence with this diversity of the infrastructure, the execution time of any single job, as well as the total grid performance can both be affected substantially, which can be demonstrated by measurements. Running a simple benchmarking suite can show this heterogeneity and give us results about the differences over the grid sites.", ["Hardware", "Infrastructure", "Homogeneity and heterogeneity", "Benchmarking", "Computer architecture"]], ["Optimal Watermark Embedding and Detection Strategies Under Limited Detection Resources", "An information-theoretic approach is proposed to watermark embedding and detection under limited detector resources. First, we consider the attack-free scenario under which asymptotically optimal decision regions in the Neyman-Pearson sense are proposed, along with the optimal embedding rule. Later, we explore the case of zero-mean i.i.d. Gaussian covertext distribution with unknown variance under the attack-free scenario. For this case, we propose a lower bound on the exponential decay rate of the false-negative probability and prove that the optimal embedding and detecting strategy is superior to the customary linear, additive embedding strategy in the exponential sense. Finally, these results are extended to the case of memoryless attacks and general worst case attacks. Optimal decision regions and embedding rules are offered, and the worst attack channel is identified.", ["Optimal decision", "Variance", "Normal distribution", "Mean", "Memorylessness", "Probability", "Information theory", "Probability distribution", "Jerzy Neyman", "Exponential decay", "Steganography", "Asymptotically optimal algorithm", "Independent and identically distributed random variables", "Negative probability", "Watermark", "Type I and type II errors", "Upper and lower bounds"]], ["Crystallization in large wireless networks", "We analyze fading interference relay networks where M single-antenna source-destination terminal pairs communicate concurrently and in the same frequency band through a set of K single-antenna relays using half-duplex two-hop relaying. Assuming that the relays have channel state information (CSI), it is shown that in the large-M limit, provided K grows fast enough as a function of M, the network \"decouples\" in the sense that the individual source-destination terminal pair capacities are strictly positive. The corresponding required rate of growth of K as a function of M is found to be sufficient to also make the individual source-destination fading links converge to nonfading links. We say that the network \"crystallizes\" as it breaks up into a set of effectively isolated \"wires in the air\". A large-deviations analysis is performed to characterize the \"crystallization\" rate, i.e., the rate (as a function of M,K) at which the decoupled links converge to nonfading links. In the course of this analysis, we develop a new technique for characterizing the large-deviations behavior of certain sums of dependent random variables. For the case of no CSI at the relay level, assuming amplify-and-forward relaying, we compute the per source-destination terminal pair capacity for M,K converging to infinity, with K/M staying fixed, using tools from large random matrix theory.", ["Channel state information", "Wireless network", "Antenna (radio)", "Frequency", "Random matrix", "Infinity", "Random variable", "Matrix (mathematics)", "Half-duplex"]], ["Double Sided Watermark Embedding and Detection with Perceptual Analysis", "In our previous work, we introduced a double-sided technique that utilizes but not reject the host interference. Due to its nice property of utilizing but not rejecting the host interference, it has a big advantage over the host interference schemes in that the perceptual analysis can be easily implemented for our scheme to achieve the locally bounded maximum embedding strength. Thus, in this work, we detail how to implement the perceptual analysis in our double-sided schemes since the perceptual analysis is very important for improving the fidelity of watermarked contents. Through the extensive performance comparisons, we can further validate the performance advantage of our double-sided schemes.", ["Watermark"]], ["Towards Informative Statistical Flow Inversion", "A problem which has recently attracted research attention is that of estimating the distribution of flow sizes in internet traffic. On high traffic links it is sometimes impossible to record every packet. Researchers have approached the problem of estimating flow lengths from sampled packet data in two separate ways. Firstly, different sampling methodologies can be tried to more accurately measure the desired system parameters. One such method is the sample-and-hold method where, if a packet is sampled, all subsequent packets in that flow are sampled. Secondly, statistical methods can be used to ``invert'' the sampled data and produce an estimate of flow lengths from a sample. In this paper we propose, implement and test two variants on the sample-and-hold method. In addition we show how the sample-and-hold method can be inverted to get an estimation of the genuine distribution of flow sizes. Experiments are carried out on real network traces to compare standard packet sampling with three variants of sample-and-hold. The methods are compared for their ability to reconstruct the genuine distribution of flow sizes in the traffic.", ["Statistics", "Internet", "Data", "Sampling (statistics)", "Network packet", "Sample and hold"]], ["A Branch and Cut Algorithm for the Halfspace Depth Problem", "The concept of data depth in non-parametric multivariate descriptive statistics is the generalization of the univariate rank method to multivariate data. Halfspace depth is a measure of data depth. Given a set S of points and a point p, the halfspace depth (or rank) k of p is defined as the minimum number of points of S contained in any closed halfspace with p on its boundary. Computing halfspace depth is NP-hard, and it is equivalent to the Maximum Feasible Subsystem problem. In this thesis a mixed integer program is formulated with the big-M method for the halfspace depth problem. We suggest a branch and cut algorithm. In this algorithm, Chinneck's heuristic algorithm is used to find an upper bound and a related technique based on sensitivity analysis is used for branching. Irreducible Infeasible Subsystem (IIS) hitting set cuts are applied. We also suggest a binary search algorithm which may be more stable numerically. The algorithms are implemented with the BCP framework from the COIN-OR project.", ["Metaheuristic", "Heuristic", "Binary search algorithm", "Descriptive statistics", "Statistics", "Non-parametric statistics", "Sensitivity analysis", "Search algorithm", "Half-space", "Algorithm", "Multivariate statistics", "NP-hard", "Univariate", "Integer"]], ["A Closed-Form Method for LRU Replacement under Generalized Power-Law Demand", "We consider the well known \\emph{Least Recently Used} (LRU) replacement algorithm and analyze it under the independent reference model and generalized power-law demand. For this extensive family of demand distributions we derive a closed-form expression for the per object steady-state hit ratio. To the best of our knowledge, this is the first analytic derivation of the per object hit ratio of LRU that can be obtained in constant time without requiring laborious numeric computations or simulation. Since most applications of replacement algorithms include (at least) some scenarios under i.i.d. requests, our method has substantial practical value, especially when having to analyze multiple caches, where existing numeric methods and simulation become too time consuming.", ["Closed-form expression", "Simulation", "Numerical analysis", "Cache algorithms", "Steady state", "Power law", "Algorithm", "Conceptual model"]], ["On the Hopcroft's minimization algorithm", "We show that the absolute worst case time complexity for Hopcroft's minimization algorithm applied to unary languages is reached only for de Bruijn words. A previous paper by Berstel and Carton gave the example of de Bruijn words as a language that requires O(n log n) steps by carefully choosing the splitting sets and processing these sets in a FIFO mode. We refine the previous result by showing that the Berstel/Carton example is actually the absolute worst case time complexity in the case of unary languages. We also show that a LIFO implementation will not achieve the same worst time complexity for the case of unary languages. Lastly, we show that the same result is valid also for the cover automata and a modification of the Hopcroft's algorithm, modification used in minimization of cover automata.", ["Automaton", "Best, worst and average case", "FIFO", "Algorithm", "Time complexity", "Unary operation"]], ["A first-order Temporal Logic for Actions", "We present a multi-modal action logic with first-order modalities, which contain terms which can be unified with the terms inside the subsequent formulas and which can be quantified. This makes it possible to handle simultaneously time and states. We discuss applications of this language to action theory where it is possible to express many temporal aspects of actions, as for example, beginning, end, time points, delayed preconditions and results, duration and many others. We present tableaux rules for a decidable fragment of this logic.", ["Quantification", "First-order logic", "Modal logic", "Decidability (logic)", "Temporal logic", "Logic"]], ["Bit-Interleaved Coded Multiple Beamforming with Imperfect CSIT", "This paper addresses the performance of bit-interleaved coded multiple beamforming (BICMB) [1], [2] with imperfect knowledge of beamforming vectors. Most studies for limited-rate channel state information at the transmitter (CSIT) assume that the precoding matrix has an invariance property under an arbitrary unitary transform. In BICMB, this property does not hold. On the other hand, the optimum precoder and detector for BICMB are invariant under a diagonal unitary transform. In order to design a limited-rate CSIT system for BICMB, we propose a new distortion measure optimum under this invariance. Based on this new distortion measure, we introduce a new set of centroids and employ the generalized Lloyd algorithm for codebook design. We provide simulation results demonstrating the performance improvement achieved with the proposed distortion measure and the codebook design for various receivers with linear detectors. We show that although these receivers have the same performance for perfect CSIT, their performance varies under imperfect CSIT.", ["Beamforming", "Unitary matrix", "Channel state information", "Matrix (mathematics)", "Centroid", "Algorithm", "Simulation", "Codebook", "Transmitter", "Bit", "Invariant (mathematics)"]], ["Multi-Dimensional Recurrent Neural Networks", "Recurrent neural networks (RNNs) have proved effective at one dimensional sequence learning tasks, such as speech and online handwriting recognition. Some of the properties that make RNNs suitable for such tasks, for example robustness to input warping, and the ability to access contextual information, are also desirable in multidimensional domains. However, there has so far been no direct way of applying RNNs to data with more than one spatio-temporal dimension. This paper introduces multi-dimensional recurrent neural networks (MDRNNs), thereby extending the potential applicability of RNNs to vision, video processing, medical imaging and many other areas, while avoiding the scaling problems that have plagued other multi-dimensional models. Experimental results are provided for two image segmentation tasks.", ["Recurrent neural network", "Handwriting recognition", "Medical imaging", "Neural network", "Segmentation (image processing)", "Dimension", "Nervous system", "Handwriting"]], ["Mean Field Models of Message Throughput in Dynamic Peer-to-Peer Systems", "The churn rate of a peer-to-peer system places direct limitations on the rate at which messages can be effectively communicated to a group of peers. These limitations are independent of the topology and message transmission latency. In this paper we consider a peer-to-peer network, based on the Engset model, where peers arrive and depart independently at random. We show how the arrival and departure rates directly limit the capacity for message streams to be broadcast to all other peers, by deriving mean field models that accurately describe the system behavior. Our models cover the unit and more general k buffer cases, i.e. where a peer can buffer at most k messages at any one time, and we give results for both single and multi-source message streams. We define coverage rate as peer-messages per unit time, i.e. the rate at which a number of peers receive messages, and show that the coverage rate is limited by the churn rate and buffer size. Our theory introduces an Instantaneous Message Exchange (IME) model and provides a template for further analysis of more complicated systems. Using the IME model, and assuming random processes, we have obtained very accurate equations of the system dynamics in a variety of interesting cases, that allow us to tune a peer-to-peer system. It remains to be seen if we can maintain this accuracy for general processes and when applying a non-instantaneous model.", ["Churn rate", "Peer-to-peer", "Topology", "System dynamics", "Throughput"]], ["CDMA Technology for Intelligent Transportation Systems", "Scientists and Technologists involved in the development of radar and remote sensing systems all over the world are now trying to involve themselves in saving of manpower in the form of developing a new application of their ideas in Intelligent Transport system(ITS). The world statistics shows that by incorporating such wireless radar system in the car would decrease the world road accident by 8-10% yearly. The wireless technology has to be chosen properly which is capable of tackling the severe interferences present in the open road. A combined digital technology like Spread spectrum along with diversity reception will help a lot in this regard. Accordingly, the choice is for FHSS based space diversity system which will utilize carrier frequency around 5.8 GHz ISM band with available bandwidth of 80 MHz and no license. For efficient design, the radio channel is characterized on which the design is based. Out of two available modes e.g. Communication and Radar modes, the radar mode is providing the conditional measurement of the range of the nearest car after authentication of the received code, thus ensuring the reliability and accuracy of measurement. To make the system operational in simultaneous mode, we have started the Software Defined Radio approach for best speed and flexibility.", ["Spread spectrum", "Antenna diversity", "ISM band", "Software-defined radio", "Code division multiple access", "Frequency", "Frequency-hopping spread spectrum", "Wireless", "Statistics", "Hertz", "Remote sensing", "Diversity scheme", "Radar", "Intelligent transportation system", "Authentication", "Modulation", "Channel (communications)"]], ["RADAR Imaging in the Open field At 300 MHz-3000 MHz Radio Band", "With the technological growth of broadband wireless technology like CDMA and UWB, a lots of development efforts towards wireless communication system and Imaging radar system are well justified. Efforts are also being imparted towards a Convergence Technology.. the convergence between a communication and radar technology which will result in ITS (Intelligent Transport System) and other applications. This encourages present authors for this development. They are trying to utilize or converge the communication technologies towards radar and to achieve the Interference free and clutter free quality remote images of targets using DS-UWB wireless technology.", ["Wireless", "Radar imaging", "Radar", "Ultra-wideband", "History of technology", "Communication", "Broadband", "Code division multiple access", "Wireless broadband", "Intelligent transportation system", "Communications system"]], ["Scientific citations in Wikipedia", "The Internet-based encyclopaedia Wikipedia has grown to become one of the most visited web-sites on the Internet. However, critics have questioned the quality of entries, and an empirical study has shown Wikipedia to contain errors in a 2005 sample of science entries. Biased coverage and lack of sources are among the \"Wikipedia risks\". The present work describes a simple assessment of these aspects by examining the outbound links from Wikipedia articles to articles in scientific journals with a comparison against journal statistics from Journal Citation Reports such as impact factors. The results show an increasing use of structured citation markup and good agreement with the citation pattern seen in the scientific literature though with a slight tendency to cite articles in high-impact journals such as Nature and Science. These results increase confidence in Wikipedia as an good information organizer for science in general.", ["Journal Citation Reports", "Encyclopedia", "Wikipedia", "Scientific literature", "Scientific journal", "Internet", "Science", "Empirical", "Literature", "Citation", "Impact factor", "Academic journal"]], ["Parallelized approximation algorithms for minimum routing cost spanning trees", "We parallelize several previously proposed algorithms for the minimum routing cost spanning tree problem and some related problems.", ["Routing", "Algorithm", "Parallel algorithm"]], ["Improvements to the Psi-SSA representation", "Modern compiler implementations use the Static Single Assignment representation as a way to efficiently implement optimizing algorithms. However this representation is not well adapted to architectures with a predicated instruction set. The Psi-SSA representation extends the SSA representation such that standard SSA algorithms can be easily adapted to an architecture with a fully predicated instruction set. A new pseudo operation, the Psi operation, is introduced to merge several conditional definitions into a unique definition.", ["Algorithm", "Computer architecture", "Instruction set", "Compiler", "Static single assignment form", "Architecture"]], ["Best insertion algorithm for resource-constrained project scheduling problem", "This paper considers heuristics for well known resource-constrained project scheduling problem (RCPSP). First a feasible schedule is constructed using randomized best insertion algorithm. The construction is followed by a local search where a new solution is generated as follows: first we randomly delete m activities from the list, which are then reinserted in the list in consecutive order. At the end of run, the schedule with the minimum makespan is selected. Experimental work shows very good results on standard test instances found in PSPLIB", ["Job shop scheduling", "Algorithm", "Heuristic"]], ["Elementary transformation analysis for Array-OL", "Array-OL is a high-level specification language dedicated to the definition of intensive signal processing applications. Several tools exist for implementing an Array-OL specification as a data parallel program. While Array-OL can be used directly, it is often convenient to be able to deduce part of the specification from a sequential version of the application. This paper proposes such an analysis and examines its feasibility and its limits.", ["Digital signal processing", "Signal processing", "Parallel computing", "Data parallelism", "Specification language"]], ["On the freezing of variables in random constraint satisfaction problems", "The set of solutions of random constraint satisfaction problems (zero energy groundstates of mean-field diluted spin glasses) undergoes several structural phase transitions as the amount of constraints is increased. This set first breaks down into a large number of well separated clusters. At the freezing transition, which is in general distinct from the clustering one, some variables (spins) take the same value in all solutions of a given cluster. In this paper we study the critical behavior around the freezing transition, which appears in the unfrozen phase as the divergence of the sizes of the rearrangements induced in response to the modification of a variable. The formalism is developed on generic constraint satisfaction problems and applied in particular to the random satisfiability of boolean formulas and to the coloring of random graphs. The computation is first performed in random tree ensembles, for which we underline a connection with percolation models and with the reconstruction problem of information theory. The validity of these results for the original random ensembles is then discussed in the framework of the cavity method.", ["Percolation", "Random graph", "Information theory", "Phase transition", "Percolation theory", "Constraint satisfaction", "Spin glass", "Energy", "Variable (mathematics)", "Cluster analysis"]], ["Sequential mechanism design", "In the customary VCG (Vickrey-Clarke-Groves) mechanism truth-telling is a dominant strategy. In this paper we study the sequential VCG mechanism and show that other dominant strategies may then exist. We illustrate how this fact can be used to minimize taxes using examples concerned with Clarke tax and public projects.", ["Tax", "Mechanism design"]], ["From Nondeterministic B\\\"uchi and Streett Automata to Deterministic Parity Automata", "In this paper we revisit Safra's determinization constructions for automata on infinite words. We show how to construct deterministic automata with fewer states and, most importantly, parity acceptance conditions. Determinization is used in numerous applications, such as reasoning about tree automata, satisfiability of CTL*, and realizability and synthesis of logical specifications. The upper bounds for all these applications are reduced by using the smaller deterministic automata produced by our construction. In addition, the parity acceptance conditions allows to use more efficient algorithms (when compared to handling Rabin or Streett acceptance conditions).", ["Powerset construction", "Automaton", "Determinism", "Tree automaton", "Deterministic finite-state machine", "Realizability", "Algorithm", "Computational complexity theory"]], ["On tractability and congruence distributivity", "Constraint languages that arise from finite algebras have recently been the object of study, especially in connection with the Dichotomy Conjecture of Feder and Vardi. An important class of algebras are those that generate congruence distributive varieties and included among this class are lattices, and more generally, those algebras that have near-unanimity term operations. An algebra will generate a congruence distributive variety if and only if it has a sequence of ternary term operations, called Jonsson terms, that satisfy certain equations. We prove that constraint languages consisting of relations that are invariant under a short sequence of Jonsson terms are tractable by showing that such languages have bounded relational width.", ["Distributive property", "Algebra", "Finite set", "Variety (universal algebra)", "If and only if", "Lattice (order)", "Algebra over a field", "Congruence relation", "Arity", "Category (mathematics)", "Class (set theory)"]], ["Response Prediction of Structural System Subject to Earthquake Motions using Artificial Neural Network", "This paper uses Artificial Neural Network (ANN) models to compute response of structural system subject to Indian earthquakes at Chamoli and Uttarkashi ground motion data. The system is first trained for a single real earthquake data. The trained ANN architecture is then used to simulate earthquakes with various intensities and it was found that the predicted responses given by ANN model are accurate for practical purposes. When the ANN is trained by a part of the ground motion data, it can also identify the responses of the structural system well. In this way the safeness of the structural systems may be predicted in case of future earthquakes without waiting for the earthquake to occur for the lessons. Time period and the corresponding maximum response of the building for an earthquake has been evaluated, which is again trained to predict the maximum response of the building at different time periods. The trained time period versus maximum response ANN model is also tested for real earthquake data of other place, which was not used in the training and was found to be in good agreement.", ["Artificial neural network", "Architecture", "Earthquake", "Structural system", "Uttarkashi"]], ["Fault Classification using Pseudomodal Energies and Neuro-fuzzy modelling", "This paper presents a fault classification method which makes use of a Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the vibration signals of cylindrical shells. The calculation of Pseudomodal Energies, for the purposes of condition monitoring, has previously been found to be an accurate method of extracting features from vibration signals. This calculation is therefore used to extract features from vibration signals obtained from a diverse population of cylindrical shells. Some of the cylinders in the population have faults in different substructures. The pseudomodal energies calculated from the vibration signals are then used as inputs to a neuro-fuzzy model. A leave-one-out cross-validation process is used to test the performance of the model. It is found that the neuro-fuzzy model is able to classify faults with an accuracy of 91.62%, which is higher than the previously used multilayer perceptron.", ["Multilayer perceptron", "Cross-validation (statistics)", "Perceptron", "Resampling (statistics)"]], ["Multi-Access MIMO Systems with Finite Rate Channel State Feedback", "This paper characterizes the effect of finite rate channel state feedback on the sum rate of a multi-access multiple-input multiple-output (MIMO) system. We propose to control the users jointly, specifically, we first choose the users jointly and then select the corresponding beamforming vectors jointly. To quantify the sum rate, this paper introduces the composite Grassmann manifold and the composite Grassmann matrix. By characterizing the distortion rate function on the composite Grassmann manifold and calculating the logdet function of a random composite Grassmann matrix, a good sum rate approximation is derived. According to the distortion rate function on the composite Grassmann manifold, the loss due to finite beamforming decreases exponentially as the feedback bits on beamforming increases.", ["Function (mathematics)", "MIMO", "Beamforming", "Matrix (mathematics)", "Manifold", "Feedback", "Exponential decay", "Rate function", "Hermann Grassmann", "Function composition", "Finite set", "Distortion"]], ["Quantization Bounds on Grassmann Manifolds of Arbitrary Dimensions and MIMO Communications with Feedback", "This paper considers the quantization problem on the Grassmann manifold with dimension n and p. The unique contribution is the derivation of a closed-form formula for the volume of a metric ball in the Grassmann manifold when the radius is sufficiently small. This volume formula holds for Grassmann manifolds with arbitrary dimension n and p, while previous results are only valid for either p=1 or a fixed p with asymptotically large n. Based on the volume formula, the Gilbert-Varshamov and Hamming bounds for sphere packings are obtained. Assuming a uniformly distributed source and a distortion metric based on the squared chordal distance, tight lower and upper bounds are established for the distortion rate tradeoff. Simulation results match the derived results. As an application of the derived quantization bounds, the information rate of a Multiple-Input Multiple-Output (MIMO) system with finite-rate channel-state feedback is accurately quantified for arbitrary finite number of antennas, while previous results are only valid for either Multiple-Input Single-Output (MISO) systems or those with asymptotically large number of transmit antennas but fixed number of receive antennas.", ["Ball (mathematics)", "Manifold", "Finite set", "Radius", "MIMO", "Dimension", "Simulation", "Sphere", "Closed-form expression", "Hermann Grassmann", "Antenna (radio)", "Feedback", "Formula", "Uniform distribution (continuous)", "Quantization (signal processing)"]], ["On the Information Rate of MIMO Systems with Finite Rate Channel State Feedback and Power On/Off Strategy", "This paper quantifies the information rate of multiple-input multiple-output (MIMO) systems with finite rate channel state feedback and power on/off strategy. In power on/off strategy, a beamforming vector (beam) is either turned on (denoted by on-beam) with a constant power or turned off. We prove that the ratio of the optimal number of on-beams and the number of antennas converges to a constant for a given signal-to-noise ratio (SNR) when the number of transmit and receive antennas approaches infinity simultaneously and when beamforming is perfect. Based on this result, a near optimal strategy, i.e., power on/off strategy with a constant number of on-beams, is discussed. For such a strategy, we propose the power efficiency factor to quantify the effect of imperfect beamforming. A formula is proposed to compute the maximum power efficiency factor achievable given a feedback rate. The information rate of the overall MIMO system can be approximated by combining the asymptotic results and the formula for power efficiency factor. Simulations show that this approximation is accurate for all SNR regimes.", ["Signal-to-noise ratio", "MIMO", "Beamforming", "Information theory", "Limit of a sequence", "Infinity", "Ratio", "Formula"]], ["How Many Users should be Turned On in a Multi-Antenna Broadcast Channel?", "This paper considers broadcast channels with L antennas at the base station and m single-antenna users, where each user has perfect channel knowledge and the base station obtains channel information through a finite rate feedback. The key observation of this paper is that the optimal number of on-users (users turned on), say s, is a function of signal-to-noise ratio (SNR) and other system parameters. Towards this observation, we use asymptotic analysis to guide the design of feedback and transmission strategies. As L, m and the feedback rates approach infinity linearly, we derive the asymptotic optimal feedback strategy and a realistic criterion to decide which users should be turned on. Define the corresponding asymptotic throughput per antenna as the spatial efficiency. It is a function of the number of on-users s, and therefore, s should be appropriately chosen. Based on the above asymptotic results, we also develop a scheme for a system with finite many antennas and users. Compared with other works where s is presumed constant, our scheme achieves a significant gain by choosing the appropriate s. Furthermore, our analysis and scheme is valid for heterogeneous systems where different users may have different path loss coefficients and feedback rates.", ["Signal-to-noise ratio", "Homogeneity and heterogeneity", "Path loss", "Asymptotic analysis", "Infinity", "Channel state information", "Base station", "Throughput", "Feedback"]], ["Unequal dimensional small balls and quantization on Grassmann Manifolds", "The Grassmann manifold G_{n,p}(L) is the set of all p-dimensional planes (through the origin) in the n-dimensional Euclidean space L^{n}, where L is either R or C. This paper considers an unequal dimensional quantization in which a source in G_{n,p}(L) is quantized through a code in G_{n,q}(L), where p and q are not necessarily the same. It is different from most works in literature where p\\equiv q. The analysis for unequal dimensional quantization is based on the volume of a metric ball in G_{n,p}(L) whose center is in G_{n,q}(L). Our chief result is a closed-form formula for the volume of a metric ball when the radius is sufficiently small. This volume formula holds for Grassmann manifolds with arbitrary n, p, q and L, while previous results pertained only to some special cases. Based on this volume formula, several bounds are derived for the rate distortion tradeoff assuming the quantization rate is sufficiently high. The lower and upper bounds on the distortion rate function are asymptotically identical, and so precisely quantify the asymptotic rate distortion tradeoff. We also show that random codes are asymptotically optimal in the sense that they achieve the minimum achievable distortion with probability one as n and the code rate approach infinity linearly. Finally, we discuss some applications of the derived results to communication theory. A geometric interpretation in the Grassmann manifold is developed for capacity calculation of additive white Gaussian noise channel. Further, the derived distortion rate function is beneficial to characterizing the effect of beamforming matrix selection in multi-antenna communications.", ["Euclidean space", "Ball (mathematics)", "Function (mathematics)", "Geometry", "Matrix (mathematics)", "Additive white Gaussian noise", "Normal distribution", "Probability", "Infinity", "Radius", "Manifold", "Beamforming", "Closed-form expression", "Antenna (radio)", "Asymptotically optimal algorithm", "Hermann Grassmann", "Code rate", "Rate function", "Volume", "Literature", "Communication theory", "Space", "Quantization (physics)", "Randomness", "Upper and lower bounds", "Formula", "Communication", "N-dimensional space"]], ["Fuzzy and Multilayer Perceptron for Evaluation of HV Bushings", "The work proposes the application of fuzzy set theory (FST) to diagnose the condition of high voltage bushings. The diagnosis uses dissolved gas analysis (DGA) data from bushings based on IEC60599 and IEEE C57-104 criteria for oil impregnated paper (OIP) bushings. FST and neural networks are compared in terms of accuracy and computational efficiency. Both FST and NN simulations were able to diagnose the bushings condition with 10% error. By using fuzzy theory, the maintenance department can classify bushings and know the extent of degradation in the component.", ["Perceptron", "Neural network", "International Electrotechnical Commission", "Departments of France", "Fuzzy set", "Voltage", "High voltage", "Gas", "Set theory"]], ["A Study in a Hybrid Centralised-Swarm Agent Community", "This paper describes a systems architecture for a hybrid Centralised/Swarm based multi-agent system. The issue of local goal assignment for agents is investigated through the use of a global agent which teaches the agents responses to given situations. We implement a test problem in the form of a Pursuit game, where the Multi-Agent system is a set of captor agents. The agents learn solutions to certain board positions from the global agent if they are unable to find a solution. The captor agents learn through the use of multi-layer perceptron neural networks. The global agent is able to solve board positions through the use of a Genetic Algorithm. The cooperation between agents and the results of the simulation are discussed here. .", ["Multi-agent system", "Neural network", "Genetic algorithm", "Perceptron", "Architecture", "Multilayer perceptron"]], ["On-Line Condition Monitoring using Computational Intelligence", "This paper presents bushing condition monitoring frameworks that use multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector machines (SVM) classifiers. The first level of the framework determines if the bushing is faulty or not while the second level determines the type of fault. The diagnostic gases in the bushings are analyzed using the dissolve gas analysis. MLP gives superior performance in terms of accuracy and training time than SVM and RBF. In addition, an on-line bushing condition monitoring approach, which is able to adapt to newly acquired data are introduced. This approach is able to accommodate new classes that are introduced by incoming data and is implemented using an incremental learning algorithm that uses MLP. The testing results improved from 67.5% to 95.8% as new data were introduced and the testing results improved from 60% to 95.3% as new conditions were introduced. On average the confidence value of the framework on its decision was 0.92.", ["Computational intelligence", "Condition monitoring", "Support vector machine", "Algorithm", "Gas"]], ["TrustMIX: Trustworthy MIX for Energy Saving in Sensor Networks", "MIX has recently been proposed as a new sensor scheme with better energy management for data-gathering in Wireless Sensor Networks. However, it is not known how it performs when some of the sensors carry out sinkhole attacks. In this paper, we propose a variant of MIX with adjunct computational trust management to limit the impact of such sinkhole attacks. We evaluate how MIX resists sinkhole attacks with and without computational trust management. The main result of this paper is to find that MIX is very vulnerable to sinkhole attacks but that the adjunct trust management efficiently reduces the impact of such attacks while preserving the main feature of MIX: increased lifetime of the network.", ["Energy", "Wireless sensor network", "Sinkhole"]], ["Statistical Mechanics of Nonlinear On-line Learning for Ensemble Teachers", "We analyze the generalization performance of a student in a model composed of nonlinear perceptrons: a true teacher, ensemble teachers, and the student. We calculate the generalization error of the student analytically or numerically using statistical mechanics in the framework of on-line learning. We treat two well-known learning rules: Hebbian learning and perceptron learning. As a result, it is proven that the nonlinear model shows qualitatively different behaviors from the linear model. Moreover, it is clarified that Hebbian learning and perceptron learning show qualitatively different behaviors from each other. In Hebbian learning, we can analytically obtain the solutions. In this case, the generalization error monotonically decreases. The steady value of the generalization error is independent of the learning rate. The larger the number of teachers is and the more variety the ensemble teachers have, the smaller the generalization error is. In perceptron learning, we have to numerically obtain the solutions. In this case, the dynamical behaviors of the generalization error are non-monotonic. The smaller the learning rate is, the larger the number of teachers is; and the more variety the ensemble teachers have, the smaller the minimum value of the generalization error is.", ["Perceptron", "Hebbian theory", "Statistical mechanics", "Mechanics", "Monotonic function", "Dynamical system", "Nonlinear system", "Statistics", "Linear model"]], ["The Use of ITIL for Process Optimisation in the IT Service Centre of Harz University, exemplified in the Release Management Process", "This paper details the use of the IT Infrastructure Library Framework (ITIL) for optimising process workflows in the IT Service Centre of Harz University in Wernigerode, Germany, exemplified by the Release Management Process. It is described, how, during the course of a special ITIL project, the As-Is-Status of the various original processes was documented as part of the process life cycle and then transformed in the To-Be-Status, according to the ITIL Best Practice Framework. It is also shown, how the ITIL framework fits into the four-layered-process model, that could be derived from interviews with the universities IT support staff, and how the various modified processes interconnect with each other to form a value chain. The paper highlights the final results of the project and gives an outlook on the future use of ITIL as a business modelling tool in the IT Service Centre of Harz University. It is currently being considered, whether the process model developed during the project could be used as a reference model for other university IT centres.", ["Information Technology Infrastructure Library", "Release management", "Value chain", "Germany", "University", "Management", "Process modeling", "Reference model"]], ["Reduced Complexity Sphere Decoding for Square QAM via a New Lattice Representation", "Sphere decoding (SD) is a low complexity maximum likelihood (ML) detection algorithm, which has been adapted for different linear channels in digital communications. The complexity of the SD has been shown to be exponential in some cases, and polynomial in others and under certain assumptions. The sphere radius and the number of nodes visited throughout the tree traversal search are the decisive factors for the complexity of the algorithm. The radius problem has been addressed and treated widely in the literature. In this paper, we propose a new structure for SD, which drastically reduces the overall complexity. The complexity is measured in terms of the floating point operations per second (FLOPS) and the number of nodes visited throughout the algorithm tree search. This reduction in the complexity is due to the ability of decoding the real and imaginary parts of each jointly detected symbol independently of each other, making use of the new lattice representation. We further show by simulations that the new approach achieves 80% reduction in the overall complexity compared to the conventional SD for a 2x2 system, and almost 50% reduction for the 4x4 and 6x6 cases, thus relaxing the requirements for hardware implementation.", ["Quadrature amplitude modulation", "Tree traversal", "Floating point", "Polynomial", "Data transmission", "Hardware", "Algorithm", "Maximum likelihood", "FLOPS", "Radius", "Complexity"]], ["Using Genetic Algorithms to Optimise Rough Set Partition Sizes for HIV Data Analysis", "In this paper, we present a method to optimise rough set partition sizes, to which rule extraction is performed on HIV data. The genetic algorithm optimisation technique is used to determine the partition sizes of a rough set in order to maximise the rough sets prediction accuracy. The proposed method is tested on a set of demographic properties of individuals obtained from the South African antenatal survey. Six demographic variables were used in the analysis, these variables are; race, age of mother, education, gravidity, parity, and age of father, with the outcome or decision being either HIV positive or negative. Rough set theory is chosen based on the fact that it is easy to interpret the extracted rules. The prediction accuracy of equal width bin partitioning is 57.7% while the accuracy achieved after optimising the partitions is 72.8%. Several other methods have been used to analyse the HIV data and their results are stated and compared to that of rough set theory (RST).", ["Genetic algorithm", "HIV", "Genetics", "Set theory", "Algorithm", "Demographics", "Glossary of terms associated with gravidity", "Data analysis"]], ["Improved Approximability Result for Test Set with Small Redundancy", "Test set with redundancy is one of the focuses in recent bioinformatics research. Set cover greedy algorithm (SGA for short) is a commonly used algorithm for test set with redundancy. This paper proves that the approximation ratio of SGA can be $(2-\\frac{1}{2r})\\ln n+{3/2}\\ln r+O(\\ln\\ln n)$ by using the potential function technique. This result is better than the approximation ratio $2\\ln n$ which directly derives from set multicover, when $r=o(\\frac{\\ln n}{\\ln\\ln n})$, and is an extension of the approximability results for plain test set.", ["Bioinformatics", "Approximation algorithm", "Greedy algorithm"]], ["Condition Monitoring of HV Bushings in the Presence of Missing Data Using Evolutionary Computing", "The work proposes the application of neural networks with particle swarm optimisation (PSO) and genetic algorithms (GA) to compensate for missing data in classifying high voltage bushings. The classification is done using DGA data from 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates methods for oil impregnated paper (OIP) bushings. PSO and GA were compared in terms of accuracy and computational efficiency. Both GA and PSO simulations were able to estimate missing data values to an average 95% accuracy when only one variable was missing. However PSO rapidly deteriorated to 66% accuracy with two variables missing simultaneously, compared to 84% for GA. The data estimated using GA was found to classify the conditions of bushings than the PSO.", ["International Electrotechnical Commission", "Neural network", "Particle swarm optimization", "Genetic algorithm", "Voltage", "High voltage", "Missing data", "Genetics", "Institute of Electrical and Electronics Engineers", "Algorithm"]], ["Informatics Carnot Machine", "Based on Planck's blackbody equation it is argued that a single mode light pulse, with a large number of photons, carries one entropy unit. Similarly, an empty radiation mode carries no entropy. In this case, the calculated entropy that a coded sequence of light pulses is carrying is simply the Gibbs mixing entropy, which is identical to the logical Shannon information. This approach is supported by a demonstration that information transmission and amplification, by a sequence of light pulses in an optical fiber, is a classic Carnot machine comprising of two isothermals and two adiabatic. Therefore it is concluded that entropy under certain conditions is information.", ["Planck's law", "Optics", "Optical fiber", "Photon", "Light", "Radiation", "Amplifier", "Entropy of mixing", "Entropy", "Entropy (information theory)", "Black body"]], ["Computational Intelligence for Condition Monitoring", "Condition monitoring techniques are described in this chapter. Two aspects of condition monitoring process are considered: (1) feature extraction; and (2) condition classification. Feature extraction methods described and implemented are fractals, Kurtosis and Mel-frequency Cepstral Coefficients. Classification methods described and implemented are support vector machines (SVM), hidden Markov models (HMM), Gaussian mixture models (GMM) and extension neural networks (ENN). The effectiveness of these features were tested using SVM, HMM, GMM and ENN on condition monitoring of bearings and are found to give good results.", ["Feature extraction", "Generalized method of moments", "Normal distribution", "Cepstrum", "Kurtosis", "Markov process", "Markov chain", "Condition monitoring", "Mixture model", "Neural network", "Fractal", "Hidden Markov model", "Support vector machine", "Bioinformatics", "Computational intelligence", "Frequency", "Statistical classification"]], ["Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in hypre and PETSc", "We describe our software package Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) publicly released recently. BLOPEX is available as a stand-alone serial library, as an external package to PETSc (``Portable, Extensible Toolkit for Scientific Computation'', a general purpose suite of tools for the scalable solution of partial differential equations and related problems developed by Argonne National Laboratory), and is also built into {\\it hypre} (``High Performance Preconditioners'', scalable linear solvers package developed by Lawrence Livermore National Laboratory). The present BLOPEX release includes only one solver--the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. {\\it hypre} provides users with advanced high-quality parallel preconditioners for linear systems, in particular, with domain decomposition and multigrid preconditioners. With BLOPEX, the same preconditioners can now be efficiently used for symmetric eigenvalue problems. PETSc facilitates the integration of independently developed application modules with strict attention to component interoperability, and makes BLOPEX extremely easy to compile and use with preconditioners that are available via PETSc. We present the LOBPCG algorithm in BLOPEX for {\\it hypre} and PETSc. We demonstrate numerically the scalability of BLOPEX by testing it on a number of distributed and shared memory parallel systems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron workstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition and {\\it hypre} multigrid preconditioning. We test BLOPEX on a model problem, the standard 7-point finite-difference approximation of the 3-D Laplacian, with the problem size in the range $10^5-10^8$.", ["Lawrence Livermore National Laboratory", "Argonne National Laboratory", "Partial differential equation", "Scalability", "Conjugate gradient method", "Preconditioner", "Shared memory", "Workstation", "Eigenvalues and eigenvectors", "Multigrid method", "IBM", "Computer software", "Differential equation", "Advanced Micro Devices", "Supercomputer", "Finite difference method", "Gradient", "Analysis of algorithms", "Opteron", "Blue Gene", "Laplace operator", "Beowulf cluster", "Parallel computing", "Computational science", "Multi-core processor"]], ["On the monotonization of the training set", "We consider the problem of minimal correction of the training set to make it consistent with monotonic constraints. This problem arises during analysis of data sets via techniques that require monotone data. We show that this problem is NP-hard in general and is equivalent to finding a maximal independent set in special orgraphs. Practically important cases of that problem considered in detail. These are the cases when a partial order given on the replies set is a total order or has a dimension 2. We show that the second case can be reduced to maximization of a quadratic convex function on a convex set. For this case we construct an approximate polynomial algorithm based on convex optimization.", ["NP-hard", "Convex function", "Partially ordered set", "Convex optimization", "Convex set", "Function (mathematics)", "Mathematical optimization", "Polynomial", "Total order", "Algorithm", "Maximal independent set", "Dimension", "Time complexity", "Independent set (graph theory)", "Monotonic function"]], ["Virtualization: A double-edged sword", "Virtualization became recently a hot topic once again, after being dormant for more than twenty years. In the meantime, it has been almost forgotten, that virtual machines are not so perfect isolating environments as it seems, when looking at the principles. These lessons were already learnt earlier when the first virtualized systems have been exposed to real life usage. Contemporary virtualization software enables instant creation and destruction of virtual machines on a host, live migration from one host to another, execution history manipulation, etc. These features are very useful in practice, but also causing headaches among security specialists, especially in current hostile network environments. In the present contribution we discuss the principles, potential benefits and risks of virtualization in a deja vu perspective, related to previous experiences with virtualization in the mainframe era.", ["Virtual machine", "Hardware virtualization", "Mainframe computer"]], ["Worst-Case Background Knowledge for Privacy-Preserving Data Publishing", "Recent work has shown the necessity of considering an attacker's background knowledge when reasoning about privacy in data publishing. However, in practice, the data publisher does not know what background knowledge the attacker possesses. Thus, it is important to consider the worst-case. In this paper, we initiate a formal study of worst-case background knowledge. We propose a language that can express any background knowledge about the data. We provide a polynomial time algorithm to measure the amount of disclosure of sensitive information in the worst case, given that the attacker has at most a specified number of pieces of information in this language. We also provide a method to efficiently sanitize the data so that the amount of disclosure in the worst case is less than a specified threshold.", ["Privacy", "Polynomial time", "Polynomial", "Algorithm", "Publishing", "Best, worst and average case"]], ["The poset metrics that allow binary codes of codimension m to be m-, (m-1)-, or (m-2)-perfect", "A binary poset code of codimension M (of cardinality 2^{N-M}, where N is the code length) can correct maximum M errors. All possible poset metrics that allow codes of codimension M to be M-, (M-1)- or (M-2)-perfect are described. Some general conditions on a poset which guarantee the nonexistence of perfect poset codes are derived; as examples, we prove the nonexistence of R-perfect poset codes for some R in the case of the crown poset and in the case of the union of disjoin chains. Index terms: perfect codes, poset codes", ["Partially ordered set", "Codimension", "Cardinality"]], ["An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF", "Admission control as a mechanism for providing QoS requires an accurate description of the requested flow as well as already admitted flows. Since 802.11 WLAN capacity is shared between flows belonging to all stations, admission control requires knowledge of all flows in the WLAN. Further, estimation of the load-dependent WLAN capacity through analytical model requires inputs about channel data rate, payload size and the number of stations. These factors combined point to a centralized admission control whereas for 802.11 DCF it is ideally performed in a distributed manner. The use of measurements from the channel avoids explicit inputs about the state of the channel described above. BUFFET, a model based measurement-assisted distributed admission control scheme for DCF proposed in this paper relies on measurements to derive model inputs and predict WLAN saturation, thereby maintaining average delay within acceptable limits. Being measurement based, it adapts to a combination of data rates and payload sizes, making it completely autonomous and distributed. Performance analysis using OPNET simulations suggests that BUFFET is able to ensure average delay under 7ms at a near-optimal throughput.", ["IEEE 802.11", "Mathematical model", "Millisecond", "Profiling (computer programming)", "Data", "Institute of Electrical and Electronics Engineers"]], ["Voronoi Diagram of Polygonal Chains under the Discrete Fr\\'echet Distance", "Polygonal chains are fundamental objects in many applications like pattern recognition and protein structure alignment. A well-known measure to characterize the similarity of two polygonal chains is the famous Fr\\`{e}chet distance. In this paper, for the first time, we consider the Voronoi diagram of polygonal chains in $d$-dimension ($d=2,3$) under the discrete Fr\\`{e}chet distance. Given $n$ polygonal chains ${\\cal C}$ in $d$-dimension ($d=2,3$), each with at most $k$ vertices, we prove fundamental properties of such a Voronoi diagram {\\em VD}$_F({\\cal C})$ by presenting the first known upper and lower bounds for {\\em VD}$_F({\\cal C})$.", ["Voronoi diagram", "Structural alignment", "Protein", "Pattern recognition", "Upper and lower bounds", "Dimension"]], ["Capacity of Sparse Multipath Channels in the Ultra-Wideband Regime", "This paper studies the ergodic capacity of time- and frequency-selective multipath fading channels in the ultrawideband (UWB) regime when training signals are used for channel estimation at the receiver. Motivated by recent measurement results on UWB channels, we propose a model for sparse multipath channels. A key implication of sparsity is that the independent degrees of freedom (DoF) in the channel scale sub-linearly with the signal space dimension (product of signaling duration and bandwidth). Sparsity is captured by the number of resolvable paths in delay and Doppler. Our analysis is based on a training and communication scheme that employs signaling over orthogonal short-time Fourier (STF) basis functions. STF signaling naturally relates sparsity in delay-Doppler to coherence in time-frequency. We study the impact of multipath sparsity on two fundamental metrics of spectral efficiency in the wideband/low-SNR limit introduced by Verdu: first- and second-order optimality conditions. Recent results by Zheng et. al. have underscored the large gap in spectral efficiency between coherent and non-coherent extremes and the importance of channel learning in bridging the gap. Building on these results, our results lead to the following implications of multipath sparsity: 1) The coherence requirements are shared in both time and frequency, thereby significantly relaxing the required scaling in coherence time with SNR; 2) Sparse multipath channels are asymptotically coherent -- for a given but large bandwidth, the channel can be learned perfectly and the coherence requirements for first- and second-order optimality met through sufficiently large signaling duration; and 3) The requirement of peaky signals in attaining capacity is eliminated or relaxed in sparse environments.", ["Coherence time", "Ultra-wideband", "Receiver (radio)", "Doppler effect", "Channel state information", "Multipath propagation", "Frequency", "Fourier transform", "Orthogonality", "Signal (electronics)", "Bandwidth (signal processing)", "Coherence (physics)", "Degrees of freedom (physics and chemistry)", "Communication", "Spectral efficiency", "Dimension", "Wideband", "Fading", "Channel (communications)", "Signal-to-noise ratio"]], ["Non-Coherent Capacity and Reliability of Sparse Multipath Channels in the Wideband Regime", "In contrast to the prevalent assumption of rich multipath in information theoretic analysis of wireless channels, physical channels exhibit sparse multipath, especially at large bandwidths. We propose a model for sparse multipath fading channels and present results on the impact of sparsity on non-coherent capacity and reliability in the wideband regime. A key implication of sparsity is that the statistically independent degrees of freedom in the channel, that represent the delay-Doppler diversity afforded by multipath, scale at a sub-linear rate with the signal space dimension (time-bandwidth product). Our analysis is based on a training-based communication scheme that uses short-time Fourier (STF) signaling waveforms. Sparsity in delay-Doppler manifests itself as time-frequency coherence in the STF domain. From a capacity perspective, sparse channels are asymptotically coherent: the gap between coherent and non-coherent extremes vanishes in the limit of large signal space dimension without the need for peaky signaling. From a reliability viewpoint, there is a fundamental tradeoff between channel diversity and learnability that can be optimized to maximize the error exponent at any rate by appropriately choosing the signaling duration as a function of bandwidth.", ["Independence (probability theory)", "Wireless", "Waveform", "Fourier transform", "Wideband", "Frequency", "Information theory", "Multipath propagation", "Signal (electronics)", "Communication", "Degrees of freedom (physics and chemistry)", "Exponentiation", "Dimension", "Coherence (physics)"]], ["Scanning and Sequential Decision Making for Multi-Dimensional Data - Part II: the Noisy Case", "We consider the problem of sequential decision making on random fields corrupted by noise. In this scenario, the decision maker observes a noisy version of the data, yet judged with respect to the clean data. In particular, we first consider the problem of sequentially scanning and filtering noisy random fields. In this case, the sequential filter is given the freedom to choose the path over which it traverses the random field (e.g., noisy image or video sequence), thus it is natural to ask what is the best achievable performance and how sensitive this performance is to the choice of the scan. We formally define the problem of scanning and filtering, derive a bound on the best achievable performance and quantify the excess loss occurring when non-optimal scanners are used, compared to optimal scanning and filtering. We then discuss the problem of sequential scanning and prediction of noisy random fields. This setting is a natural model for applications such as restoration and coding of noisy images. We formally define the problem of scanning and prediction of a noisy multidimensional array and relate the optimal performance to the clean scandictability defined by Merhav and Weissman. Moreover, bounds on the excess loss due to sub-optimal scans are derived, and a universal prediction algorithm is suggested. This paper is the second part of a two-part paper. The first paper dealt with sequential decision making on noiseless data arrays, namely, when the decision maker is judged with respect to the same data array it observes.", ["Random field", "Decision making", "Array data structure"]], ["Cryptanalysis of group-based key agreement protocols using subgroup distance functions", "We introduce a new approach for cryptanalysis of key agreement protocols based on noncommutative groups. This approach uses functions that estimate the distance of a group element to a given subgroup. We test it against the Shpilrain-Ushakov protocol, which is based on Thompson's group F.", ["Cryptanalysis"]], ["An online algorithm for generating fractal hash chains applied to digital chains of custody", "This paper gives an online algorithm for generating Jakobsson's fractal hash chains. Our new algorithm compliments Jakobsson's fractal hash chain algorithm for preimage traversal since his algorithm assumes the entire hash chain is precomputed and a particular list of Ceiling(log n) hash elements or pebbles are saved. Our online algorithm for hash chain traversal incrementally generates a hash chain of n hash elements without knowledge of n before it starts. For any n, our algorithm stores only the Ceiling(log n) pebbles which are precisely the inputs for Jakobsson's amortized hash chain preimage traversal algorithm. This compact representation is useful to generate, traverse, and store a number of large digital hash chains on a small and constrained device. We also give an application using both Jakobsson's and our new algorithm applied to digital chains of custody for validating dynamically changing forensics data.", ["Online algorithm", "Hash chain", "Fractal", "Algorithm", "Amortized analysis"]], ["A stochastic non-cooperative game for energy efficiency in wireless data networks", "In this paper the issue of energy efficiency in CDMA wireless data networks is addressed through a game theoretic approach. Building on a recent paper by the first two authors, wherein a non-cooperative game for spreading-code optimization, power control, and receiver design has been proposed to maximize the ratio of data throughput to transmit power for each active user, a stochastic algorithm is here described to perform adaptive implementation of the said non-cooperative game. The proposed solution is based on a combination of RLS-type and LMS-type adaptations, and makes use of readily available measurements. Simulation results show that its performance approaches with satisfactory accuracy that of the non-adaptive game, which requires a much larger amount of prior information.", ["Non-cooperative game", "Energy", "Game theory", "Stochastic", "Wireless", "Cooperative game", "Program optimization", "Algorithm", "Mathematical optimization", "Code division multiple access", "Throughput", "Data", "Computer network"]], ["An Extensible Timing Infrastructure for Adaptive Large-scale Applications", "Real-time access to accurate and reliable timing information is necessary to profile scientific applications, and crucial as simulations become increasingly complex, adaptive, and large-scale. The Cactus Framework provides flexible and extensible capabilities for timing information through a well designed infrastructure and timing API. Applications built with Cactus automatically gain access to built-in timers, such as gettimeofday and getrusage, system-specific hardware clocks, and high-level interfaces such as PAPI. We describe the Cactus timer interface, its motivation, and its implementation. We then demonstrate how this timing information can be used by an example scientific application to profile itself, and to dynamically adapt itself to a changing environment at run time.", ["Application programming interface", "Clock signal", "Run time (program lifecycle phase)", "Infrastructure", "Precision Approach Path Indicator", "Simulation", "Motivation"]], ["Spectral Efficiency of Spectrum Pooling Systems", "In this contribution, we investigate the idea of using cognitive radio to reuse locally unused spectrum to increase the total system capacity. We consider a multiband/wideband system in which the primary and cognitive users wish to communicate to different receivers, subject to mutual interference and assume that each user knows only his channel and the unused spectrum through adequate sensing. The basic idea under the proposed scheme is based on the notion of spectrum pooling. The idea is quite simple: a cognitive radio will listen to the channel and, if sensed idle, will transmit during the voids. It turns out that, although its simplicity, the proposed scheme showed very interesting features with respect to the spectral efficiency and the maximum number of possible pairwise cognitive communications. We impose the constraint that users successively transmit over available bands through selfish water filling. For the first time, our study has quantified the asymptotic (with respect to the band) achievable gain of using spectrum pooling in terms of spectral efficiency compared to classical radio systems. We then derive the total spectral efficiency as well as the maximum number of possible pairwise communications of such a spectrum pooling system.", ["Cognitive radio", "Radio", "Spectral efficiency"]], ["A competitive multi-agent model of interbank payment systems", "We develop a dynamic multi-agent model of an interbank payment system where banks choose their level of available funds on the basis of private payoff maximisation. The model consists of the repetition of a simultaneous move stage game with incomplete information, incomplete monitoring, and stochastic payoffs. Adaptation takes place with bayesian updating, with banks maximizing immediate payoffs. We carry out numerical simulations to solve the model and investigate two special scenarios: an operational incident and exogenous throughput guidelines for payment submission. We find that the demand for intraday credit is an S-shaped function of the cost ratio between intraday credit costs and the costs associated with delaying payments. We also find that the demand for liquidity is increased both under operational incidents and in the presence of effective throughput guidelines.", ["Exogeny", "Multi-agent system", "Mathematical optimization", "Market liquidity", "Stochastic"]], ["On the Shannon capacity and queueing stability of random access multicast", "We study and compare the Shannon capacity region and the stable throughput region for a random access system in which source nodes multicast their messages to multiple destination nodes. Under an erasure channel model which accounts for interference and allows for multipacket reception, we first characterize the Shannon capacity region. We then consider a queueing-theoretic formulation and characterize the stable throughput region for two different transmission policies: a retransmission policy and random linear coding. Our results indicate that for large blocklengths, the random linear coding policy provides a higher stable throughput than the retransmission policy. Furthermore, our results provide an example of a transmission policy for which the Shannon capacity region strictly outer bounds the stable throughput region, which contradicts an unproven conjecture that the Shannon capacity and stable throughput coincide for random access systems.", ["Linear code", "Multicast", "Transmission (telecommunications)", "Forward error correction", "Channel (communications)", "Throughput", "Channel capacity"]], ["Measuring and Localing Homology Classes", "We develop a method for measuring and localizing homology classes. This involves two problems. First, we define relevant notions of size for both a homology class and a homology group basis, using ideas from relative homology. Second, we propose an algorithm to compute the optimal homology basis, using techniques from persistent homology and finite field algebra. Classes of the computed optimal basis are localized with cycles conveying their sizes. The algorithm runs in $O(\\beta^4 n^3 \\log^2 n)$ time, where $n$ is the size of the simplicial complex and $\\beta$ is the Betti number of the homology group.", ["Betti number", "Simplicial complex", "Finite field", "Homology (mathematics)", "Finite set", "Algorithm", "Natural logarithm", "Algebra"]], ["Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement", "A transmitter without channel state information (CSI) wishes to send a delay-limited Gaussian source over a slowly fading channel. The source is coded in superimposed layers, with each layer successively refining the description in the previous one. The receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion. The expected distortion is minimized by optimally allocating the transmit power among the source layers. For two source layers, the allocation is optimal when power is first assigned to the higher layer up to a power ceiling that depends only on the channel fading distribution; all remaining power, if any, is allocated to the lower layer. For convex distortion cost functions with convex constraints, the minimization is formulated as a convex optimization problem. In the limit of a continuum of infinite layers, the minimum expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution. As the bandwidth ratio b (channel uses per source symbol) tends to zero, the power distribution that minimizes expected distortion converges to the one that maximizes expected capacity. While expected distortion can be improved by acquiring CSI at the transmitter (CSIT) or by increasing diversity from the realization of independent fading paths, at high SNR the performance benefit from diversity exceeds that from CSIT, especially when b is large.", ["Channel state information", "Fading", "Convex optimization", "Optimization problem", "Differential equation", "Linear differential equation", "Transmitter", "Mathematical optimization", "Distortion"]], ["Computability of simple games: A characterization and application to the core", "The class of algorithmically computable simple games (i) includes the class of games that have finite carriers and (ii) is included in the class of games that have finite winning coalitions. This paper characterizes computable games, strengthens the earlier result that computable games violate anonymity, and gives examples showing that the above inclusions are strict. It also extends Nakamura's theorem about the nonemptyness of the core and shows that computable games have a finite Nakamura number, implying that the number of alternatives that the players can deal with rationally is restricted.", ["Computable function", "Finite set"]], ["Recovering Multiplexing Loss Through Successive Relaying Using Repetition Coding", "In this paper, a transmission protocol is studied for a two relay wireless network in which simple repetition coding is applied at the relays. Information-theoretic achievable rates for this transmission scheme are given, and a space-time V-BLAST signalling and detection method that can approach them is developed. It is shown through the diversity multiplexing tradeoff analysis that this transmission scheme can recover the multiplexing loss of the half-duplex relay network, while retaining some diversity gain. This scheme is also compared with conventional transmission protocols that exploit only the diversity of the network at the cost of a multiplexing loss. It is shown that the new transmission protocol offers significant performance advantages over conventional protocols, especially when the interference between the two relays is sufficiently strong.", ["Wireless network", "Duplex (telecommunications)", "Signaling (telecommunications)", "Communications protocol", "Interference (communication)", "Multiplexing", "Wireless", "Half-duplex", "Computer network", "BLAST", "Information theory", "Streaming media", "Transmission (telecommunications)", "Forward error correction", "Diversity scheme"]], ["Acyclicity of Preferences, Nash Equilibria, and Subgame Perfect Equilibria: a Formal and Constructive Equivalence", "In 1953, Kuhn showed that every sequential game has a Nash equilibrium by showing that a procedure, named ``backward induction'' in game theory, yields a Nash equilibrium. It actually yields Nash equilibria that define a proper subclass of Nash equilibria. In 1965, Selten named this proper subclass subgame perfect equilibria. In game theory, payoffs are rewards usually granted at the end of a game. Although traditional game theory mainly focuses on real-valued payoffs that are implicitly ordered by the usual total order over the reals, works of Simon or Blackwell already involved partially ordered payoffs. This paper generalises the notion of sequential game by replacing real-valued payoff functions with abstract atomic objects, called outcomes, and by replacing the usual total order over the reals with arbitrary binary relations over outcomes, called preferences. This introduces a general abstract formalism where Nash equilibrium, subgame perfect equilibrium, and ``backward induction'' can still be defined. This paper proves that the following three propositions are equivalent: 1) Preferences over the outcomes are acyclic. 2) Every sequential game has a Nash equilibrium. 3) Every sequential game has a subgame perfect equilibrium. The result is fully computer-certified using Coq. Beside the additional guarantee of correctness, the activity of formalisation using Coq also helps clearly identify the useful definitions and the main articulations of the proof.", ["Nash equilibrium", "Sequential game", "Subgame perfect equilibrium", "Backward induction", "Partially ordered set", "Game theory", "Total order", "Binary relation", "Real number", "Computer"]], ["Optimal Separable Algorithms to Compute the Reverse Euclidean Distance Transformation and Discrete Medial Axis in Arbitrary Dimension", "In binary images, the distance transformation (DT) and the geometrical skeleton extraction are classic tools for shape analysis. In this paper, we present time optimal algorithms to solve the reverse Euclidean distance transformation and the reversible medial axis extraction problems for $d$-dimensional images. We also present a $d$-dimensional medial axis filtering process that allows us to control the quality of the reconstructed shape.", ["Euclidean distance", "Algorithm", "Medial axis", "Geometry", "Coordinate system"]], ["Multiuser detection in a dynamic environment Part I: User identification and data detection", "In random-access communication systems, the number of active users varies with time, and has considerable bearing on receiver's performance. Thus, techniques aimed at identifying not only the information transmitted, but also that number, play a central role in those systems. An example of application of these techniques can be found in multiuser detection (MUD). In typical MUD analyses, receivers are based on the assumption that the number of active users is constant and known at the receiver, and coincides with the maximum number of users entitled to access the system. This assumption is often overly pessimistic, since many users might be inactive at any given time, and detection under the assumption of a number of users larger than the real one may impair performance. The main goal of this paper is to introduce a general approach to the problem of identifying active users and estimating their parameters and data in a random-access system where users are continuously entering and leaving the system. The tool whose use we advocate is Random-Set Theory: applying this, we derive optimum receivers in an environment where the set of transmitters comprises an unknown number of elements. In addition, we can derive Bayesian-filter equations which describe the evolution with time of the a posteriori probability density of the unknown user parameters, and use this density to derive optimum detectors. In this paper we restrict ourselves to interferer identification and data detection, while in a companion paper we shall examine the more complex problem of estimating users' parameters.", ["A priori and a posteriori", "Communication", "Evolution", "Data", "MUD"]], ["The Road to Quantum Artificial Intelligence", "This paper overviews the basic principles and recent advances in the emerging field of Quantum Computation (QC), highlighting its potential application to Artificial Intelligence (AI). The paper provides a very brief introduction to basic QC issues like quantum registers, quantum gates and quantum algorithms and then it presents references, ideas and research guidelines on how QC can be used to deal with some basic AI problems, such as search and pattern matching, as soon as quantum computers become widely available.", ["Quantum algorithm", "Algorithm", "Pattern matching", "Artificial intelligence", "Quantum computer", "Computation"]], ["Open Access Publishing in Particle Physics: A Brief Introduction for the non-Expert", "Open Access to particle physics literature does not sound particularly new or exciting, since particle physicists have been reading preprints for decades, and arXiv.org for 15 years. However new movements in Europe are attempting to make the peer-reviewed literature of the field fully Open Access. This is not a new movement, nor is it restricted to this field. However, given the field's history of preprints and eprints, it is well suited to a change to a fully Open Access publishing model. Data shows that 90% of HEP published literature is freely available online, meaning that HEP libraries have little need for expensive journal subscriptions. As libraries begin to cancel journal subscriptions, the peer review process will lose its primary source of funding. Open Access publishing models can potentially address this issue. European physicists and funding agencies are proposing a consortium, SCOAP3, that might solve many of the objections to traditional Open Access publishing models in Particle Physics. These proposed changes should be viewed as a starting point for a serious look at the field's publication model, and are at least worthy of attention, if not adoption.", ["Peer review", "Literature", "Particle physics", "Consortium", "ArXiv", "Library", "Europe", "Physics"]], ["Linear Tabling Strategies and Optimizations", "Recently, the iterative approach named linear tabling has received considerable attention because of its simplicity, ease of implementation, and good space efficiency. Linear tabling is a framework from which different methods can be derived based on the strategies used in handling looping subgoals. One decision concerns when answers are consumed and returned. This paper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies, and compares them both qualitatively and quantitatively. The results indicate that, while the lazy strategy has good locality and is well suited for finding all solutions, the eager strategy is comparable in speed with the lazy strategy and is well suited for programs with cuts. Linear tabling relies on depth-first iterative deepening rather than suspension to compute fixpoints. Each cluster of inter-dependent subgoals as represented by a top-most looping subgoal is iteratively evaluated until no subgoal in it can produce any new answers. Naive re-evaluation of all looping subgoals, albeit simple, may be computationally unacceptable. In this paper, we also introduce semi-naive optimization, an effective technique employed in bottom-up evaluation of logic programs to avoid redundant joins of answers, into linear tabling. We give the conditions for the technique to be safe (i.e. sound and complete) and propose an optimization technique called {\\it early answer promotion} to enhance its effectiveness. Benchmarking in B-Prolog demonstrates that with this optimization linear tabling compares favorably well in speed with the state-of-the-art implementation of SLG.", ["Iteration", "Compiler optimization", "Logic", "Mathematical optimization", "Iterative deepening depth-first search", "Prolog", "Urban areas in Sweden", "Linear"]], ["Linearly bounded infinite graphs", "Linearly bounded Turing machines have been mainly studied as acceptors for context-sensitive languages. We define a natural class of infinite automata representing their observable computational behavior, called linearly bounded graphs. These automata naturally accept the same languages as the linearly bounded machines defining them. We present some of their structural properties as well as alternative characterizations in terms of rewriting systems and context-sensitive transductions. Finally, we compare these graphs to rational graphs, which are another class of automata accepting the context-sensitive languages, and prove that in the bounded-degree case, rational graphs are a strict sub-class of linearly bounded graphs.", ["Alan Turing", "Automaton", "Turing machine", "Finite-state machine", "Infinity", "Observable", "Graph (mathematics)"]], ["Making Random Choices Invisible to the Scheduler", "When dealing with process calculi and automata which express both nondeterministic and probabilistic behavior, it is customary to introduce the notion of scheduler to solve the nondeterminism. It has been observed that for certain applications, notably those in security, the scheduler needs to be restricted so not to reveal the outcome of the protocol's random choices, or otherwise the model of adversary would be too strong even for ``obviously correct'' protocols. We propose a process-algebraic framework in which the control on the scheduler can be specified in syntactic terms, and we show how to apply it to solve the problem mentioned above. We also consider the definition of (probabilistic) may and must preorders, and we show that they are precongruences with respect to the restricted schedulers. Furthermore, we show that all the operators of the language, except replication, distribute over probabilistic summation, which is a useful property for verification.", ["Process calculus", "Syntax", "Automaton", "Probability"]], ["Multidimensional Coded Modulation in Block-Fading Channnels", "We study the problem of constructing coded modulation schemes over multidimensional signal sets in Nakagami-$m$ block-fading channels. In particular, we consider the optimal diversity reliability exponent of the error probability when the multidimensional constellation is obtained as the rotation of classical complex-plane signal constellations. We show that multidimensional rotations of full dimension achieve the optimal diversity reliability exponent, also achieved by Gaussian constellations. Multidimensional rotations of full dimension induce a large decoding complexity, and in some cases it might be beneficial to use multiple rotations of smaller dimension. We also study the diversity reliability exponent in this case, which yields the optimal rate-diversity-complexity tradeoff in block-fading channels with discrete inputs.", ["Modulation", "Constellation", "Exponentiation", "Rotation"]], ["Generalizing Consistency and other Constraint Properties to Quantified Constraints", "Quantified constraints and Quantified Boolean Formulae are typically much more difficult to reason with than classical constraints, because quantifier alternation makes the usual notion of solution inappropriate. As a consequence, basic properties of Constraint Satisfaction Problems (CSP), such as consistency or substitutability, are not completely understood in the quantified case. These properties are important because they are the basis of most of the reasoning methods used to solve classical (existentially quantified) constraints, and one would like to benefit from similar reasoning methods in the resolution of quantified constraints. In this paper, we show that most of the properties that are used by solvers for CSP can be generalized to quantified CSP. This requires a re-thinking of a number of basic concepts; in particular, we propose a notion of outcome that generalizes the classical notion of solution and on which all definitions are based. We propose a systematic study of the relations which hold between these properties, as well as complexity results regarding the decision of these properties. Finally, and since these problems are typically intractable, we generalize the approach used in CSP and propose weaker, easier to check notions based on locality, which allow to detect these properties incompletely but in polynomial time.", ["Deductive reasoning", "Polynomial time", "Existential quantification", "Constraint satisfaction problem", "Quantification", "Computational complexity theory", "Consistency", "Boolean algebra", "Reason", "Polynomial"]], ["MI image registration using prior knowledge", "Subtraction of aligned images is a means to assess changes in a wide variety of clinical applications. In this paper we explore the information theoretical origin of Mutual Information (MI), which is based on Shannon's entropy.However, the interpretation of standard MI registration as a communication channel suggests that MI is too restrictive a criterion. In this paper the concept of Mutual Information (MI) is extended to (Normalized) Focussed Mutual Information (FMI) to incorporate prior knowledge to overcome some shortcomings of MI. We use this to develop new methodologies to successfully address specific registration problems, the follow-up of dental restorations, cephalometry, and the monitoring of implants.", ["Image registration", "Entropy", "Information theory", "Channel (communications)", "Communication", "Mutual information", "Claude Shannon"]], ["A Logic of Reachable Patterns in Linked Data-Structures", "We define a new decidable logic for expressing and checking invariants of programs that manipulate dynamically-allocated objects via pointers and destructive pointer updates. The main feature of this logic is the ability to limit the neighborhood of a node that is reachable via a regular expression from a designated node. The logic is closed under boolean operations (entailment, negation) and has a finite model property. The key technical result is the proof of decidability. We show how to express precondition, postconditions, and loop invariants for some interesting programs. It is also possible to express properties such as disjointness of data-structures, and low-level heap mutations. Moreover, our logic can express properties of arbitrary data-structures and of an arbitrary number of pointer fields. The latter provides a way to naturally specify postconditions that relate the fields on entry to a procedure to the fields on exit. Therefore, it is possible to use the logic to automatically prove partial correctness of programs performing low-level heap mutations.", ["Regular expression", "Kripke semantics", "Entailment", "Linked Data", "Decidability (logic)", "Correctness (computer science)", "Disjoint sets", "Invariant (computer science)", "Negation", "Pointer (computing)", "Dynamic memory allocation", "Precondition", "Boolean data type", "Formal verification", "Boolean function", "Finite set"]], ["On How Developers Test Open Source Software Systems", "Engineering software systems is a multidisciplinary activity, whereby a number of artifacts must be created - and maintained - synchronously. In this paper we investigate whether production code and the accompanying tests co-evolve by exploring a project's versioning system, code coverage reports and size-metrics. Three open source case studies teach us that testing activities usually start later on during the lifetime and are more \"phased\", although we did not observe increasing testing activity before releases. Furthermore, we note large differences in the levels of test coverage given the proportion of test code.", ["Open source", "Code coverage", "Software versioning", "Computer software", "Open-source software", "Case study", "Engineering", "Software system"]], ["Triple-loop networks with arbitrarily many minimum distance diagrams", "Minimum distance diagrams are a way to encode the diameter and routing information of multi-loop networks. For the widely studied case of double-loop networks, it is known that each network has at most two such diagrams and that they have a very definite form \"L-shape''. In contrast, in this paper we show that there are triple-loop networks with an arbitrarily big number of associated minimum distance diagrams. For doing this, we build-up on the relations between minimum distance diagrams and monomial ideals.", ["Routing"]], ["Subjective Information Measure and Rate Fidelity Theory", "Using fish-covering model, this paper intuitively explains how to extend Hartley's information formula to the generalized information formula step by step for measuring subjective information: metrical information (such as conveyed by thermometers), sensory information (such as conveyed by color vision), and semantic information (such as conveyed by weather forecasts). The pivotal step is to differentiate condition probability and logical condition probability of a message. The paper illustrates the rationality of the formula, discusses the coherence of the generalized information formula and Popper's knowledge evolution theory. For optimizing data compression, the paper discusses rate-of-limiting-errors and its similarity to complexity-distortion based on Kolmogorov's complexity theory, and improves the rate-distortion theory into the rate-fidelity theory by replacing Shannon's distortion with subjective mutual information. It is proved that both the rate-distortion function and the rate-fidelity function are equivalent to a rate-of-limiting-errors function with a group of fuzzy sets as limiting condition, and can be expressed by a formula of generalized mutual information for lossy coding, or by a formula of generalized entropy for lossless coding. By analyzing the rate-fidelity function related to visual discrimination and digitized bits of pixels of images, the paper concludes that subjective information is less than or equal to objective (Shannon's) information; there is an optimal matching point at which two kinds of information are equal; the matching information increases with visual discrimination (defined by confusing probability) rising; for given visual discrimination, too high resolution of images or too much objective information is wasteful.", ["Andrey Kolmogorov", "Fuzzy set", "Mutual information", "Subjectivity", "Evolution", "Entropy", "Rationality", "Semantics", "Thermometer", "Probability", "Data compression", "Data", "Lossy compression", "Complexity", "Karl Popper", "Pixel", "Color vision"]], ["Structural Health Monitoring Using Neural Network Based Vibrational System Identification", "Composite fabrication technologies now provide the means for producing high-strength, low-weight panels, plates, spars and other structural components which use embedded fiber optic sensors and piezoelectric transducers. These materials, often referred to as smart structures, make it possible to sense internal characteristics, such as delaminations or structural degradation. In this effort we use neural network based techniques for modeling and analyzing dynamic structural information for recognizing structural defects. This yields an adaptable system which gives a measure of structural integrity for composite structures.", ["Piezoelectricity", "Composite material", "System identification", "Neural network", "Fiber", "Optical fiber", "Nervous system"]], ["Distributed Transmit Diversity in Relay Networks", "We analyze fading relay networks, where a single-antenna source-destination terminal pair communicates through a set of half-duplex single-antenna relays using a two-hop protocol with linear processing at the relay level. A family of relaying schemes is presented which achieves the entire optimal diversity-multiplexing (DM) tradeoff curve. As a byproduct of our analysis, it follows that delay diversity and phase-rolling at the relay level are optimal with respect to the entire DM-tradeoff curve, provided the delays and the modulation frequencies, respectively, are chosen appropriately.", ["Modulation", "Frequency", "Multiplexing", "Transmit (FTP client)", "Half-duplex", "Antenna (radio)"]], ["Power-Efficient Direct-Voting Assurance for Data Fusion in Wireless Sensor Networks", "Wireless sensor networks place sensors into an area to collect data and send them back to a base station. Data fusion, which fuses the collected data before they are sent to the base station, is usually implemented over the network. Since the sensor is typically placed in locations accessible to malicious attackers, information assurance of the data fusion process is very important. A witness-based approach has been proposed to validate the fusion data. In this approach, the base station receives the fusion data and \"votes\" on the data from a randomly chosen sensor node. The vote comes from other sensor nodes, called \"witnesses,\" to verify the correctness of the fusion data. Because the base station obtains the vote through the chosen node, the chosen node could forge the vote if it is compromised. Thus, the witness node must encrypt the vote to prevent this forgery. Compared with the vote, the encryption requires more bits, increasing transmission burden from the chosen node to the base station. The chosen node consumes more power. This work improves the witness-based approach using direct voting mechanism such that the proposed scheme has better performance in terms of assurance, overhead, and delay. The witness node transmits the vote directly to the base station. Forgery is not a problem in this scheme. Moreover, fewer bits are necessary to represent the vote, significantly reducing the power consumption. Performance analysis and simulation results indicate that the proposed approach can achieve a 40 times better overhead than the witness-based approach.", ["Profiling (computer programming)", "Encryption", "Sensor", "Sensor node", "Simulation", "Base station", "Wireless sensor network", "Data fusion", "Information assurance"]], ["Morphing Ensemble Kalman Filters", "A new type of ensemble filter is proposed, which combines an ensemble Kalman filter (EnKF) with the ideas of morphing and registration from image processing. This results in filters suitable for nonlinear problems whose solutions exhibit moving coherent features, such as thin interfaces in wildfire modeling. The ensemble members are represented as the composition of one common state with a spatial transformation, called registration mapping, plus a residual. A fully automatic registration method is used that requires only gridded data, so the features in the model state do not need to be identified by the user. The morphing EnKF operates on a transformed state consisting of the registration mapping and the residual. Essentially, the morphing EnKF uses intermediate states obtained by morphing instead of linear combinations of the states.", ["Kalman filter", "Filter (signal processing)", "Image processing", "Electronic filter", "Wildfire", "Ensemble Kalman filter", "Nonlinear system"]], ["Optimal Iris Fuzzy Sketches", "Fuzzy sketches, introduced as a link between biometry and cryptography, are a way of handling biometric data matching as an error correction issue. We focus here on iris biometrics and look for the best error-correcting code in that respect. We show that two-dimensional iterative min-sum decoding leads to results near the theoretical limits. In particular, we experiment our techniques on the Iris Challenge Evaluation (ICE) database and validate our findings.", ["Forward error correction", "Error detection and correction", "Iteration", "Biometrics", "Cryptography", "Biostatistics", "Data", "Iris (anatomy)"]], ["On the Obfuscation Complexity of Planar Graphs", "Being motivated by John Tantalo's Planarity Game, we consider straight line plane drawings of a planar graph $G$ with edge crossings and wonder how obfuscated such drawings can be. We define $obf(G)$, the obfuscation complexity of $G$, to be the maximum number of edge crossings in a drawing of $G$. Relating $obf(G)$ to the distribution of vertex degrees in $G$, we show an efficient way of constructing a drawing of $G$ with at least $obf(G)/3$ edge crossings. We prove bounds $(\\delta(G)^2/24-o(1))n^2 < \\obf G <3 n^2$ for an $n$-vertex planar graph $G$ with minimum vertex degree $\\delta(G)\\ge 2$. The shift complexity of $G$, denoted by $shift(G)$, is the minimum number of vertex shifts sufficient to eliminate all edge crossings in an arbitrarily obfuscated drawing of $G$ (after shifting a vertex, all incident edges are supposed to be redrawn correspondingly). If $\\delta(G)\\ge 3$, then $shift(G)$ is linear in the number of vertices due to the known fact that the matching number of $G$ is linear. However, in the case $\\delta(G)\\ge2$ we notice that $shift(G)$ can be linear even if the matching number is bounded. As for computational complexity, we show that, given a drawing $D$ of a planar graph, it is NP-hard to find an optimum sequence of shifts making $D$ crossing-free.", ["Planar graph", "NP-hard", "Graph (mathematics)", "Computational complexity theory", "Degree (graph theory)", "Complexity", "Obfuscation"]], ["On the expressive power of planar perfect matching and permanents of bounded treewidth matrices", "Valiant introduced some 25 years ago an algebraic model of computation along with the complexity classes VP and VNP, which can be viewed as analogues of the classical classes P and NP. They are defined using non-uniform sequences of arithmetic circuits and provides a framework to study the complexity for sequences of polynomials. Prominent examples of difficult (that is, VNP-complete) problems in this model includes the permanent and hamiltonian polynomials. While the permanent and hamiltonian polynomials in general are difficult to evaluate, there have been research on which special cases of these polynomials admits efficient evaluation. For instance, Barvinok has shown that if the underlying matrix has bounded rank, both the permanent and the hamiltonian polynomials can be evaluated in polynomial time, and thus are in VP. Courcelle, Makowsky and Rotics have shown that for matrices of bounded treewidth several difficult problems (including evaluating the permanent and hamiltonian polynomials) can be solved efficiently. An earlier result of this flavour is Kasteleyn's theorem which states that the sum of weights of perfect matchings of a planar graph can be computed in polynomial time, and thus is in VP also. For general graphs this problem is VNP-complete. In this paper we investigate the expressive power of the above results. We show that the permanent and hamiltonian polynomials for matrices of bounded treewidth both are equivalent to arithmetic formulas. Also, arithmetic weakly skew circuits are shown to be equivalent to the sum of weights of perfect matchings of planar graphs.", ["Matching (graph theory)", "Planar graph", "Tree decomposition", "Polynomial time", "Arithmetic", "P versus NP problem", "Graph (mathematics)", "Matrix (mathematics)", "Complexity class", "NP (complexity)", "Theorem", "Computation", "Expressive power", "Polynomial", "Model of computation", "Hamiltonian path", "Glossary of graph theory"]], ["On complexity of optimized crossover for binary representations", "We consider the computational complexity of producing the best possible offspring in a crossover, given two solutions of the parents. The crossover operators are studied on the class of Boolean linear programming problems, where the Boolean vector of variables is used as the solution representation. By means of efficient reductions of the optimized gene transmitting crossover problems (OGTC) we show the polynomial solvability of the OGTC for the maximum weight set packing problem, the minimum weight set partition problem and for one of the versions of the simple plant location problem. We study a connection between the OGTC for linear Boolean programming problem and the maximum weight independent set problem on 2-colorable hypergraph and prove the NP-hardness of several special cases of the OGTC problem in Boolean linear programming.", ["Gene", "Linear programming", "Binary numeral system", "NP-hard", "Independent set (graph theory)", "Partition problem", "Packing problem", "Graph coloring", "Mathematical optimization", "Partition of a set", "Computational complexity theory", "Hypergraph", "Linear", "Polynomial"]], ["Maximizing Maximal Angles for Plane Straight-Line Graphs", "Let $G=(S, E)$ be a plane straight-line graph on a finite point set $S\\subset\\R^2$ in general position. The incident angles of a vertex $p \\in S$ of $G$ are the angles between any two edges of $G$ that appear consecutively in the circular order of the edges incident to $p$. A plane straight-line graph is called $\\phi$-open if each vertex has an incident angle of size at least $\\phi$. In this paper we study the following type of question: What is the maximum angle $\\phi$ such that for any finite set $S\\subset\\R^2$ of points in general position we can find a graph from a certain class of graphs on $S$ that is $\\phi$-open? In particular, we consider the classes of triangulations, spanning trees, and paths on $S$ and give tight bounds in most cases.", ["Subset", "Spanning tree", "Set (mathematics)", "Finite set", "Graph (mathematics)", "Line graph", "Plane (geometry)", "General position", "Phi", "Angles"]], ["Symbolic Reachability Analysis of Higher-Order Context-Free Processes", "We consider the problem of symbolic reachability analysis of higher-order context-free processes. These models are generalizations of the context-free processes (also called BPA processes) where each process manipulates a data structure which can be seen as a nested stack of stacks. Our main result is that, for any higher-order context-free process, the set of all predecessors of a given regular set of configurations is regular and effectively constructible. This result generalizes the analogous result which is known for level 1 context-free processes. We show that this result holds also in the case of backward reachability analysis under a regular constraint on configurations. As a corollary, we obtain a symbolic model checking algorithm for the temporal logic E(U,X) with regular atomic predicates, i.e., the fragment of CTL restricted to the EU and EX modalities.", ["Temporal logic", "Logic", "Data structure", "Model checking", "Modal logic", "Corollary", "Algorithm"]], ["Towards Understanding the Origin of Genetic Languages", "Molecular biology is a nanotechnology that works--it has worked for billions of years and in an amazing variety of circumstances. At its core is a system for acquiring, processing and communicating information that is universal, from viruses and bacteria to human beings. Advances in genetics and experience in designing computers have taken us to a stage where we can understand the optimisation principles at the root of this system, from the availability of basic building blocks to the execution of tasks. The languages of DNA and proteins are argued to be the optimal solutions to the information processing tasks they carry out. The analysis also suggests simpler predecessors to these languages, and provides fascinating clues about their origin. Obviously, a comprehensive unraveling of the puzzle of life would have a lot to say about what we may design or convert ourselves into.", ["Nanotechnology", "Bacteria", "Molecular biology", "DNA", "Biology", "Genetics", "Information processing"]], ["Translating a first-order modal language to relational algebra", "This paper is about Kripke structures that are inside a relational database and queried with a modal language. At first the modal language that is used is introduced, followed by a definition of the database and relational algebra. Based on these definitions two things are presented: a mapping from components of the modal structure to a relational database schema and instance, and a translation from queries in the modal language to relational algebra queries.", ["Relational algebra", "Relational database", "First-order logic", "Database", "Database schema"]], ["Interior Point Decoding for Linear Vector Channels", "In this paper, a novel decoding algorithm for low-density parity-check (LDPC) codes based on convex optimization is presented. The decoding algorithm, called interior point decoding, is designed for linear vector channels. The linear vector channels include many practically important channels such as inter symbol interference channels and partial response channels. It is shown that the maximum likelihood decoding (MLD) rule for a linear vector channel can be relaxed to a convex optimization problem, which is called a relaxed MLD problem. The proposed decoding algorithm is based on a numerical optimization technique so called interior point method with barrier function. Approximate variations of the gradient descent and the Newton methods are used to solve the convex optimization problem. In a decoding process of the proposed algorithm, a search point always lies in the fundamental polytope defined based on a low-density parity-check matrix. Compared with a convectional joint message passing decoder, the proposed decoding algorithm achieves better BER performance with less complexity in the case of partial response channels in many cases.", ["Interior point method", "Gradient descent", "Maximum likelihood", "Matrix (mathematics)", "Polytope", "Convex optimization", "Optimization problem", "Parity-check matrix", "Function (mathematics)", "Gradient", "Mathematical optimization", "Convex set", "Low-density parity-check code", "Interior (topology)", "Linear", "Algorithm", "Message passing"]], ["Average Stopping Set Weight Distribution of Redundant Random Matrix Ensembles", "In this paper, redundant random matrix ensembles (abbreviated as redundant random ensembles) are defined and their stopping set (SS) weight distributions are analyzed. A redundant random ensemble consists of a set of binary matrices with linearly dependent rows. These linearly dependent rows (redundant rows) significantly reduce the number of stopping sets of small size. An upper and lower bound on the average SS weight distribution of the redundant random ensembles are shown. From these bounds, the trade-off between the number of redundant rows (corresponding to decoding complexity of BP on BEC) and the critical exponent of the asymptotic growth rate of SS weight distribution (corresponding to decoding performance) can be derived. It is shown that, in some cases, a dense matrix with linearly dependent rows yields asymptotically (i.e., in the regime of small erasure probability) better performance than regular LDPC matrices with comparable parameters.", ["Matrix (mathematics)", "Logical matrix", "Random matrix", "Linear independence", "Probability distribution", "Upper and lower bounds", "Probability", "Sparse matrix", "Asymptotic analysis", "Exponentiation", "Low-density parity-check code", "Critical exponent"]], ["On Undetected Error Probability of Binary Matrix Ensembles", "In this paper, an analysis of the undetected error probability of ensembles of binary matrices is presented. The ensemble called the Bernoulli ensemble whose members are considered as matrices generated from i.i.d. Bernoulli source is mainly considered here. The main contributions of this work are (i) derivation of the error exponent of the average undetected error probability and (ii) closed form expressions for the variance of the undetected error probability. It is shown that the behavior of the exponent for a sparse ensemble is somewhat different from that for a dense ensemble. Furthermore, as a byproduct of the proof of the variance formula, simple covariance formula of the weight distribution is derived.", ["Exponentiation", "Logical matrix", "Probability", "Matrix (mathematics)", "Covariance", "Variance", "Probability distribution", "Closed-form expression"]], ["The use of the logarithm of the variate in the calculation of differential entropy among certain related statistical distributions", "This paper demonstrates that basic statistics (mean, variance) of the logarithm of the variate itself can be used in the calculation of differential entropy among random variables known to be multiples and powers of a common underlying variate. For the same set of distributions, the variance of the differential self-information is shown also to be a function of statistics of the logarithmic variate. Then entropy and its \"variance\" can be estimated using only statistics of the logarithmic variate plus constants, without reference to the traditional parameters of the variate.", ["Self-information", "Function (mathematics)", "Logarithm", "Differential entropy", "Statistics", "Differential equation", "Probability distribution", "Random variable", "Randomness", "Variable (mathematics)", "Distribution (mathematics)", "Variance", "Mean", "Entropy"]], ["On Term Rewriting Systems Having a Rational Derivation", "Several types of term rewriting systems can be distinguished by the way their rules overlap. In particular, we define the classes of prefix, suffix, bottom-up and top-down systems, which generalize similar classes on words. Our aim is to study the derivation relation of such systems (i.e. the reflexive and transitive closure of their rewriting relation) and, if possible, to provide a finite mechanism characterizing it. Using a notion of rational relations based on finite graph grammars, we show that the derivation of any bottom-up, top-down or suffix systems is rational, while it can be non recursive for prefix systems.", ["Finite set", "Transitive closure", "Rewriting", "Prefix"]], ["The Distance Geometry of Music", "We demonstrate relationships between the classic Euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. Specifically, we show how the structure of the Euclidean algorithm defines a family of rhythms which encompass over forty timelines (\\emph{ostinatos}) from traditional world music. We prove that these \\emph{Euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the Euclidean distances between all pairs of onsets, viewing onsets as points on a circle. Indeed, Euclidean rhythms are the unique rhythms that maximize this notion of \\emph{evenness}. We also show that essentially all Euclidean rhythms are \\emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. Finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. All of our results for musical rhythms apply equally well to musical scales. In addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by Erd\\H{o}s in the plane.", ["Distance geometry", "Euclidean algorithm", "Algorithm", "Musical scale", "Mathematics", "World music"]], ["Efficiency and Nash Equilibria in a Scrip System for P2P Networks", "A model of providing service in a P2P network is analyzed. It is shown that by adding a scrip system, a mechanism that admits a reasonable Nash equilibrium that reduces free riding can be obtained. The effect of varying the total amount of money (scrip) in the system on efficiency (i.e., social welfare) is analyzed, and it is shown that by maintaining the appropriate ratio between the total amount of money and the number of agents, efficiency is maximized. The work has implications for many online systems, not only P2P networks but also a wide variety of online forums for which scrip systems are popular, but formal analyses have been lacking.", ["Nash equilibrium", "Welfare", "Peer-to-peer", "Scrip"]], ["Optimizing Scrip Systems: Efficiency, Crashes, Hoarders, and Altruists", "We discuss the design of efficient scrip systems and develop tools for empirically analyzing them. For those interested in the empirical study of scrip systems, we demonstrate how characteristics of agents in a system can be inferred from the equilibrium distribution of money. From the perspective of a system designer, we examine the effect of the money supply on social welfare and show that social welfare is maximized by increasing the money supply up to the point that the system experiences a ``monetary crash,'' where money is sufficiently devalued that no agent is willing to perform a service. We also examine the implications of the presence of altruists and hoarders on the performance of the system. While a small number of altruists may improve social welfare, too many can also cause the system to experience a monetary crash, which may be bad for social welfare. Hoarders generally decrease social welfare but, surprisingly, they also promote system stability by helping prevent monetary crashes. In addition, we provide new technical tools for analyzing and computing equilibria by showing that our model exhibits strategic complementarities, which implies that there exist equilibria in pure strategies that can be computed efficiently.", ["Scrip", "Welfare", "Devaluation", "Empirical", "Markov chain", "Altruism", "Computing", "Money supply"]], ["The Battery-Discharge-Model: A Class of Stochastic Finite Automata to Simulate Multidimensional Continued Fraction Expansion", "We define an infinite stochastic state machine, the Battery-Discharge-Model (BDM), which simulates the behaviour of linear and jump complexity of the continued fraction expansion of multidimensional formal power series, a relevant security measure in the cryptanalysis of stream ciphers. We also obtain finite approximations to the infinite BDM, where polynomially many states suffice to approximate with an exponentially small error the probabilities and averages for linear and jump complexity of M-multisequences of length n over the finite field F_q, for any M, n, q.", ["Continued fraction", "Finite field", "Infinity", "Formal power series", "Finite-state machine", "Finite set", "Discharge (band)", "Stochastic", "Cryptanalysis", "Power series", "Probability", "Measure (mathematics)", "Cipher", "Linear"]], ["The Asymptotic Normalized Linear Complexity of Multisequences", "We show that the asymptotic linear complexity of a multisequence a in F_q^\\infty that is I := liminf L_a(n)/n and S := limsup L_a(n)/n satisfy the inequalities M/(M+1) <= S <= 1 and M(1-S) <= I <= 1-S/M, if all M sequences have nonzero discrepancy infinitely often, and all pairs (I,S) satisfying these conditions are met by 2^{\\aleph_0} multisequences a. This answers an Open Problem by Dai, Imamura, and Yang. Keywords: Linear complexity, multisequence, Battery Discharge Model, isometry.", ["Limit superior and limit inferior", "Isometry", "Nucleic acid sequence", "Complexity", "Aleph", "Linear"]], ["Grover search algorithm", "A quantum algorithm is a set of instructions for a quantum computer, however, unlike algorithms in classical computer science their results cannot be guaranteed. A quantum system can undergo two types of operation, measurement and quantum state transformation, operations themselves must be unitary (reversible). Most quantum algorithms involve a series of quantum state transformations followed by a measurement. Currently very few quantum algorithms are known and no general design methodology exists for their construction.", ["Quantum computer", "Quantum algorithm", "Quantum state", "Grover's algorithm", "Physical system", "Science", "Computer science"]], ["Secure Two-party Protocols for Point Inclusion Problem", "It is well known that, in theory, the general secure multi-party computation problem is solvable using circuit evaluation protocols. However, the communication complexity of the resulting protocols depend on the size of the circuit that expresses the functionality to be computed and hence can be impractical. Hence special solutions are needed for specific problems for efficiency reasons. The point inclusion problem in computational geometry is a special multiparty computation and has got many applications. Previous protocols for the secure point inclusion problem are not adequate. In this paper we modify some known solutions to the point inclusion problem in computational geometry to the frame work of secure two-party computation.", ["Secure multi-party computation", "Computational geometry", "Geometry", "Communication complexity", "Communication", "Computation"]], ["Second-Order Type Isomorphisms Through Game Semantics", "The characterization of second-order type isomorphisms is a purely syntactical problem that we propose to study under the enlightenment of game semantics. We study this question in the case of second-order &#955;$\\mu$-calculus, which can be seen as an extension of system F to classical logic, and for which we de&#64257;ne a categorical framework: control hyperdoctrines. Our game model of &#955;$\\mu$-calculus is based on polymorphic arenas (closely related to Hughes' hyperforests) which evolve during the play (following the ideas of Murawski-Ong). We show that type isomorphisms coincide with the \"equality\" on arenas associated with types. Finally we deduce the equational characterization of type isomorphisms from this equality. We also recover from the same model Roberto Di Cosmo's characterization of type isomorphisms for system F. This approach leads to a geometrical comprehension on the question of second order type isomorphisms, which can be easily extended to some other polymorphic calculi including additional programming features.", ["Syntax", "Game semantics", "Classical logic", "Logic", "Geometry", "Semantics", "System F", "Category (Kant)", "Order type", "Calculus"]], ["Curry-style type Isomorphisms and Game Semantics", "Curry-style system F, ie. system F with no explicit types in terms, can be seen as a core presentation of polymorphism from the point of view of programming languages. This paper gives a characterisation of type isomorphisms for this language, by using a game model whose intuitions come both from the syntax and from the game semantics universe. The model is composed of: an untyped part to interpret terms, a notion of game to interpret types, and a typed part to express the fact that an untyped strategy plays on a game. By analysing isomorphisms in the model, we prove that the equational system corresponding to type isomorphisms for Curry-style system F is the extension of the equational system for Church-style isomorphisms with a new, non-trivial equation: forall X.A = A[forall Y.Y/X] if X appears only positively in A.", ["Game semantics", "System F", "Syntax", "Semantics", "Isomorphism", "Curry", "Programming language"]], ["Truecluster matching", "Cluster matching by permuting cluster labels is important in many clustering contexts such as cluster validation and cluster ensemble techniques. The classic approach is to minimize the euclidean distance between two cluster solutions which induces inappropriate stability in certain settings. Therefore, we present the truematch algorithm that introduces two improvements best explained in the crisp case. First, instead of maximizing the trace of the cluster crosstable, we propose to maximize a chi-square transformation of this crosstable. Thus, the trace will not be dominated by the cells with the largest counts but by the cells with the most non-random observations, taking into account the marginals. Second, we suggest a probabilistic component in order to break ties and to make the matching algorithm truly random on random data. The truematch algorithm is designed as a building block of the truecluster framework and scales in polynomial time. First simulation results confirm that the truematch algorithm gives more consistent truecluster results for unequal cluster sizes. Free R software is available.", ["Simulation", "Euclidean distance", "Algorithm", "Randomness", "Random variable", "Polynomial time", "Cell (biology)", "Probability", "Polynomial"]], ["Defect-Tolerant CMOL Cell Assignment via Satisfiability", "We present a CAD framework for CMOL, a hybrid CMOS/ molecular circuit architecture. Our framework first transforms any logically synthesized circuit based on AND/OR/NOT gates to a NOR gate circuit, and then maps the NOR gates to CMOL. We encode the CMOL cell assignment problem as boolean conditions. The boolean constraint is satisfiable if and only if there is a way to map all the NOR gates to the CMOL cells. We further investigate various types of static defects for the CMOL architecture, and propose a reconfiguration technique that can deal with these defects through our CAD framework. This is the first automated framework for CMOL cell assignment, and the first to model several different CMOL static defects. Empirical results show that our approach is efficient and scalable.", ["NOR gate", "CMOS", "If and only if", "Satisfiability", "Assignment problem", "Architecture", "Boolean domain", "Computer-aided design"]], ["Computing Integer Powers in Floating-Point Arithmetic", "We introduce two algorithms for accurately evaluating powers to a positive integer in floating-point arithmetic, assuming a fused multiply-add (fma) instruction is available. We show that our log-time algorithm always produce faithfully-rounded results, discuss the possibility of getting correctly rounded results, and show that results correctly rounded in double precision can be obtained if extended-precision is available with the possibility to round into double precision (with a single rounding).", ["Floating point", "Arithmetic", "Natural number", "Integer", "Double precision floating-point format", "Algorithm", "Rounding"]], ["PERCEVAL: a Computer-Driven System for Experimentation on Auditory and Visual Perception", "Since perception tests are highly time-consuming, there is a need to automate as many operations as possible, such as stimulus generation, procedure control, perception testing, and data analysis. The computer-driven system we are presenting here meets these objectives. To achieve large flexibility, the tests are controlled by scripts. The system's core software resembles that of a lexical-syntactic analyzer, which reads and interprets script files sent to it. The execution sequence (trial) is modified in accordance with the commands and data received. This type of operation provides a great deal of flexibility and supports a wide variety of tests such as auditory-lexical decision making, phoneme monitoring, gating, phonetic categorization, word identification, voice quality, etc. To achieve good performance, we were careful about timing accuracy, which is the greatest problem in computerized perception tests.", ["Computer software", "Computer", "Phoneme", "Data analysis", "Decision making", "Visual perception"]], ["World-set Decompositions: Expressiveness and Efficient Algorithms", "Uncertain information is commonplace in real-world data management scenarios. The ability to represent large sets of possible instances (worlds) while supporting efficient storage and processing is an important challenge in this context. The recent formalism of world-set decompositions (WSDs) provides a space-efficient representation for uncertain data that also supports scalable processing. WSDs are complete for finite world-sets in that they can represent any finite set of possible worlds. For possibly infinite world-sets, we show that a natural generalization of WSDs precisely captures the expressive power of c-tables. We then show that several important decision problems are efficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we give a polynomial-time algorithm for factorizing WSDs, i.e. an efficient algorithm for minimizing such representations.", ["Polynomial", "Finite set", "NP-hard", "Solvable group", "Expressive power", "Algorithm", "Infinity"]], ["Mixed membership stochastic blockmodels", "Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social and protein interaction networks.", ["Gene", "Latent variable model", "Algorithm", "Latent variable", "Social network", "Structure (mathematical logic)", "Gene regulatory network", "Exchangeable random variables", "Email", "Protein"]], ["Loop corrections for message passing algorithms in continuous variable models", "In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model. Using the exactness of the averages for belief propagation for Gaussian models, a different way of obtaining the covariances is found, based on Belief Propagation on cavity graphs. We discuss the relation of this loop correction algorithm to Expectation Propagation algorithms for the case in which the model is no longer Gaussian, but slightly perturbed by nonlinear terms.", ["Nonlinear system", "Renormalization", "Gaussian process", "Belief propagation", "Message passing"]], ["Modeling Epidemic Spread in Synthetic Populations - Virtual Plagues in Massively Multiplayer Online Games", "A virtual plague is a process in which a behavior-affecting property spreads among characters in a Massively Multiplayer Online Game (MMOG). The MMOG individuals constitute a synthetic population, and the game can be seen as a form of interactive executable model for studying disease spread, albeit of a very special kind. To a game developer maintaining an MMOG, recognizing, monitoring, and ultimately controlling a virtual plague is important, regardless of how it was initiated. The prospect of using tools, methods and theory from the field of epidemiology to do this seems natural and appealing. We will address the feasibility of such a prospect, first by considering some basic measures used in epidemiology, then by pointing out the differences between real world epidemics and virtual plagues. We also suggest directions for MMOG developer control through epidemiological modeling. Our aim is understanding the properties of virtual plagues, rather than trying to eliminate them or mitigate their effects, as would be in the case of real infectious disease.", ["Infectious disease", "Disease", "Epidemiology", "Epidemic", "Massively multiplayer online game", "Mathematical modelling of infectious disease", "Multiplayer video game", "Bubonic plague"]], ["Temporal Runtime Verification using Monadic Difference Logic", "In this paper we present an algorithm for performing runtime verification of a bounded temporal logic over timed runs. The algorithm consists of three elements. First, the bounded temporal formula to be verified is translated into a monadic first-order logic over difference inequalities, which we call monadic difference logic. Second, at each step of the timed run, the monadic difference formula is modified by computing a quotient with the state and time of that step. Third, the resulting formula is checked for being a tautology or being unsatisfiable by a decision procedure for monadic difference logic. We further provide a simple decision procedure for monadic difference logic based on the data structure Difference Decision Diagrams. The algorithm is complete in a very strong sense on a subclass of temporal formulae characterized as homogeneously monadic and it is approximate on other formulae. The approximation comes from the fact that not all unsatisfiable or tautological formulae are recognised at the earliest possible time of the runtime verification. Contrary to existing approaches, the presented algorithms do not work by syntactic rewriting but employ efficient decision structures which make them applicable in real applications within for instance business software.", ["First-order logic", "Temporal logic", "Data structure", "Computer software", "Algorithm", "Computing", "Tautology (logic)", "Logic", "Decision problem", "Business software", "Unary operation", "Inequality (mathematics)"]], ["Dynamic User-Defined Similarity Searching in Semi-Structured Text Retrieval", "Modern text retrieval systems often provide a similarity search utility, that allows the user to find efficiently a fixed number k of documents in the data set that are most similar to a given query (here a query is either a simple sequence of keywords or the identifier of a full document found in previous searches that is considered of interest). We consider the case of a textual database made of semi-structured documents. Each field, in turns, is modelled with a specific vector space. The problem is more complex when we also allow each such vector space to have an associated user-defined dynamic weight that influences its contribution to the overall dynamic aggregated and weighted similarity. This dynamic problem has been tackled in a recent paper by Singitham et al. in in VLDB 2004. Their proposed solution, which we take as baseline, is a variant of the cluster-pruning technique that has the potential for scaling to very large corpora of documents, and is far more efficient than the naive exhaustive search. We devise an alternative way of embedding weights in the data structure, coupled with a non-trivial application of a clustering algorithm based on the furthest point first heuristic for the metric k-center problem. The validity of our approach is demonstrated experimentally by showing significant performance improvements over the scheme proposed in Singitham et al. in VLDB 2004. We improve significantly tradeoffs between query time and output quality with respect to the baseline method in Singitham et al. in in VLDB 2004, and also with respect to a novel method by Chierichetti et al. to appear in ACM PODS 2007. We also speed up the pre-processing time by a factor at least thirty.", ["Vector space", "Algorithm", "Text corpus", "Data set", "Heuristic", "Brute-force search", "Symposium on Principles of Database Systems", "Association for Computing Machinery", "Very large database", "Data structure", "Cluster analysis", "Nearest neighbor search", "Document retrieval", "Euclidean vector", "Embedding", "Database", "Information retrieval"]], ["An Improved Tight Closure Algorithm for Integer Octagonal Constraints", "Integer octagonal constraints (a.k.a. ``Unit Two Variables Per Inequality'' or ``UTVPI integer constraints'') constitute an interesting class of constraints for the representation and solution of integer problems in the fields of constraint programming and formal analysis and verification of software and hardware systems, since they couple algorithms having polynomial complexity with a relatively good expressive power. The main algorithms required for the manipulation of such constraints are the satisfiability check and the computation of the inferential closure of a set of constraints. The latter is called `tight' closure to mark the difference with the (incomplete) closure algorithm that does not exploit the integrality of the variables. In this paper we present and fully justify an O(n^3) algorithm to compute the tight closure of a set of UTVPI integer constraints.", ["Computer software", "Hardware", "Polynomial time", "Algorithm", "Expressive power", "Polynomial", "Closure (topology)", "Constraint programming"]], ["Local Area Damage Detection in Composite Structures Using Piezoelectric Transducers", "An integrated and automated smart structures approach for structural health monitoring is presented, utilizing an array of piezoelectric transducers attached to or embedded within the structure for both actuation and sensing. The system actively interrogates the structure via broadband excitation of multiple actuators across a desired frequency range. The structure's vibration signature is then characterized by computing the transfer functions between each actuator/sensor pair, and compared to the baseline signature. Experimental results applying the system to local area damage detection in a MD Explorer rotorcraft composite flexbeam are presented.", ["Frequency", "Sensor", "Transducer", "Broadband", "Computing", "Structural health monitoring", "Piezoelectricity", "Actuator", "Composite material"]], ["Two sources are better than one for increasing the Kolmogorov complexity of infinite sequences", "The randomness rate of an infinite binary sequence is characterized by the sequence of ratios between the Kolmogorov complexity and the length of the initial segments of the sequence. It is known that there is no uniform effective procedure that transforms one input sequence into another sequence with higher randomness rate. By contrast, we display such a uniform effective procedure having as input two independent sequences with positive but arbitrarily small constant randomness rate. Moreover the transformation is a truth-table reduction and the output has randomness rate arbitrarily close to 1.", ["Kolmogorov complexity", "Function (mathematics)", "Sequence", "Truth table", "Randomness", "Complexity"]], ["A randomized algorithm for the on-line weighted bipartite matching problem", "We study the on-line minimum weighted bipartite matching problem in arbitrary metric spaces. Here, $n$ not necessary disjoint points of a metric space $M$ are given, and are to be matched on-line with $n$ points of $M$ revealed one by one. The cost of a matching is the sum of the distances of the matched points, and the goal is to find or approximate its minimum. The competitive ratio of the deterministic problem is known to be $\\Theta(n)$. It was conjectured that a randomized algorithm may perform better against an oblivious adversary, namely with an expected competitive ratio $\\Theta(\\log n)$. We prove a slightly weaker result by showing a $o(\\log^3 n)$ upper bound on the expected competitive ratio. As an application the same upper bound holds for the notoriously hard fire station problem, where $M$ is the real line.", ["Randomized algorithm", "Metric space", "Real line", "Competitive analysis (online algorithm)", "Algorithm", "Fire station"]], ["Recursive n-gram hashing is pairwise independent, at best", "Many applications use sequences of n consecutive symbols (n-grams). Hashing these n-grams can be a performance bottleneck. For more speed, recursive hash families compute hash values by updating previous values. We prove that recursive hash families cannot be more than pairwise independent. While hashing by irreducible polynomials is pairwise independent, our implementations either run in time O(n) or use an exponential amount of memory. As a more scalable alternative, we make hashing by cyclic polynomials pairwise independent by ignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials is is twice as fast as hashing by irreducible polynomials. We also show that randomized Karp-Rabin hash families are not pairwise independent.", ["Hash function", "N-gram", "Minimax", "Recursion", "Pairwise independence", "Polynomial", "Gram"]], ["Towards an exact adaptive algorithm for the determinant of a rational matrix", "In this paper we propose several strategies for the exact computation of the determinant of a rational matrix. First, we use the Chinese Remaindering Theorem and the rational reconstruction to recover the rational determinant from its modular images. Then we show a preconditioning for the determinant which allows us to skip the rational reconstruction process and reconstruct an integer result. We compare those approaches with matrix preconditioning which allow us to treat integer instead of rational matrices. This allows us to introduce integer determinant algorithms to the rational determinant problem. In particular, we discuss the applicability of the adaptive determinant algorithm of [9] and compare it with the integer Chinese Remaindering scheme. We present an analysis of the complexity of the strategies and evaluate their experimental performance on numerous examples. This experience allows us to develop an adaptive strategy which would choose the best solution at the run time, depending on matrix properties. All strategies have been implemented in LinBox linear algebra library.", ["Linear algebra", "Determinant", "Algebra", "Preconditioner", "Computation", "Algorithm", "Run time (program lifecycle phase)", "Matrix (mathematics)", "Rational number", "Integer", "Theorem", "Adaptation"]], ["Modeling Computations in a Semantic Network", "Semantic network research has seen a resurgence from its early history in the cognitive sciences with the inception of the Semantic Web initiative. The Semantic Web effort has brought forth an array of technologies that support the encoding, storage, and querying of the semantic network data structure at the world stage. Currently, the popular conception of the Semantic Web is that of a data modeling medium where real and conceptual entities are related in semantically meaningful ways. However, new models have emerged that explicitly encode procedural information within the semantic network substrate. With these new technologies, the Semantic Web has evolved from a data modeling medium to a computational medium. This article provides a classification of existing computational modeling efforts and the requirements of supporting technologies that will aid in the further growth of this burgeoning domain.", ["Semantic network", "Semantic Web", "Cognition", "Cognitive science", "Data modeling", "Semantics", "Data structure", "Concept", "Scientific modelling", "Computer simulation", "Data"]], ["Symmetry Partition Sort", "In this paper, we propose a useful replacement for quicksort-style utility functions. The replacement is called Symmetry Partition Sort, which has essentially the same principle as Proportion Extend Sort. The maximal difference between them is that the new algorithm always places already partially sorted inputs (used as a basis for the proportional extension) on both ends when entering the partition routine. This is advantageous to speeding up the partition routine. The library function based on the new algorithm is more attractive than Psort which is a library function introduced in 2004. Its implementation mechanism is simple. The source code is clearer. The speed is faster, with O(n log n) performance guarantee. Both the robustness and adaptivity are better. As a library function, it is competitive.", ["Source code", "Quicksort", "Approximation algorithm", "Symmetry", "Algorithm", "Utility"]], ["Many concepts and two logics of algorithmic reduction", "Within the program of finding axiomatizations for various parts of computability logic, it was proved earlier that the logic of interactive Turing reduction is exactly the implicative fragment of Heyting's intuitionistic calculus. That sort of reduction permits unlimited reusage of the computational resource represented by the antecedent. An at least equally basic and natural sort of algorithmic reduction, however, is the one that does not allow such reusage. The present article shows that turning the logic of the first sort of reduction into the logic of the second sort of reduction takes nothing more than just deleting the contraction rule from its Gentzen-style axiomatization. The first (Turing) sort of interactive reduction is also shown to come in three natural versions. While those three versions are very different from each other, their logical behaviors (in isolation) turn out to be indistinguishable, with that common behavior being precisely captured by implicative intuitionistic logic. Among the other contributions of the present article is an informal introduction of a series of new -- finite and bounded -- versions of recurrence operations and the associated reduction operations. An online source on computability logic can be found at http://www.cis.upenn.edu/~giorgi/cl.html", ["Intuitionistic logic", "Computability logic", "Gerhard Gentzen", "Turing reduction", "Axiomatic system", "Axiomatic set theory", "Calculus", "Arend Heyting", "Finite set", "Alan Turing", "Algorithm", "Computational resource", "Logic", "Computability"]], ["On the End-to-End Distortion for a Buffered Transmission over Fading Channel", "In this paper, we study the end-to-end distortion/delay tradeoff for a analogue source transmitted over a fading channel. The analogue source is quantized and stored in a buffer until it is transmitted. There are two extreme cases as far as buffer delay is concerned: no delay and infinite delay. We observe that there is a significant power gain by introducing a buffer delay. Our goal is to investigate the situation between these two extremes. Using recently proposed \\emph{effective capacity} concept, we derive a closed-form formula for this tradeoff. For SISO case, an asymptotically tight upper bound for our distortion-delay curve is derived, which approaches to the infinite delay lower bound as $\\mathcal{D}_\\infty \\exp(\\frac{C}{\\tau_n})$, with $\\tau_n$ is the normalized delay, $C$ is a constant. For more general MIMO channel, we computed the distortion SNR exponent -- the exponential decay rate of the expected distortion in the high SNR regime. Numerical results demonstrate that introduction of a small amount delay can save significant transmission power.", ["Fading", "MIMO", "Exponentiation", "Closed-form expression", "Exponential decay", "Big O notation", "Exponential function", "Upper and lower bounds", "Infinity"]], ["Applying the Z-transform for the static analysis of floating-point numerical filters", "Digital linear filters are used in a variety of applications (sound treatment, control/command, etc.), implemented in software, in hardware, or a combination thereof. For safety-critical applications, it is necessary to bound all variables and outputs of all filters. We give a compositional, effective abstraction for digital linear filters expressed as block diagrams, yielding sound, precise bounds for fixed-point or floating-point implementations of the filters.", ["Computer software", "Z-transform", "Linear filter", "Hardware", "Static program analysis", "Linear", "Floating point", "Life-critical system"]], ["Multi-Agent Modeling Using Intelligent Agents in the Game of Lerpa", "Game theory has many limitations implicit in its application. By utilizing multiagent modeling, it is possible to solve a number of problems that are unsolvable using traditional game theory. In this paper reinforcement learning is applied to neural networks to create intelligent agents", ["Game theory", "Reinforcement learning", "Neural network"]], ["Automatic Detection of Pulmonary Embolism using Computational Intelligence", "This article describes the implementation of a system designed to automatically detect the presence of pulmonary embolism in lung scans. These images are firstly segmented, before alignment and feature extraction using PCA. The neural network was trained using the Hybrid Monte Carlo method, resulting in a committee of 250 neural networks and good results are obtained.", ["Monte Carlo method", "Pulmonary embolism", "Lung", "Neural network", "Embolism", "Computational intelligence", "Monte Carlo", "Feature extraction", "Nervous system"]], ["Submission of content to a digital object repository using a configurable workflow system", "The prototype of a workflow system for the submission of content to a digital object repository is here presented. It is based entirely on open-source standard components and features a service-oriented architecture. The front-end consists of Java Business Process Management (jBPM), Java Server Faces (JSF), and Java Server Pages (JSP). A Fedora Repository and a mySQL data base management system serve as a back-end. The communication between front-end and back-end uses a SOAP minimal binding stub. We describe the design principles and the construction of the prototype and discuss the possibilities and limitations of work ow creation by administrators. The code of the prototype is open-source and can be retrieved in the project escipub at http://sourceforge.net", ["SOAP", "MySQL", "Service-oriented architecture", "Database management system", "JBPM", "Business process management", "JavaServer Faces", "Database", "Open source", "JavaServer Pages", "Fedora (operating system)", "Communication", "Business process", "Architecture", "Java (programming language)", "Prototype", "Workflow"]], ["Multiplication of free random variables and the S-transform: the case of vanishing mean", "This note extends Voiculescu's S-transform based analytical machinery for free multiplicative convolution to the case where the mean of the probability measures vanishes. We show that with the right interpretation of the S-transform in the case of vanishing mean, the usual formula makes perfectly good sense.", ["Convolution", "Random variable", "Probability", "Variable (mathematics)", "Mathematical analysis", "Mean"]], ["Watermark Embedding and Detection", "The embedder and the detector (or decoder) are the two most important components of the digital watermarking systems. Thus in this work, we discuss how to design a better embedder and detector (or decoder). I first give a summary of the prospective applications of watermarking technology and major watermarking schemes in the literature. My review on the literature closely centers upon how the side information is exploited at both embedders and detectors. In Chapter 3, I explore the optimum detector or decoder according to a particular probability distribution of the host signals. We found that the performance of both multiplicative and additive spread spectrum schemes depends on the shape parameter of the host signals. For spread spectrum schemes, the performance of the detector or the decoder is reduced by the host interference. Thus I present a new host-interference rejection technique for the multiplicative spread spectrum schemes. Its embedding rule is tailored to the optimum detection or decoding rule. Though the host interference rejection schemes enjoy a big performance gain over the traditional spread spectrum schemes, their drawbacks that it is difficult for them to be implemented with the perceptual analysis to achieve the maximum allowable embedding level discourage their use in real scenarios. Thus, in the last chapters of this work, I introduce a double-sided technique to tackle this drawback. It differs from the host interference rejection schemes in that it utilizes but does not reject the host interference at its embedder. The perceptual analysis can be easily implemented in our scheme to achieve the maximum allowable level of embedding strength.", ["Probability", "Probability distribution", "Decoder", "Perception", "Digital watermarking", "Watermark", "Shape parameter", "Spread spectrum"]], ["Anonymity in the Wild: Mixes on unstructured networks", "As decentralized computing scenarios get ever more popular, unstructured topologies are natural candidates to consider running mix networks upon. We consider mix network topologies where mixes are placed on the nodes of an unstructured network, such as social networks and scale-free random networks. We explore the efficiency and traffic analysis resistance properties of mix networks based on unstructured topologies as opposed to theoretically optimal structured topologies, under high latency conditions. We consider a mix of directed and undirected network models, as well as one real world case study -- the LiveJournal friendship network topology. Our analysis indicates that mix-networks based on scale-free and small-world topologies have, firstly, mix-route lengths that are roughly comparable to those in expander graphs; second, that compromise of the most central nodes has little effect on anonymization properties, and third, batch sizes required for warding off intersection attacks need to be an order of magnitude higher in unstructured networks in comparison with expander graph topologies.", ["Graph (mathematics)", "Scale-free network", "Network topology", "Social network", "Topology", "Expander graph", "Mix network", "Anonymity", "Computing", "Order of magnitude", "LiveJournal", "Undirected graph"]], ["Abstract numeration systems on bounded languages and multiplication by a constant", "A set of integers is $S$-recognizable in an abstract numeration system $S$ if the language made up of the representations of its elements is accepted by a finite automaton. For abstract numeration systems built over bounded languages with at least three letters, we show that multiplication by an integer $\\lambda\\ge2$ does not preserve $S$-recognizability, meaning that there always exists a $S$-recognizable set $X$ such that $\\lambda X$ is not $S$-recognizable. The main tool is a bijection between the representation of an integer over a bounded language and its decomposition as a sum of binomial coefficients with certain properties, the so-called combinatorial numeration system.", ["Bijection", "Automaton", "Multiplication", "Finite set", "Combinatorics", "Integer", "Binomial coefficient"]], ["Challenges and Opportunities of Evolutionary Robotics", "Robotic hardware designs are becoming more complex as the variety and number of on-board sensors increase and as greater computational power is provided in ever-smaller packages on-board robots. These advances in hardware, however, do not automatically translate into better software for controlling complex robots. Evolutionary techniques hold the potential to solve many difficult problems in robotics which defy simple conventional approaches, but present many challenges as well. Numerous disciplines including artificial life, cognitive science and neural networks, rule-based systems, behavior-based control, genetic algorithms and other forms of evolutionary computation have contributed to shaping the current state of evolutionary robotics. This paper provides an overview of developments in the emerging field of evolutionary robotics, and discusses some of the opportunities and challenges which currently face practitioners in the field.", ["Evolutionary robotics", "Evolutionary computation", "Cognitive science", "Neural network", "Algorithm", "Computation", "Hardware", "Artificial life", "Behavior-based robotics", "Genetic algorithm", "Rule-based system", "Robotics", "Nervous system"]], ["Virtual Sensor Based Fault Detection and Classification on a Plasma Etch Reactor", "The SEMATECH sponsored J-88-E project teaming Texas Instruments with NeuroDyne (et al.) focused on Fault Detection and Classification (FDC) on a Lam 9600 aluminum plasma etch reactor, used in the process of semiconductor fabrication. Fault classification was accomplished by implementing a series of virtual sensor models which used data from real sensors (Lam Station sensors, Optical Emission Spectroscopy, and RF Monitoring) to predict recipe setpoints and wafer state characteristics. Fault detection and classification were performed by comparing predicted recipe and wafer state values with expected values. Models utilized include linear PLS, Polynomial PLS, and Neural Network PLS. Prediction of recipe setpoints based upon sensor data provides a capability for cross-checking that the machine is maintaining the desired setpoints. Wafer state characteristics such as Line Width Reduction and Remaining Oxide were estimated on-line using these same process sensors (Lam, OES, RFM). Wafer-to-wafer measurement of these characteristics in a production setting (where typically this information may be only sparsely available, if at all, after batch processing runs with numerous wafers have been completed) would provide important information to the operator that the process is or is not producing wafers within acceptable bounds of product quality. Production yield is increased, and correspondingly per unit cost is reduced, by providing the operator with the opportunity to adjust the process or machine before etching more wafers.", ["Texas Instruments", "Semiconductor", "Sensor", "Spectroscopy", "Radio frequency", "Neural network", "Plasma (physics)", "Batch processing", "Aluminium", "SEMATECH", "Plasma etching", "Semiconductor device fabrication", "Etching", "Diabetes management", "Texas", "Oxide"]], ["Motivation, Design, and Ubiquity: A Discussion of Research Ethics and Computer Science", "Modern society is permeated with computers, and the software that controls them can have latent, long-term, and immediate effects that reach far beyond the actual users of these systems. This places researchers in Computer Science and Software Engineering in a critical position of influence and responsibility, more than any other field because computer systems are vital research tools for other disciplines. This essay presents several key ethical concerns and responsibilities relating to research in computing. The goal is to promote awareness and discussion of ethical issues among computer science researchers. A hypothetical case study is provided, along with questions for reflection and discussion.", ["Computer science", "Motivation", "Computer", "Science", "Essay", "Engineering", "Software engineering", "Case study"]], ["Sampling Colourings of the Triangular Lattice", "We show that the Glauber dynamics on proper 9-colourings of the triangular lattice is rapidly mixing, which allows for efficient sampling. Consequently, there is a fully polynomial randomised approximation scheme (FPRAS) for counting proper 9-colourings of the triangular lattice. Proper colourings correspond to configurations in the zero-temperature anti-ferromagnetic Potts model. We show that the spin system consisting of proper 9-colourings of the triangular lattice has strong spatial mixing. This implies that there is a unique infinite-volume Gibbs distribution, which is an important property studied in statistical physics. Our results build on previous work by Goldberg, Martin and Paterson, who showed similar results for 10 colours on the triangular lattice. Their work was preceded by Salas and Sokal's 11-colour result. Both proofs rely on computational assistance, and so does our 9-colour proof. We have used a randomised heuristic to guide us towards rigourous results.", ["Ferromagnetism", "Potts model", "Gibbs measure", "Temperature", "Antiferromagnetism", "Hexagonal lattice", "Statistical physics", "Physics", "Heuristic", "Polynomial", "Polynomial-time approximation scheme", "Audio mixing (recorded music)"]], ["Relating two standard notions of secrecy", "Two styles of definitions are usually considered to express that a security protocol preserves the confidentiality of a data s. Reachability-based secrecy means that s should never be disclosed while equivalence-based secrecy states that two executions of a protocol with distinct instances for s should be indistinguishable to an attacker. Although the second formulation ensures a higher level of security and is closer to cryptographic notions of secrecy, decidability results and automatic tools have mainly focused on the first definition so far. This paper initiates a systematic investigation of the situations where syntactic secrecy entails strong secrecy. We show that in the passive case, reachability-based secrecy actually implies equivalence-based secrecy for digital signatures, symmetric and asymmetric encryption provided that the primitives are probabilistic. For active adversaries, we provide sufficient (and rather tight) conditions on the protocol for this implication to hold.", ["Public-key cryptography", "Confidentiality", "Cryptography", "Syntax", "Cryptographic protocol", "Security", "Encryption", "Digital signature"]], ["A collaborative framework to exchange and share product information within a supply chain context", "The new requirement for \"collaboration\" between multidisciplinary collaborators induces to exchange and share adequate information on the product, processes throughout the products' lifecycle. Thus, effective capture of information, and also its extraction, recording, exchange, sharing, and reuse become increasingly critical. These lead companies to adopt new improved methodologies in managing the exchange and sharing of information. The aim of this paper is to describe a collaborative framework system to exchange and share information, which is based on: (i) The Product Process Collaboration Organization model (PPCO) which defines product and process information, and the various collaboration methods for the organizations involved in the supply chain. (ii) Viewpoint model describes relationships between each actor and the comprehensive Product/Process model, defining each actor's \"domain of interest\" within the evolving product definition. (iii) A layer which defines the comprehensive organization and collaboration relationships between the actors within the supply chain. (iv) Based on the above relationships, the last layer proposes a typology of exchanged messages. A communication method, based on XML, is developed that supports optimal exchange/sharing of information. To illustrate the proposed framework system, an example is presented related to collaborative design of a new piston for an automotive engine. The focus is on user-viewpoint integration to ensure that the adequate information is retrieved from the PPCO.", ["Supply chain", "Communication", "View model", "Piston", "XML", "Internal combustion engine", "Automotive industry", "Process modeling", "Actor", "Collaboration", "Product (business)"]], ["Interpolant-Based Transition Relation Approximation", "In predicate abstraction, exact image computation is problematic, requiring in the worst case an exponential number of calls to a decision procedure. For this reason, software model checkers typically use a weak approximation of the image. This can result in a failure to prove a property, even given an adequate set of predicates. We present an interpolant-based method for strengthening the abstract transition relation in case of such failures. This approach guarantees convergence given an adequate set of predicates, without requiring an exact image computation. We show empirically that the method converges more rapidly than an earlier method based on counterexample analysis.", ["Approximation", "Computer software", "Computation", "Interpolation", "Model checking", "State transition system", "Counterexample", "Predicate (mathematical logic)"]], ["Compressed Regression", "Recent research has studied the role of sparsity in high dimensional regression and signal reconstruction, establishing theoretical limits for recovering sparse models from sparse data. This line of work shows that $\\ell_1$-regularized least squares regression can accurately estimate a sparse linear model from $n$ noisy examples in $p$ dimensions, even if $p$ is much larger than $n$. In this paper we study a variant of this problem where the original $n$ input variables are compressed by a random linear transformation to $m \\ll n$ examples in $p$ dimensions, and establish conditions under which a sparse linear model can be successfully recovered from the compressed data. A primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data. We characterize the number of random projections that are required for $\\ell_1$-regularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one, a property called ``sparsistence.'' In addition, we show that $\\ell_1$-regularized compressed regression asymptotically predicts as well as an oracle linear model, a property called ``persistence.'' Finally, we characterize the privacy properties of the compression procedure in information-theoretic terms, establishing upper bounds on the mutual information between the compressed and uncompressed data that decay to zero.", ["Mutual information", "Linear map", "Least squares", "Probability", "Information theory", "Linear model", "Regression analysis", "Linear regression"]], ["Tropical Implicitization and Mixed Fiber Polytopes", "The software TrIm offers implementations of tropical implicitization and tropical elimination, as developed by Tevelev and the authors. Given a polynomial map with generic coefficients, TrIm computes the tropical variety of the image. When the image is a hypersurface, the output is the Newton polytope of the defining polynomial. TrIm can thus be used to compute mixed fiber polytopes, including secondary polytopes.", ["Polynomial", "Hypersurface", "Computer software", "Fiber", "Polytope", "Tropics"]], ["Efficient Batch Update of Unique Identifiers in a Distributed Hash Table for Resources in a Mobile Host", "Resources in a distributed system can be identified using identifiers based on random numbers. When using a distributed hash table to resolve such identifiers to network locations, the straightforward approach is to store the network location directly in the hash table entry associated with an identifier. When a mobile host contains a large number of resources, this requires that all of the associated hash table entries must be updated when its network address changes. We propose an alternative approach where we store a host identifier in the entry associated with a resource identifier and the actual network address of the host in a separate host entry. This can drastically reduce the time required for updating the distributed hash table when a mobile host changes its network address. We also investigate under which circumstances our approach should or should not be used. We evaluate and confirm the usefulness of our approach with experiments run on top of OpenDHT.", ["Distributed hash table", "Hash table", "Identifier", "Distributed computing"]], ["A Novel Model of Working Set Selection for SMO Decomposition Methods", "In the process of training Support Vector Machines (SVMs) by decomposition methods, working set selection is an important technique, and some exciting schemes were employed into this field. To improve working set selection, we propose a new model for working set selection in sequential minimal optimization (SMO) decomposition methods. In this model, it selects B as working set without reselection. Some properties are given by simple proof, and experiments demonstrate that the proposed method is in general faster than existing methods.", ["Support vector machine", "Mathematical optimization"]], ["Code spectrum and reliability function: Gaussian channel", "A new approach for upper bounding the channel reliability function using the code spectrum is described. It allows to treat both low and high rate cases in a unified way. In particular, the earlier known upper bounds are improved, and a new derivation of the sphere-packing bound is presented.", ["Additive white Gaussian noise", "Sphere"]], ["Non-Parametric Field Estimation using Randomly Deployed, Noisy, Binary Sensors", "The reconstruction of a deterministic data field from binary-quantized noisy observations of sensors which are randomly deployed over the field domain is studied. The study focuses on the extremes of lack of deterministic control in the sensor deployment, lack of knowledge of the noise distribution, and lack of sensing precision and reliability. Such adverse conditions are motivated by possible real-world scenarios where a large collection of low-cost, crudely manufactured sensors are mass-deployed in an environment where little can be assumed about the ambient noise. A simple estimator that reconstructs the entire data field from these unreliable, binary-quantized, noisy observations is proposed. Technical conditions for the almost sure and integrated mean squared error (MSE) convergence of the estimate to the data field, as the number of sensors tends to infinity, are derived and their implications are discussed. For finite-dimensional, bounded-variation, and Sobolev-differentiable function classes, specific integrated MSE decay rates are derived. For the first and third function classes these rates are found to be minimax order optimal with respect to infinite precision sensing and known noise distribution.", ["Function (mathematics)", "Differentiable function", "Mean squared error", "Minimax", "Sensor", "Infinity", "Dimension (vector space)", "Probability distribution", "Almost surely", "Limit of a sequence", "Finite set", "Quantization (physics)"]], ["Probabilistic Interval Temporal Logic and Duration Calculus with Infinite Intervals: Complete Proof Systems", "The paper presents probabilistic extensions of interval temporal logic (ITL) and duration calculus (DC) with infinite intervals and complete Hilbert-style proof systems for them. The completeness results are a strong completeness theorem for the system of probabilistic ITL with respect to an abstract semantics and a relative completeness theorem for the system of probabilistic DC with respect to real-time semantics. The proposed systems subsume probabilistic real-time DC as known from the literature. A correspondence between the proposed systems and a system of probabilistic interval temporal logic with finite intervals and expanding modalities is established too.", ["Temporal logic", "Calculus", "David Hilbert", "Mathematical proof", "Hilbert system", "Theorem", "Infinity", "Finite set", "Literature", "Interval (mathematics)", "Semantics", "Logic", "Probability"]], ["Universal Quantile Estimation with Feedback in the Communication-Constrained Setting", "We consider the following problem of decentralized statistical inference: given i.i.d. samples from an unknown distribution, estimate an arbitrary quantile subject to limits on the number of bits exchanged. We analyze a standard fusion-based architecture, in which each of $m$ sensors transmits a single bit to the fusion center, which in turn is permitted to send some number $k$ bits of feedback. Supposing that each of $\\nodenum$ sensors receives $n$ observations, the optimal centralized protocol yields mean-squared error decaying as $\\order(1/[n m])$. We develop and analyze the performance of various decentralized protocols in comparison to this centralized gold-standard. First, we describe a decentralized protocol based on $k = \\log(\\nodenum)$ bits of feedback that is strongly consistent, and achieves the same asymptotic MSE as the centralized optimum. Second, we describe and analyze a decentralized protocol based on only a single bit ($k=1$) of feedback. For step sizes independent of $m$, it achieves an asymptotic MSE of order $\\order[1/(n \\sqrt{m})]$, whereas for step sizes decaying as $1/\\sqrt{m}$, it achieves the same $\\order(1/[n m])$ decay in MSE as the centralized optimum. Our theoretical results are complemented by simulations, illustrating the tradeoffs between these different protocols.", ["Statistical inference", "Inference", "Quantile", "Feedback", "Gold", "Mean squared error", "Architecture", "Statistics", "Bit"]], ["Position Coding", "A position coding pattern is an array of symbols in which subarrays of a certain fixed size appear at most once. So, each subarray uniquely identifies a location in the larger array, which means there is a bijection of some sort from this set of subarrays to a set of coordinates. The key to Fly Pentop Computer paper and other examples of position codes is a method to read the subarray and then convert it to coordinates. Position coding makes use of ideas from discrete mathematics and number theory. In this paper, we will describe the underlying mathematics of two position codes, one being the Anoto code that is the basis of \"Fly paper\". Then, we will present two new codes, one which uses binary wavelets as part of the bijection.", ["Bijection", "Number theory", "Discrete mathematics", "Flypaper", "Mathematics", "Wavelet", "Anoto"]], ["Inferring the Composition of a Trader Population in a Financial Market", "We discuss a method for predicting financial movements and finding pockets of predictability in the price-series, which is built around inferring the heterogeneity of trading strategies in a multi-agent trader population. This work explores extensions to our previous framework (arXiv:physics/0506134). Here we allow for more intelligent agents possessing a richer strategy set, and we no longer constrain the estimate for the heterogeneity of the agents to a probability space. We also introduce a scheme which allows the incorporation of models with a wide variety of agent types, and discuss a mechanism for the removal of bias from relevant parameters.", ["Probability space", "ArXiv", "Multi-agent system", "Predictability", "Intelligent agent", "Physics"]], ["Families of traveling impulses and fronts in some models with cross-diffusion", "An analysis of traveling wave solutions of partial differential equation (PDE) systems with cross-diffusion is presented. The systems under study fall in a general class of the classical Keller-Segel models to describe chemotaxis. The analysis is conducted using the theory of the phase plane analysis of the corresponding wave systems without a priory restrictions on the boundary conditions of the initial PDE. Special attention is paid to families of traveling wave solutions. Conditions for existence of front-impulse, impulse-front, and front-front traveling wave solutions are formulated. In particular, the simplest mathematical model is presented that has an impulse-impulse solution; we also show that a non-isolated singular point in the ordinary differential equation (ODE) wave system implies existence of free-boundary fronts. The results can be used for construction and analysis of different mathematical models describing systems with chemotaxis.", ["Ordinary differential equation", "Partial differential equation", "Differential equation", "Equation", "Boundary value problem", "Classical mechanics", "Mathematical model", "Chemotaxis", "Wave", "Mathematics"]], ["Epistemic Analysis of Strategic Games with Arbitrary Strategy Sets", "We provide here an epistemic analysis of arbitrary strategic games based on the possibility correspondences. Such an analysis calls for the use of transfinite iterations of the corresponding operators. Our approach is based on Tarski's Fixpoint Theorem and applies both to the notions of rationalizability and the iterated elimination of strictly dominated strategies.", ["Strategic dominance", "Alfred Tarski", "Epistemology"]], ["Moving Vertices to Make Drawings Plane", "A straight-line drawing $\\delta$ of a planar graph $G$ need not be plane, but can be made so by moving some of the vertices. Let shift$(G,\\delta)$ denote the minimum number of vertices that need to be moved to turn $\\delta$ into a plane drawing of $G$. We show that shift$(G,\\delta)$ is NP-hard to compute and to approximate, and we give explicit bounds on shift$(G,\\delta)$ when $G$ is a tree or a general planar graph. Our hardness results extend to 1BendPointSetEmbeddability, a well-known graph-drawing problem.", ["Planar graph", "NP-hard", "Graph (mathematics)", "Vertex (geometry)", "Hardness of approximation"]], ["Probabilistic Anonymity and Admissible Schedulers", "When studying safety properties of (formal) protocol models, it is customary to view the scheduler as an adversary: an entity trying to falsify the safety property. We show that in the context of security protocols, and in particular of anonymizing protocols, this gives the adversary too much power; for instance, the contents of encrypted messages and internal computations by the parties should be considered invisible to the adversary. We restrict the class of schedulers to a class of admissible schedulers which better model adversarial behaviour. These admissible schedulers base their decision solely on the past behaviour of the system that is visible to the adversary. Using this, we propose a definition of anonymity: for all admissible schedulers the identity of the users and the observations of the adversary are independent stochastic variables. We also develop a proof technique for typical cases that can be used to proof anonymity: a system is anonymous if it is possible to `exchange' the behaviour of two users without the adversary `noticing'.", ["Encryption", "Anonymity", "Falsifiability", "Stochastic", "Model checking", "Probability", "Mathematical proof"]], ["Improved Neural Modeling of Real-World Systems Using Genetic Algorithm Based Variable Selection", "Neural network models of real-world systems, such as industrial processes, made from sensor data must often rely on incomplete data. System states may not all be known, sensor data may be biased or noisy, and it is not often known which sensor data may be useful for predictive modelling. Genetic algorithms may be used to help to address this problem by determining the near optimal subset of sensor variables most appropriate to produce good models. This paper describes the use of genetic search to optimize variable selection to determine inputs into the neural network model. We discuss genetic algorithm implementation issues including data representation types and genetic operators such as crossover and mutation. We present the use of this technique for neural network modelling of a typical industrial application, a liquid fed ceramic melter, and detail the results of the genetic search to optimize the neural network model for this application.", ["Genetic algorithm", "Neural network", "Predictive modelling", "Subset", "Mutation", "Genetics", "Conceptual model", "Algorithm", "Ceramic", "Liquid", "Scientific modelling"]], ["Design, Implementation, and Cooperative Coevolution of an Autonomous/ Teleoperated Control System for a Serpentine Robotic Manipulator", "Design, implementation, and machine learning issues associated with developing a control system for a serpentine robotic manipulator are explored. The controller developed provides autonomous control of the serpentine robotic manipulatorduring operation of the manipulator within an enclosed environment such as an underground storage tank. The controller algorithms make use of both low-level joint angle control employing force/position feedback constraints, and high-level coordinated control of end-effector positioning. This approach has resulted in both high-level full robotic control and low-level telerobotic control modes, and provides a high level of dexterity for the operator.", ["Underground storage tank", "Machine learning", "Control system", "Telerobotics", "Robotics", "Feedback", "Robot end effector", "Tank"]], ["Small Worlds: Strong Clustering in Wireless Networks", "Small-worlds represent efficient communication networks that obey two distinguishing characteristics: a high clustering coefficient together with a small characteristic path length. This paper focuses on an interesting paradox, that removing links in a network can increase the overall clustering coefficient. Reckful Roaming, as introduced in this paper, is a 2-localized algorithm that takes advantage of this paradox in order to selectively remove superfluous links, this way optimizing the clustering coefficient while still retaining a sufficiently small characteristic path length.", ["Clustering coefficient", "Communication", "Algorithm", "Computer network", "Roaming", "Wireless network", "Telecommunications network", "Small Worlds (Torchwood)"]], ["Applying Test-Paradigms in a Generic Tutoring System Concept for Web-based Learning", "Realizing test scenarios through a tutoring system involve questions about architecture and didactic methods in such a system. Observing the fact that traditional tutoring systems normally are domain-static, this paper shows investigations for a generic domain-independent tutoring system for utilizing test scenarios in computer-based and web-based environments. Furthermore, test paradigms are analyzed and it is presented an approach for realizing functionality for applying test paradigms in the presented generic tutoring system architecture by an XML-specified language.", ["Systems architecture", "XML", "Web application", "Architecture", "Paradigm", "Tutor"]], ["WACA: A Hierarchical Weighted Clustering Algorithm optimized for Mobile Hybrid Networks", "Clustering techniques create hierarchal network structures, called clusters, on an otherwise flat network. In a dynamic environment-in terms of node mobility as well as in terms of steadily changing device parameters-the clusterhead election process has to be re-invoked according to a suitable update policy. Cluster re-organization causes additional message exchanges and computational complexity and it execution has to be optimized. Our investigations focus on the problem of minimizing clusterhead re-elections by considering stability criteria. These criteria are based on topological characteristics as well as on device parameters. This paper presents a weighted clustering algorithm optimized to avoid needless clusterhead re-elections for stable clusters in mobile ad-hoc networks. The proposed localized algorithm deals with mobility, but does not require geographical, speed or distances information.", ["WACA Ground", "Topology", "Computational complexity theory", "Ad hoc", "Mobile ad hoc network", "Cluster analysis", "Algorithm"]], ["Sublinear Algorithms for Approximating String Compressibility", "We raise the question of approximating the compressibility of a string with respect to a fixed compression scheme, in sublinear time. We study this question in detail for two popular lossless compression schemes: run-length encoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for approximating compressibility with respect to both schemes. We also give several lower bounds that show that our algorithms for both schemes cannot be improved significantly. Our investigation of LZ yields results whose interest goes beyond the initial questions we set out to study. In particular, we prove combinatorial structural lemmas that relate the compressibility of a string with respect to Lempel-Ziv to the number of distinct short substrings contained in it. In addition, we show that approximating the compressibility with respect to LZ is related to approximating the support size of a distribution.", ["Run-length encoding", "Lossless data compression", "Data compression", "LZ77 and LZ78", "Algorithm", "Sublinear function", "Combinatorics", "Abraham Lempel", "Compressibility"]], ["On Anomalies in Annotation Systems", "Today's computer-based annotation systems implement a wide range of functionalities that often go beyond those available in traditional paper-and-pencil annotations. Conceptually, annotation systems are based on thoroughly investigated psycho-sociological and pedagogical learning theories. They offer a huge diversity of annotation types that can be placed in textual as well as in multimedia format. Additionally, annotations can be published or shared with a group of interested parties via well-organized repositories. Although highly sophisticated annotation systems exist both conceptually as well as technologically, we still observe that their acceptance is somewhat limited. In this paper, we argue that nowadays annotation systems suffer from several fundamental problems that are inherent in the traditional paper-and-pencil annotation paradigm. As a solution, we propose to shift the annotation paradigm for the implementation of annotation system.", ["Annotation", "Social psychology", "Sociology", "Paradigm", "Computer", "Learning theory (education)", "Pedagogy", "Pencil", "Multimedia"]], ["Inquiring the Potential of Evoking Small-World Properties for Self-Organizing Communication Networks", "Mobile multi-hop ad hoc networks allow establishing local groups of communicating devices in a self-organizing way. However, in a global setting such networks fail to work properly due to network partitioning. Providing that devices are capable of communicating both locally-e.g. using Wi-Fi or Bluetooth-and additionally also with arbitrary remote devices-e.g. using GSM/UMTS links-the objective is to find efficient ways of inter-linking multiple network partitions. Tackling this problem of topology control, we focus on the class of small-world networks that obey two distinguishing characteristics: they have a strong local clustering while still retaining a small average distance between two nodes. This paper reports on results gained investigating the question if small-world properties are indicative for an efficient link management in multiple multi-hop ad hoc network partitions.", ["Bluetooth", "Topology", "Universal Mobile Telecommunications System", "Ad hoc", "Wi-Fi", "Self-organization", "Small-world network", "Communication", "GSM"]], ["Asynchronous games: innocence without alternation", "The notion of innocent strategy was introduced by Hyland and Ong in order to capture the interactive behaviour of lambda-terms and PCF programs. An innocent strategy is defined as an alternating strategy with partial memory, in which the strategy plays according to its view. Extending the definition to non-alternating strategies is problematic, because the traditional definition of views is based on the hypothesis that Opponent and Proponent alternate during the interaction. Here, we take advantage of the diagrammatic reformulation of alternating innocence in asynchronous games, in order to provide a tentative definition of innocence in non-alternating games. The task is interesting, and far from easy. It requires the combination of true concurrency and game semantics in a clean and organic way, clarifying the relationship between asynchronous games and concurrent games in the sense of Abramsky and Melli\\`es. It also requires an interactive reformulation of the usual acyclicity criterion of linear logic, as well as a directed variant, as a scheduling criterion.", ["Linear logic", "Game semantics", "Concurrency (computer science)", "Semantics", "Lambda calculus", "Logic"]], ["Cointegration of the Daily Electric Power System Load and the Weather", "The paper makes a thermal predictive analysis of the electric power system security for a day ahead. This predictive analysis is set as a thermal computation of the expected security. This computation is obtained by cointegrating the daily electric power systen load and the weather, by finding the daily electric power system thermodynamics and by introducing tests for this thermodynamics. The predictive analysis made shows the electricity consumers' wisdom.", ["Electricity", "Electric power", "Cointegration", "Thermodynamics", "Computation"]], ["Redesigning Computer-based Learning Environments: Evaluation as Communication", "In the field of evaluation research, computer scientists live constantly upon dilemmas and conflicting theories. As evaluation is differently perceived and modeled among educational areas, it is not difficult to become trapped in dilemmas, which reflects an epistemological weakness. Additionally, designing and developing a computer-based learning scenario is not an easy task. Advancing further, with end-users probing the system in realistic settings, is even harder. Computer science research in evaluation faces an immense challenge, having to cope with contributions from several conflicting and controversial research fields. We believe that deep changes must be made in our field if we are to advance beyond the CBT (computer-based training) learning model and to build an adequate epistemology for this challenge. The first task is to relocate our field by building upon recent results from philosophy, psychology, social sciences, and engineering. In this article we locate evaluation in respect to communication studies. Evaluation presupposes a definition of goals to be reached, and we suggest that it is, by many means, a silent communication between teacher and student, peers, and institutional entities. If we accept that evaluation can be viewed as set of invisible rules known by nobody, but somehow understood by everybody, we should add anthropological inquiries to our research toolkit. The paper is organized around some elements of the social communication and how they convey new insights to evaluation research for computer and related scientists. We found some technical limitations and offer discussions on how we relate to technology at same time we establish expectancies and perceive others work.", ["Computer science", "Science", "Psychology", "Epistemology", "Communication", "E-learning", "Technology", "Engineering", "Communication studies", "Social sciences", "Social communication", "Teacher", "Evaluation", "Scientist"]], ["A Communication Model for Adaptive Service Provisioning in Hybrid Wireless Networks", "Mobile entities with wireless links are able to form a mobile ad-hoc network. Such an infrastructureless network does not have to be administrated. However, self-organizing principles have to be applied to deal with upcoming problems, e.g. information dissemination. These kinds of problems are not easy to tackle, requiring complex algorithms. Moreover, the usefulness of pure ad-hoc networks is arguably limited. Hence, enthusiasm for mobile ad-hoc networks, which could eliminate the need for any fixed infrastructure, has been damped. The goal is to overcome the limitations of pure ad-hoc networks by augmenting them with instant Internet access, e.g. via integration of UMTS respectively GSM links. However, this raises multiple questions at the technical as well as the organizational level. Motivated by characteristics of small-world networks that describe an efficient network even without central or organized design, this paper proposes to combine mobile ad-hoc networks and infrastructured networks to form hybrid wireless networks. One main objective is to investigate how this approach can reduce the costs of a permanent backbone link and providing in the same way the benefits of useful information from Internet connectivity or service providers. For the purpose of bridging between the different types of networks, an adequate middleware service is the focus of our investigation. This paper shows our first steps forward to this middleware by introducing the Injection Communication paradigm as principal concept.", ["Mobile ad hoc network", "Universal Mobile Telecommunications System", "GSM", "Wireless", "Ad hoc", "Wireless network", "Algorithm", "Provisioning", "Mobile phone", "Infrastructure", "Middleware", "Internet access", "Communication", "Wireless ad hoc network", "Computer network", "Internet"]], ["Automatically Restructuring Practice Guidelines using the GEM DTD", "This paper describes a system capable of semi-automatically filling an XML template from free texts in the clinical domain (practice guidelines). The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and actions/recommendations). Therefore, there is a need to compute the exact scope of conditions over text sequences expressing the required actions. We present a system developed for this task. We show that it yields good performance when applied to the analysis of French practice guidelines.", ["XML"]], ["Multimedia Content Distribution in Hybrid Wireless Networks using Weighted Clustering", "Fixed infrastructured networks naturally support centralized approaches for group management and information provisioning. Contrary to infrastructured networks, in multi-hop ad-hoc networks each node acts as a router as well as sender and receiver. Some applications, however, requires hierarchical arrangements that-for practical reasons-has to be done locally and self-organized. An additional challenge is to deal with mobility that causes permanent network partitioning and re-organizations. Technically, these problems can be tackled by providing additional uplinks to a backbone network, which can be used to access resources in the Internet as well as to inter-link multiple ad-hoc network partitions, creating a hybrid wireless network. In this paper, we present a prototypically implemented hybrid wireless network system optimized for multimedia content distribution. To efficiently manage the ad-hoc communicating devices a weighted clustering algorithm is introduced. The proposed localized algorithm deals with mobility, but does not require geographical information or distances.", ["Router", "Ad hoc", "Backbone network", "Multimedia", "Hierarchy", "Computer network", "Mesh networking", "Wireless network", "Wireless ad hoc network", "Algorithm", "Content delivery", "Hybrid (biology)"]], ["Localized Support for Injection Point Election in Hybrid Networks", "Ad-hoc networks, a promising trend in wireless technology, fail to work properly in a global setting. In most cases, self-organization and cost-free local communication cannot compensate the need for being connected, gathering urgent information just-in-time. Equipping mobile devices additionally with GSM or UMTS adapters in order to communicate with arbitrary remote devices or even a fixed network infrastructure provides an opportunity. Devices that operate as intermediate nodes between the ad-hoc network and a reliable backbone network are potential injection points. They allow disseminating received information within the local neighborhood. The effectiveness of different devices to serve as injection point differs substantially. For practical reasons the determination of injection points should be done locally, within the ad-hoc network partitions. We analyze different localized algorithms using at most 2-hop neighboring information. Results show that devices selected this way spread information more efficiently through the ad-hoc network. Our results can also be applied in order to support the election process for clusterheads in the field of clustering mechanisms.", ["Universal Mobile Telecommunications System", "GSM", "Self-organization", "Communication", "Mobile device", "Backbone network", "Infrastructure", "Ad hoc", "Computer network", "Wireless ad hoc network"]], ["A taxonomic Approach to Topology Control in Ad-hoc and Wireless Networks", "Topology Control (TC) aims at tuning the topology of highly dynamic networks to provide better control over network resources and to increase the efficiency of communication. Recently, many TC protocols have been proposed. The protocols are designed for preserving connectivity, minimizing energy consumption, maximizing the overall network coverage or network capacity. Each TC protocol makes different assumptions about the network topology, environment detection resources, and control capacities. This circumstance makes it extremely difficult to comprehend the role and purpose of each protocol. To tackle this situation, a taxonomy for TC protocols is presented throughout this paper. Additionally, some TC protocols are classified based upon this taxonomy.", ["Communication", "Taxonomy", "Ad hoc", "Energy", "Topology", "Network topology"]], ["The multiple viewpoints as approach to information retrieval within collaborative development context", "Nowadays, to achieve competitive advantage, the industrial companies are considering that success is sustained to great product development. That is to manage the product throughout its entire lifecycle. Achieving this goal requires a tight collaboration between actors from a wide variety of domains, using different software tools producing various product data types and formats. The actors' collaboration is mainly based on the exchange /share product information. The representation of the actors' viewpoints is the underlying requirement of the collaborative product development. The multiple viewpoints approach was designed to provide an organizational framework following the actors' perspectives in the collaboration, and their relationships. The approach acknowledges the inevitability of multiple integration of product information as different views, promotes gathering of actors' interest, and encourages retrieved adequate information while providing support for integration through PLM and/or SCM collaboration. In this paper, a multiple viewpoints representation is proposed. The product, process, organization information models are discussed. A series of issues referring to the viewpoints representation are discussed in detail. Based on XML standard, taking electrical connector as an example, an application case of part of product information modeling is stated.", ["Electrical connector", "XML", "Information retrieval", "Competitive advantage", "New product development"]], ["Vector Precoding for Wireless MIMO Systems: A Replica Analysis", "We apply the replica method to analyze vector precoding, a method to reduce transmit power in antenna array communications. The analysis applies to a very general class of channel matrices. The statistics of the channel matrix enter the transmitted energy per symbol via its R-transform. We find that vector precoding performs much better for complex than for real alphabets. As a byproduct, we find a nonlinear precoding method with polynomial complexity that outperforms NP-hard Tomlinson-Harashima precoding for binary modulation on complex channels if the number of transmit antennas is slightly larger than twice the number of receive antennas.", ["NP-hard", "Precoding", "Nonlinear system", "Polynomial time", "Statistics", "Modulation", "Energy", "MIMO", "Polynomial", "Antenna (radio)"]], ["Collaborative product and process model: Multiple Viewpoints approach", "The design and development of complex products invariably involves many actors who have different points of view on the problem they are addressing, the product being developed, and the process by which it is being developed. The actors' viewpoints approach was designed to provide an organisational framework in which these different perspectives or points of views, and their relationships, could be explicitly gathered and formatted (by actor activity's focus). The approach acknowledges the inevitability of multiple interpretation of product information as different views, promotes gathering of actors' interests, and encourages retrieved adequate information while providing support for integration through PLM and/or SCM collaboration. In this paper, we present our multiple viewpoints approach, and we illustrate it by an industrial example on cyclone vessel product.", ["Process modeling", "New product development"]], ["Developing a Collaborative and Autonomous Training and Learning Environment for Hybrid Wireless Networks", "With larger memory capacities and the ability to link into wireless networks, more and more students uses palmtop and handheld computers for learning activities. However, existing software for Web-based learning is not well-suited for such mobile devices, both due to constrained user interfaces as well as communication effort required. A new generation of applications for the learning domain that is explicitly designed to work on these kinds of small mobile devices has to be developed. For this purpose, we introduce CARLA, a cooperative learning system that is designed to act in hybrid wireless networks. As a cooperative environment, CARLA aims at disseminating teaching material, notes, and even components of itself through both fixed and mobile networks to interested nodes. Due to the mobility of nodes, CARLA deals with upcoming problems such as network partitions and synchronization of teaching material, resource dependencies, and time constraints.", ["Communication", "Mobile device", "User interface", "Computer software", "Wireless network", "Wireless", "World Wide Web", "Handheld PC", "Synchronization", "Computer network", "Mobile phone", "Cooperative", "Mobile computing", "Computer"]], ["Temporal Reasoning without Transitive Tables", "Representing and reasoning about qualitative temporal information is an essential part of many artificial intelligence tasks. Lots of models have been proposed in the litterature for representing such temporal information. All derive from a point-based or an interval-based framework. One fundamental reasoning task that arises in applications of these frameworks is given by the following scheme: given possibly indefinite and incomplete knowledge of the binary relationships between some temporal objects, find the consistent scenarii between all these objects. All these models require transitive tables -- or similarly inference rules-- for solving such tasks. We have defined an alternative model, S-languages - to represent qualitative temporal information, based on the only two relations of \\emph{precedence} and \\emph{simultaneity}. In this paper, we show how this model enables to avoid transitive tables or inference rules to handle this kind of problem.", ["Artificial intelligence", "Rule of inference", "Reason", "Literature", "Time"]], ["Constructing a maximum utility slate of on-line advertisements", "We present an algorithm for constructing an optimal slate of sponsored search advertisements which respects the ordering that is the outcome of a generalized second price auction, but which must also accommodate complicating factors such as overall budget constraints. The algorithm is easily fast enough to use on the fly for typical problem sizes, or as a subroutine in an overall optimization.", ["Auction", "Mathematical optimization", "Subroutine", "Advertising", "Vickrey auction", "Algorithm"]], ["Opportunistic Network Coding for Video Streaming over Wireless", "In this paper, we study video streaming over wireless networks with network coding capabilities. We build upon recent work, which demonstrated that network coding can increase throughput over a broadcast medium, by mixing packets from different flows into a single packet, thus increasing the information content per transmission. Our key insight is that, when the transmitted flows are video streams, network codes should be selected so as to maximize not only the network throughput but also the video quality. We propose video-aware opportunistic network coding schemes that take into account both (i) the decodability of network codes by several receivers and (ii) the importance and deadlines of video packets. Simulation results show that our schemes significantly improve both video quality and throughput.", ["Wireless", "Transmission (telecommunications)", "Network packet", "Receiver (radio)", "Wireless network", "Simulation", "Forward error correction", "Computer network", "Broadcasting", "Throughput"]], ["Duality and Stability Regions of Multi-rate Broadcast and Multiple Access Networks", "We characterize stability regions of two-user fading Gaussian multiple access (MAC) and broadcast (BC) networks with centralized scheduling. The data to be transmitted to the users is encoded into codewords of fixed length. The rates of the codewords used are restricted to a fixed set of finite cardinality. With successive decoding and interference cancellation at the receivers, we find the set of arrival rates that can be stabilized over the MAC and BC networks. In MAC and BC networks with average power constraints, we observe that the duality property that relates the MAC and BC information theoretic capacity regions extend to their stability regions as well. In MAC and BC networks with peak power constraints, the union of stability regions of dual MAC networks is found to be strictly contained in the BC stability region.", ["Code word", "Cardinality", "Data", "Information theory"]], ["Analyzing Design Process and Experiments on the AnITA Generic Tutoring System", "In the field of tutoring systems, investigations have shown that there are many tutoring systems specific to a specific domain that, because of their static architecture, cannot be adapted to other domains. As consequence, often neither methods nor knowledge can be reused. In addition, the knowledge engineer must have programming skills in order to enhance and evaluate the system. One particular challenge is to tackle these problems with the development of a generic tutoring system. AnITA, as a stand-alone application, has been developed and implemented particularly for this purpose. However, in the testing phase, we discovered that this architecture did not fully match the user's intuitive understanding of the use of a learning tool. Therefore, AnITA has been redesigned to exclusively work as a client/server application and renamed to AnITA2. This paper discusses the evolvements made on the AnITA tutoring system, the goal of which is to use generic principles for system re-use in any domain. Two experiments were conducted, and the results are presented in this paper.", ["Knowledge engineering", "Server (computing)"]], ["A Proof of a Recursion for Bessel Moments", "We provide a proof of a conjecture in (Bailey, Borwein, Borwein, Crandall 2007) on the existence and form of linear recursions for moments of powers of the Bessel function $K_0$.", ["Bessel function", "Mathematical proof", "Conjecture"]], ["Evolutionary Mesh Numbering: Preliminary Results", "Mesh numbering is a critical issue in Finite Element Methods, as the computational cost of one analysis is highly dependent on the order of the nodes of the mesh. This paper presents some preliminary investigations on the problem of mesh numbering using Evolutionary Algorithms. Three conclusions can be drawn from these experiments. First, the results of the up-to-date method used in all FEM softwares (Gibb's method) can be consistently improved; second, none of the crossover operators tried so far (either general or problem specific) proved useful; third, though the general tendency in Evolutionary Computation seems to be the hybridization with other methods (deterministic or heuristic), none of the presented attempt did encounter any success yet. The good news, however, is that this algorithm allows an improvement over the standard heuristic method between 12% and 20% for both the 1545 and 5453-nodes meshes used as test-bed. Finally, some strange interaction between the selection scheme and the use of problem specific mutation operator was observed, which appeals for further investigation.", ["Hybrid (biology)", "Algorithm", "Evolutionary computation", "Mutation", "Evolution", "Finite element method"]], ["A Generic Model of Contracts for Embedded Systems", "We present the mathematical foundations of the contract-based model developed in the framework of the SPEEDS project. SPEEDS aims at developing methods and tools to support \"speculative design\", a design methodology in which distributed designers develop different aspects of the overall system, in a concurrent but controlled way. Our generic mathematical model of contract supports this style of development. This is achieved by focusing on behaviors, by supporting the notion of \"rich component\" where diverse (functional and non-functional) aspects of the system can be considered and combined, by representing rich components via their set of associated contracts, and by formalizing the whole process of component composition.", ["Embedded system", "Mathematical model", "Mathematics", "Methodology", "Design", "Employment"]], ["VPSPACE and a transfer theorem over the complex field", "We extend the transfer theorem of [KP2007] to the complex field. That is, we investigate the links between the class VPSPACE of families of polynomials and the Blum-Shub-Smale model of computation over C. Roughly speaking, a family of polynomials is in VPSPACE if its coefficients can be computed in polynomial space. Our main result is that if (uniform, constant-free) VPSPACE families can be evaluated efficiently then the class PAR of decision problems that can be solved in parallel polynomial time over the complex field collapses to P. As a result, one must first be able to show that there are VPSPACE families which are hard to evaluate in order to separate P from NP over C, or even from PAR.", ["Polynomial time", "PSPACE", "Computation", "Polynomial", "Model of computation", "Order of St. Patrick", "Theorem"]], ["Optimal Choice of Threshold in Two Level Processor Sharing", "We analyze the Two Level Processor Sharing (TLPS) scheduling discipline with the hyper-exponential job size distribution and with the Poisson arrival process. TLPS is a convenient model to study the benefit of the file size based differentiation in TCP/IP networks. In the case of the hyper-exponential job size distribution with two phases, we find a closed form analytic expression for the expected sojourn time and an approximation for the optimal value of the threshold that minimizes the expected sojourn time. In the case of the hyper-exponential job size distribution with more than two phases, we derive a tight upper bound for the expected sojourn time conditioned on the job size. We show that when the variance of the job size distribution increases, the gain in system performance increases and the sensitivity to the choice of the threshold near its optimal value decreases.", ["Internet Protocol Suite", "Scheduling (computing)", "Poisson process", "Transmission Control Protocol", "Variance", "Upper and lower bounds", "Analytical expression", "Tetration"]], ["Detection of Gauss-Markov Random Fields with Nearest-Neighbor Dependency", "The problem of hypothesis testing against independence for a Gauss-Markov random field (GMRF) is analyzed. Assuming an acyclic dependency graph, an expression for the log-likelihood ratio of detection is derived. Assuming random placement of nodes over a large region according to the Poisson or uniform distribution and nearest-neighbor dependency graph, the error exponent of the Neyman-Pearson detector is derived using large-deviations theory. The error exponent is expressed as a dependency-graph functional and the limit is evaluated through a special law of large numbers for stabilizing graph functionals. The exponent is analyzed for different values of the variance ratio and correlation. It is found that a more correlated GMRF has a higher exponent at low values of the variance ratio whereas the situation is reversed at high values of the variance ratio.", ["Markov random field", "Law of large numbers", "Likelihood-ratio test", "Correlation and dependence", "Uniform distribution (continuous)", "Carl Friedrich Gauss", "Poisson distribution", "Statistical hypothesis testing", "Likelihood function", "Variance", "Randomness", "Errors and residuals in statistics", "Jerzy Neyman", "Random field", "Hypothesis", "Probability distribution", "Type I and type II errors", "Dependency graph", "Exponentiation", "Functional (mathematics)", "Markov process"]], ["Non-Cooperative Scheduling of Multiple Bag-of-Task Applications", "Multiple applications that execute concurrently on heterogeneous platforms compete for CPU and network resources. In this paper we analyze the behavior of $K$ non-cooperative schedulers using the optimal strategy that maximize their efficiency while fairness is ensured at a system level ignoring applications characteristics. We limit our study to simple single-level master-worker platforms and to the case where each scheduler is in charge of a single application consisting of a large number of independent tasks. The tasks of a given application all have the same computation and communication requirements, but these requirements can vary from one application to another. In this context, we assume that each scheduler aims at maximizing its throughput. We give closed-form formula of the equilibrium reached by such a system and study its performance. We characterize the situations where this Nash equilibrium is optimal (in the Pareto sense) and show that even though no catastrophic situation (Braess-like paradox) can occur, such an equilibrium can be arbitrarily bad for any classical performance measure.", ["Nash equilibrium", "Communication", "Homogeneity and heterogeneity", "Paradox", "Central processing unit", "Closed-form expression", "Scheduling (computing)", "Computation", "Throughput"]], ["Relative Strength of Strategy Elimination Procedures", "We compare here the relative strength of four widely used procedures on finite strategic games: iterated elimination of weakly/strictly dominated strategies by a pure/mixed strategy. A complication is that none of these procedures is based on a monotonic operator. To deal with this problem we use 'global' versions of these operators.", ["Strategic dominance", "Monotonic function", "Mixed strategy"]], ["On the growth of components with non fixed excesses", "Denote by an $l$-component a connected graph with $l$ edges more than vertices. We prove that the expected number of creations of $(l+1)$-component, by means of adding a new edge to an $l$-component in a randomly growing graph with $n$ vertices, tends to 1 as $l,n$ tends to $\\infty$ but with $l = o(n^{1/4})$. We also show, under the same conditions on $l$ and $n$, that the expected number of vertices that ever belong to an $l$-component is $\\sim (12l)^{1/3} n^{2/3}$.", ["Graph (mathematics)"]], ["Another Proof of Wright's Inequalities", "We present a short way of proving the inequalities obtained by Wright in [Journal of Graph Theory, 4: 393 - 407 (1980)] concerning the number of connected graphs with $\\ell$ edges more than vertices.", ["Inequality (mathematics)"]], ["A Methodology for Efficient Space-Time Adapter Design Space Exploration: A Case Study of an Ultra Wide Band Interleaver", "This paper presents a solution to efficiently explore the design space of communication adapters. In most digital signal processing (DSP) applications, the overall architecture of the system is significantly affected by communication architecture, so the designers need specifically optimized adapters. By explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs a C description of Input/Output data scheduling, and user requirements (throughput, latency, parallelism...), and formalizes communication constraints through a Resource Constraints Graph (RCG). The RCG properties enable an efficient architecture space exploration in order to synthesize a STAR component. The proposed approach has been tested to design an industrial data mixing block example: an Ultra-Wideband interleaver.", ["Spacetime", "Digital signal processing", "Ultra-wideband", "Graph theory", "Digital signal", "Communication", "Interleaving", "Wideband", "Signal processing", "Input/output", "Data", "Throughput", "Architecture"]], ["Information Criteria and Arithmetic Codings : An Illustration on Raw Images", "In this paper we give a short theoretical description of the general predictive adaptive arithmetic coding technique. The links between this technique and the works of J. Rissanen in the 80's, in particular the BIC information criterion used in parametrical model selection problems, are established. We also design lossless and lossy coding techniques of images. The lossless technique uses a mix between fixed-length coding and arithmetic coding and provides better compression results than those separate methods. That technique is also seen to have an interesting application in the domain of statistics since it gives a data-driven procedure for the non-parametrical histogram selection problem. The lossy technique uses only predictive adaptive arithmetic codes and shows how a good choice of the order of prediction might lead to better results in terms of compression. We illustrate those coding techniques on a raw grayscale image.", ["Statistics", "Histogram", "Selection algorithm", "Illustration", "Grayscale", "Arithmetic coding", "Arithmetic", "Model selection", "Lossy compression"]], ["Modeling and analysis using hybrid Petri nets", "This paper is devoted to the use of hybrid Petri nets (PNs) for modeling and control of hybrid dynamic systems (HDS). Modeling, analysis and control of HDS attract ever more of researchers' attention and several works have been devoted to these topics. We consider in this paper the extensions of the PN formalism (initially conceived for modeling and analysis of discrete event systems) in the direction of hybrid modeling. We present, first, the continuous PN models. These models are obtained from discrete PNs by the fluidification of the markings. They constitute the first steps in the extension of PNs toward hybrid modeling. Then, we present two hybrid PN models, which differ in the class of HDS they can deal with. The first one is used for deterministic HDS modeling, whereas the second one can deal with HDS with nondeterministic behavior. Keywords: Hybrid dynamic systems; D-elementary hybrid Petri nets; Hybrid automata; Controller synthesis", ["Discrete event simulation", "Automaton", "Petri net", "Dynamical system"]], ["MacWilliams Identity for Codes with the Rank Metric", "The MacWilliams identity, which relates the weight distribution of a code to the weight distribution of its dual code, is useful in determining the weight distribution of codes. In this paper, we derive the MacWilliams identity for linear codes with the rank metric, and our identity has a different form than that by Delsarte. Using our MacWilliams identity, we also derive related identities for rank metric codes. These identities parallel the binomial and power moment identities derived for codes with the Hamming metric.", ["Hamming distance"]], ["FreeBSD Mandatory Access Control Usage for Implementing Enterprise Security Policies", "FreeBSD was one of the first widely deployed free operating systems to provide mandatory access control. It supports a number of classic MAC models. This tutorial paper addresses exploiting this implementation to enforce typical enterprise security policies of varying complexities.", ["Mandatory access control", "Security policy", "FreeBSD"]], ["Le travail collaboratif dans le cadre d'un projet architectural", "The analysis of the practices and the tendencies of the users at the time of the search for information on Internet makes it possible to highlight several points. The search for information becomes powerful after knowledge of the typology of the various systems of research. This typology supports the adoption of a methodology of research which one can characterize by pull systems, intelligent agents, etc. In addition, the importance of the structure of the electronic document, correctly elaborated in advance, will support a higher relevance ratio to find information. In our article, the problems turn around the study of the behavior of the users in situation of search for information, as well as the constitution of a pole of documentary resources within a framework of an architectural project. It is noted that the evolution of the documentary resources is related to information technologies.", ["Evolution", "Electronic document", "Documentary film", "Internet", "Intelligent agent", "Personality type"]], ["How to measure efficiency?", "In the context of applied game theory in networking environments, a number of concepts have been proposed to measure both efficiency and optimality of resource allocations, the most famous certainly being the price of anarchy and the Jain index. Yet, very few have tried to question these measures and compare them one to another, in a general framework, which is the aim of the present article.", ["Game theory", "Anarchy"]], ["FIPA-based Interoperable Agent Mobility Proposal", "This paper presents a proposal for a flexible agent mobility architecture based on IEEE-FIPA standards and intended to be one of them. This proposal is a first step towards interoperable mobility mechanisms, which are needed for future agent migration between different kinds of platforms. Our proposal is presented as a flexible and robust architecture that has been successfully implemented in the JADE and AgentScape platforms. It is based on an open set of protocols, allowing new protocols and future improvements to be accommodated in the architecture. With this proposal we demonstrate that a standard architecture for agent mobility capable of supporting several agent platforms can be defined and implemented.", ["Open set", "Interoperability", "FIPA", "Systems architecture", "Institute of Electrical and Electronics Engineers", "JADE (programming language)", "Architecture"]], ["Towards understanding and modelling office daily life", "Measuring and modeling human behavior is a very complex task. In this paper we present our initial thoughts on modeling and automatic recognition of some human activities in an office. We argue that to successfully model human activities, we need to consider both individual behavior and group dynamics. To demonstrate these theoretical approaches, we introduce an experimental system for analyzing everyday activity in our office.", ["Scientific modelling", "Experiment", "Group dynamics", "Theory", "Human behavior"]], ["Information-theoretic security without an honest majority", "We present six multiparty protocols with information-theoretic security that tolerate an arbitrary number of corrupt participants. All protocols assume pairwise authentic private channels and a broadcast channel (in a single case, we require a simultaneous broadcast channel). We give protocols for veto, vote, anonymous bit transmission, collision detection, notification and anonymous message transmission. Not assuming an honest majority, in most cases, a single corrupt participant can make the protocol abort. All protocols achieve functionality never obtained before without the use of either computational assumptions or of an honest majority.", ["Information theory", "Collision detection"]], ["On the Performance Evaluation of Encounter-based Worm Interactions Based on Node Characteristics", "An encounter-based network is a frequently disconnected wireless ad-hoc network requiring nearby neighbors to store and forward data utilizing mobility and encounters over time. Using traditional approaches such as gateways or firewalls for deterring worm propagation in encounter-based networks is inappropriate. Because this type of network is highly dynamic and has no specific boundary, a distributed counter-worm mechanism is needed. We propose models for the worm interaction approach that relies upon automated beneficial worm generation to alleviate problems of worm propagation in such networks. We study and analyze the impact of key mobile node characteristics including node cooperation, immunization, on-off behavior on the worm propagations and interactions. We validate our proposed model using extensive simulations. We also find that, in addition to immunization, cooperation can reduce the level of worm infection. Furthermore, on-off behavior linearly impacts only timing aspect but not the overall infection. Using realistic mobile network measurements, we find that encounters are non-uniform, the trends are consistent with the model but the magnitudes are drastically different. Immunization seems to be the most effective in such scenarios. These findings provide insight that we hope would aid to develop counter-worm protocols in future encounter-based networks.", ["Wireless", "Communications protocol", "Store and forward", "Node (networking)", "Network traffic measurement", "Firewall (computing)", "Wireless ad hoc network", "Cellular network", "Computer network", "Mobile network operator", "Ad hoc", "Gateway (telecommunications)"]], ["Power Allocation for Discrete-Input Delay-Limited Fading Channels", "We consider power allocation algorithms for fixed-rate transmission over Nakagami-m non-ergodic block-fading channels with perfect transmitter and receiver channel state information and discrete input signal constellations, under both short- and long-term power constraints. Optimal power allocation schemes are shown to be direct applications of previous results in the literature. We show that the SNR exponent of the optimal short-term scheme is given by m times the Singleton bound. We also illustrate the significant gains available by employing long-term power constraints. In particular, we analyze the optimal long-term solution, showing that zero outage can be achieved provided that the corresponding short-term SNR exponent with the same system parameters is strictly greater than one. Conversely, if the short-term SNR exponent is smaller than one, we show that zero outage cannot be achieved. In this case, we derive the corresponding long-term SNR exponent as a function of the Singleton bound. Due to the nature of the expressions involved, the complexity of optimal schemes may be prohibitive for system implementation. We therefore propose simple sub-optimal power allocation schemes whose outage probability performance is very close to the minimum outage probability obtained by optimal schemes. We also show the applicability of these techniques to practical systems employing orthogonal frequency division multiplexing.", ["Channel state information", "Orthogonal frequency-division multiplexing", "Multiplexing", "Orthogonality", "Frequency", "Frequency-division multiplexing", "Signal-to-noise ratio", "Signal (electronics)", "Singleton bound", "Exponentiation", "Transmitter", "Algorithm", "Ergodicity", "Channel (communications)", "Probability", "Fading"]], ["Critique of Feinstein's Proof that P is not Equal to NP", "We examine a proof by Craig Alan Feinstein that P is not equal to NP. We present counterexamples to claims made in his paper and expose a flaw in the methodology he uses to make his assertions. The fault in his argument is the incorrect use of reduction. Feinstein makes incorrect assumptions about the complexity of a problem based on the fact that there is a more complex problem that can be used to solve it. His paper introduces the terminology \"imaginary processor\" to describe how it is possible to beat the brute force reduction he offers to solve the Subset-Sum problem. The claims made in the paper would not be validly established even were imaginary processors to exist.", ["Central processing unit", "Subset"]], ["Getting started in probabilistic graphical models", "Probabilistic graphical models (PGMs) have become a popular tool for computational analysis of biological data in a variety of domains. But, what exactly are they and how do they work? How can we use PGMs to discover patterns that are biologically relevant? And to what extent can PGMs help us formulate new hypotheses that are testable at the bench? This note sketches out some answers and illustrates the main ideas behind the statistical approach to biological pattern discovery.", ["Statistics", "Probability", "Graphical model"]], ["Building Portable Thread Schedulers for Hierarchical Multiprocessors: the BubbleSched Framework", "Exploiting full computational power of current more and more hierarchical multiprocessor machines requires a very careful distribution of threads and data among the underlying non-uniform architecture. Unfortunately, most operating systems only provide a poor scheduling API that does not allow applications to transmit valuable scheduling hints to the system. In a previous paper, we showed that using a bubble-based thread scheduler can significantly improve applications' performance in a portable way. However, since multithreaded applications have various scheduling requirements, there is no universal scheduler that could meet all these needs. In this paper, we present a framework that allows scheduling experts to implement and experiment with customized thread schedulers. It provides a powerful API for dynamically distributing bubbles among the machine in a high-level, portable, and efficient way. Several examples show how experts can then develop, debug and tune their own portable bubble schedulers.", ["Thread (computer science)", "Multiprocessing", "Application programming interface", "Hierarchy", "Operating system", "Scheduling (computing)", "Architecture", "Debugging"]], ["An Efficient OpenMP Runtime System for Hierarchical Arch", "Exploiting the full computational power of always deeper hierarchical multiprocessor machines requires a very careful distribution of threads and data among the underlying non-uniform architecture. The emergence of multi-core chips and NUMA machines makes it important to minimize the number of remote memory accesses, to favor cache affinities, and to guarantee fast completion of synchronization steps. By using the BubbleSched platform as a threading backend for the GOMP OpenMP compiler, we are able to easily transpose affinities of thread teams into scheduling hints using abstractions called bubbles. We then propose a scheduling strategy suited to nested OpenMP parallelism. The resulting preliminary performance evaluations show an important improvement of the speedup on a typical NAS OpenMP benchmark application.", ["OpenMP", "Multiprocessing", "Multi-core processor", "Cache", "Parallel computing", "Hierarchy", "Thread (computer science)", "Moore's law", "Compiler"]], ["A Finite Semantics of Simply-Typed Lambda Terms for Infinite Runs of<br> Automata", "Model checking properties are often described by means of finite automata. Any particular such automaton divides the set of infinite trees into finitely many classes, according to which state has an infinite run. Building the full type hierarchy upon this interpretation of the base type gives a finite semantics for simply-typed lambda-trees. A calculus based on this semantics is proven sound and complete. In particular, for regular infinite lambda-trees it is decidable whether a given automaton has a run or not. As regular lambda-trees are precisely recursion schemes, this decidability result holds for arbitrary recursion schemes of arbitrary level, without any syntactical restriction.", ["Calculus", "Finite set", "Model checking", "Recursion", "Finite-state machine", "Semantics", "Lambda calculus", "Syntax", "Hierarchy", "Automaton", "Decidability (logic)", "Infinity"]], ["Efficient Multidimensional Data Redistribution for Resizable Parallel Computations", "Traditional parallel schedulers running on cluster supercomputers support only static scheduling, where the number of processors allocated to an application remains fixed throughout the execution of the job. This results in under-utilization of idle system resources thereby decreasing overall system throughput. In our research, we have developed a prototype framework called ReSHAPE, which supports dynamic resizing of parallel MPI applications executing on distributed memory platforms. The resizing library in ReSHAPE includes support for releasing and acquiring processors and efficiently redistributing application state to a new set of processors. In this paper, we derive an algorithm for redistributing two-dimensional block-cyclic arrays from $P$ to $Q$ processors, organized as 2-D processor grids. The algorithm ensures a contention-free communication schedule for data redistribution if $P_r \\leq Q_r$ and $P_c \\leq Q_c$. In other cases, the algorithm implements circular row and column shifts on the communication schedule to minimize node contention.", ["Supercomputer", "Parallel computing", "Throughput", "Communication", "Algorithm", "Prototype", "Central processing unit", "Message Passing Interface", "Distributed memory"]], ["Stability of boundary measures", "We introduce the boundary measure at scale r of a compact subset of the n-dimensional Euclidean space. We show how it can be computed for point clouds and suggest these measures can be used for feature detection. The main contribution of this work is the proof a quantitative stability theorem for boundary measures using tools of convex analysis and geometric measure theory. As a corollary we obtain a stability result for Federer's curvature measures of a compact, allowing to compute them from point-cloud approximations of the compact.", ["Euclidean space", "Measure (mathematics)", "Convex analysis", "Curvature", "Theorem", "Subset", "Geometry", "Geometric measure theory", "Compact space", "Boundary (topology)", "Corollary", "Convex set", "Mathematical proof", "N-dimensional space"]], ["Dualheap Selection Algorithm: Efficient, Inherently Parallel and Somewhat Mysterious", "An inherently parallel algorithm is proposed that efficiently performs selection: finding the K-th largest member of a set of N members. Selection is a common component of many more complex algorithms and therefore is a widely studied problem. Not much is new in the proposed dualheap selection algorithm: the heap data structure is from J.W.J.Williams, the bottom-up heap construction is from R.W. Floyd, and the concept of a two heap data structure is from J.W.J. Williams and D.E. Knuth. The algorithm's novelty is limited to a few relatively minor implementation twists: 1) the two heaps are oriented with their roots at the partition values rather than at the minimum and maximum values, 2)the coding of one of the heaps (the heap of smaller values) employs negative indexing, and 3) the exchange phase of the algorithm is similar to a bottom-up heap construction, but navigates the heap with a post-order tree traversal. When run on a single processor, the dualheap selection algorithm's performance is competitive with quickselect with median estimation, a common variant of C.A.R. Hoare's quicksort algorithm. When run on parallel processors, the dualheap selection algorithm is superior due to its subtasks that are easily partitioned and innately balanced.", ["Parallel algorithm", "Quicksort", "Donald Knuth", "Tony Hoare", "Data structure", "Tree traversal", "Central processing unit", "Robert W. Floyd", "Selection algorithm", "Algorithm", "Dynamic memory allocation", "Heap (data structure)", "J.Williams (singer)"]], ["Resource control of object-oriented programs", "A sup-interpretation is a tool which provides an upper bound on the size of a value computed by some symbol of a program. Sup-interpretations have shown their interest to deal with the complexity of first order functional programs. For instance, they allow to characterize all the functions bitwise computable in Alogtime. This paper is an attempt to adapt the framework of sup-interpretations to a fragment of oriented-object programs, including distinct encodings of numbers through the use of constructor symbols, loop and while constructs and non recursive methods with side effects. We give a criterion, called brotherly criterion, which ensures that each brotherly program computes objects whose size is polynomially bounded by the inputs sizes.", ["Bitwise operation", "Object-oriented programming", "Upper and lower bounds", "First-order logic", "Recursion", "Computable function", "Functional programming"]], ["Space-time coding techniques with bit-interleaved coded modulations for MIMO block-fading channels", "The space-time bit-interleaved coded modulation (ST-BICM) is an efficient technique to obtain high diversity and coding gain on a block-fading MIMO channel. Its maximum-likelihood (ML) performance is computed under ideal interleaving conditions, which enables a global optimization taking into account channel coding. Thanks to a diversity upperbound derived from the Singleton bound, an appropriate choice of the time dimension of the space-time coding is possible, which maximizes diversity while minimizing complexity. Based on the analysis, an optimized interleaver and a set of linear precoders, called dispersive nucleo algebraic (DNA) precoders are proposed. The proposed precoders have good performance with respect to the state of the art and exist for any number of transmit antennas and any time dimension. With turbo codes, they exhibit a frame error rate which does not increase with frame length.", ["Singleton bound", "MIMO", "Turbo code", "DNA", "Global optimization", "Forward error correction", "Antenna (radio)", "Interleaving", "Mathematical optimization", "Dimension", "Maximum likelihood", "Bit", "Spacetime", "Modulation"]], ["Pricing American Options for Jump Diffusions by Iterating Optimal Stopping Problems for Diffusions", "We approximate the price of the American put for jump diffusions by a sequence of functions, which are computed iteratively. This sequence converges to the price function uniformly and exponentially fast. Each element of the approximating sequence solves an optimal stopping problem for geometric Brownian motion, and can be numerically computed using the classical finite difference methods. We prove the convergence of this numerical scheme and present examples to illustrate its performance.", ["Function (mathematics)", "Finite difference", "Convergent series", "Geometric Brownian motion", "Sequence", "Brownian motion", "Numerical analysis", "Limit of a sequence", "Finite set", "Iterated function", "Geometry", "United States"]], ["Interference and Outage in Clustered Wireless Ad Hoc Networks", "In the analysis of large random wireless networks, the underlying node distribution is almost ubiquitously assumed to be the homogeneous Poisson point process. In this paper, the node locations are assumed to form a Poisson clustered process on the plane. We derive the distributional properties of the interference and provide upper and lower bounds for its CCDF. We consider the probability of successful transmission in an interference limited channel when fading is modeled as Rayleigh. We provide a numerically integrable expression for the outage probability and closed-form upper and lower bounds.We show that when the transmitter-receiver distance is large, the success probability is greater than that of a Poisson arrangement. These results characterize the performance of the system under geographical or MAC-induced clustering. We obtain the maximum intensity of transmitting nodes for a given outage constraint, i.e., the transmission capacity (of this spatial arrangement) and show that it is equal to that of a Poisson arrangement of nodes. For the analysis, techniques from stochastic geometry are used, in particular the probability generating functional of Poisson cluster processes, the Palm characterization of Poisson cluster processes and the Campbell-Mecke theorem.", ["Wireless", "Poisson process", "Point process", "Geometry", "Wireless network", "Generating function", "Stochastic", "Transmitter"]], ["Progresses in the Analysis of Stochastic 2D Cellular Automata: a Study of Asynchronous 2D Minority", "Cellular automata are often used to model systems in physics, social sciences, biology that are inherently asynchronous. Over the past 20 years, studies have demonstrated that the behavior of cellular automata drastically changed under asynchronous updates. Still, the few mathematical analyses of asynchronism focus on one-dimensional probabilistic cellular automata, either on single examples or on specific classes. As for other classic dynamical systems in physics, extending known methods from one- to two-dimensional systems is a long lasting challenging problem. In this paper, we address the problem of analysing an apparently simple 2D asynchronous cellular automaton: 2D Minority where each cell, when fired, updates to the minority state of its neighborhood. Our experiments reveal that in spite of its simplicity, the minority rule exhibits a quite complex response to asynchronism. By focusing on the fully asynchronous regime, we are however able to describe completely the asymptotic behavior of this dynamics as long as the initial configuration satisfies some natural constraints. Besides these technical results, we have strong reasons to believe that our techniques relying on defining an energy function from the transition table of the automaton may be extended to the wider class of threshold automata.", ["Cellular automaton", "Social sciences", "Energy", "Biology", "Dynamical system", "Stochastic", "Cell (biology)", "Physics", "Asymptotic analysis", "Mathematics", "State transition table", "Automaton"]], ["Analysis of Inter-Domain Traffic Correlations: Random Matrix Theory Approach", "The traffic behavior of University of Louisville network with the interconnected backbone routers and the number of Virtual Local Area Network (VLAN) subnets is investigated using the Random Matrix Theory (RMT) approach. We employ the system of equal interval time series of traffic counts at all router to router and router to subnet connections as a representation of the inter-VLAN traffic. The cross-correlation matrix C of the traffic rate changes between different traffic time series is calculated and tested against null-hypothesis of random interactions. The majority of the eigenvalues \\lambda_{i} of matrix C fall within the bounds predicted by the RMT for the eigenvalues of random correlation matrices. The distribution of eigenvalues and eigenvectors outside of the RMT bounds displays prominent and systematic deviations from the RMT predictions. Moreover, these deviations are stable in time. The method we use provides a unique possibility to accomplish three concurrent tasks of traffic analysis. The method verifies the uncongested state of the network, by establishing the profile of random interactions. It recognizes the system-specific large-scale interactions, by establishing the profile of stable in time non-random interactions. Finally, by looking into the eigenstatistics we are able to detect and allocate anomalies of network traffic interactions.", ["Cross-correlation", "Router (computing)", "Eigenvalues and eigenvectors", "Random matrix", "Subnetwork", "Matrix (mathematics)", "Router", "Traffic analysis", "Correlation and dependence", "Virtual LAN", "University of Louisville", "Local area network", "Null hypothesis", "Time series", "Correlation matrix"]], ["Abstract machines for dialogue games", "The notion of abstract Boehm tree has arisen as an operationally-oriented distillation of works on game semantics, and has been investigated in two papers. This paper revisits the notion, providing more syntactic support and more examples (like call-by-value evaluation) illustrating the generality of the underlying computing device. Precise correspondences between various formulations of the evaluation mechanism of abstract Boehm trees are established.", ["Game semantics", "Evaluation strategy", "Computing", "Semantics", "Syntax"]], ["A new lower bound on the independence number of a graph", "For a given connected graph G on n vertices and m edges, we prove that its independence number is at least (2m+n+2-sqrt(sqr(2m+n+2)-16sqr(n)))/8.", ["Independent set (graph theory)", "Graph (mathematics)"]], ["Decisive Markov Chains", "We consider qualitative and quantitative verification problems for infinite-state Markov chains. We call a Markov chain decisive w.r.t. a given set of target states F if it almost certainly eventually reaches either F or a state from which F can no longer be reached. While all finite Markov chains are trivially decisive (for every set F), this also holds for many classes of infinite Markov chains. Infinite Markov chains which contain a finite attractor are decisive w.r.t. every set F. In particular, this holds for probabilistic lossy channel systems (PLCS). Furthermore, all globally coarse Markov chains are decisive. This class includes probabilistic vector addition systems (PVASS) and probabilistic noisy Turing machines (PNTM). We consider both safety and liveness problems for decisive Markov chains, i.e., the probabilities that a given set of states F is eventually reached or reached infinitely often, respectively. 1. We express the qualitative problems in abstract terms for decisive Markov chains, and show an almost complete picture of its decidability for PLCS, PVASS and PNTM. 2. We also show that the path enumeration algorithm of Iyer and Narasimha terminates for decisive Markov chains and can thus be used to solve the approximate quantitative safety problem. A modified variant of this algorithm solves the approximate quantitative liveness problem. 3. Finally, we show that the exact probability of (repeatedly) reaching F cannot be effectively expressed (in a uniform way) in Tarski-algebra for either PLCS, PVASS or (P)NTM.", ["Markov chain", "Finite set", "Attractor", "Algorithm", "Turing machine", "Algebra", "Vector addition", "Decidability (logic)", "Infinity", "Probability", "Lossy compression", "Abstraction"]], ["Randomness Extraction via Delta-Biased Masking in the Presence of a Quantum Attacker", "Randomness extraction is of fundamental importance for information-theoretic cryptography. It allows to transform a raw key about which an attacker has some limited knowledge into a fully secure random key, on which the attacker has essentially no information. Up to date, only very few randomness-extraction techniques are known to work against an attacker holding quantum information on the raw key. This is very much in contrast to the classical (non-quantum) setting, which is much better understood and for which a vast amount of different techniques are known and proven to work. We prove a new randomness-extraction technique, which is known to work in the classical setting, to be secure against a quantum attacker as well. Randomness extraction is done by XOR'ing a so-called delta-biased mask to the raw key. Our result allows to extend the classical applications of this extractor to the quantum setting. We discuss the following two applications. We show how to encrypt a long message with a short key, information-theoretically secure against a quantum attacker, provided that the attacker has enough quantum uncertainty on the message. This generalizes the concept of entropically-secure encryption to the case of a quantum attacker. As second application, we show how to do error-correction without leaking partial information to a quantum attacker. Such a technique is useful in settings where the raw key may contain errors, since standard error-correction techniques may provide the attacker with information on, say, a secret key that was used to obtain the raw key.", ["Standard error (statistics)", "Cryptography", "Error detection and correction", "Exclusive or", "Information theoretic security", "Key (cryptography)", "Entropy", "Quantum information", "Information theory", "Randomness", "Uncertainty principle"]], ["Algorithms for Omega-Regular Games with Imperfect Information", "We study observation-based strategies for two-player turn-based games on graphs with omega-regular objectives. An observation-based strategy relies on imperfect information about the history of a play, namely, on the past sequence of observations. Such games occur in the synthesis of a controller that does not see the private state of the plant. Our main results are twofold. First, we give a fixed-point algorithm for computing the set of states from which a player can win with a deterministic observation-based strategy for any omega-regular objective. The fixed point is computed in the lattice of antichains of state sets. This algorithm has the advantages of being directed by the objective and of avoiding an explicit subset construction on the game graph. Second, we give an algorithm for computing the set of states from which a player can win with probability 1 with a randomized observation-based strategy for a Buechi objective. This set is of interest because in the absence of perfect information, randomized strategies are more powerful than deterministic ones. We show that our algorithms are optimal by proving matching lower bounds.", ["Subset", "Powerset construction", "Perfect information", "Plant", "Probability", "Randomness", "Computing", "Fixed-point arithmetic", "Algorithm", "Fixed point (mathematics)", "Almost surely"]], ["The Complexity of Determining Existence a Hamiltonian Cycle is $O(n^3)$", "The Hamiltonian cycle problem in digraph is mapped into a matching cover bipartite graph. Based on this mapping, it is proved that determining existence a Hamiltonian cycle in graph is $O(n^3)$.", ["Hamiltonian path", "Graph (mathematics)", "Bipartite graph", "Complexity", "Directed graph", "Hamiltonian path problem", "Hamiltonian (quantum mechanics)"]], ["A Design Methodology for Space-Time Adapter", "This paper presents a solution to efficiently explore the design space of communication adapters. In most digital signal processing (DSP) applications, the overall architecture of the system is significantly affected by communication architecture, so the designers need specifically optimized adapters. By explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs a C description of Input/Output data scheduling, and user requirements (throughput, latency, parallelism...), and formalizes communication constraints through a Resource Constraints Graph (RCG). The RCG properties enable an efficient architecture space exploration in order to synthesize a STAR component. The proposed approach has been tested to design an industrial data mixing block example: an Ultra-Wideband interleaver.", ["Digital signal processing", "Spacetime", "Ultra-wideband", "Communication", "Wideband", "Graph theory", "Digital signal", "Signal processing", "Interleaving", "Input/output", "Throughput", "Data", "Architecture", "Space exploration", "Digital signal processor"]], ["Abstract Storage Devices", "A quantum storage device differs radically from a conventional physical storage device. Its state can be set to any value in a certain (infinite) state space, but in general every possible read operation yields only partial information about the stored state. The purpose of this paper is to initiate the study of a combinatorial abstraction, called abstract storage device (ASD), which models deterministic storage devices with the property that only partial information about the state can be read, but that there is a degree of freedom as to which partial information should be retrieved. This concept leads to a number of interesting problems which we address, like the reduction of one device to another device, the equivalence of devices, direct products of devices, as well as the factorization of a device into primitive devices. We prove that every ASD has an equivalent ASD with minimal number of states and of possible read operations. Also, we prove that the reducibility problem for ASD's is NP-complete, that the equivalence problem is at least as hard as the graph isomorphism problem, and that the factorization into binary-output devices (if it exists) is unique.", ["Graph isomorphism problem", "NP-complete", "Factorization", "Isomorphism", "Graph isomorphism", "Combinatorics", "Abstraction", "Graph (mathematics)"]], ["A Survey of Unix Init Schemes", "In most modern operating systems, init (as in \"initialization\") is the program launched by the kernel at boot time. It runs as a daemon and typically has PID 1. Init is responsible for spawning all other processes and scavenging zombies. It is also responsible for reboot and shutdown operations. This document describes existing solutions that implement the init process and/or init scripts in Unix-like systems. These solutions range from the legacy and still-in-use BSD and SystemV schemes, to recent and promising schemes from Ubuntu, Apple, Sun and independent developers. Our goal is to highlight their focus and compare their sets of features.", ["Unix-like", "Unix", "Berkeley Software Distribution", "Ubuntu (operating system)", "Kernel (computing)", "Daemon (computing)", "Booting", "Operating system", "Sun Microsystems", "Apple Inc."]], ["Dirty-paper Coding without Channel Information at the Transmitter and Imperfect Estimation at the Receiver", "In this paper, we examine the effects of imperfect channel estimation at the receiver and no channel knowledge at the transmitter on the capacity of the fading Costa's channel with channel state information non-causally known at the transmitter. We derive the optimal Dirty-paper coding (DPC) scheme and its corresponding achievable rates with the assumption of Gaussian inputs. Our results, for uncorrelated Rayleigh fading, provide intuitive insights on the impact of the channel estimate and the channel characteristics (e.g. SNR, fading process, channel training) on the achievable rates. These are useful in practical scenarios of multiuser wireless communications (e.g. Broadcast Channels) and information embedding applications (e.g. robust watermarking). We also studied optimal training design adapted to each application. We provide numerical results for a single-user fading Costa's channel with maximum-likehood (ML) channel estimation. These illustrate an interesting practical trade-off between the amount of training and its impact to the interference cancellation performance using DPC scheme.", ["Rayleigh fading", "Channel state information", "Causality", "Transmitter"]], ["Extraction d'entit\\'es dans des collections \\'evolutives", "The goal of our work is to use a set of reports and extract named entities, in our case the names of Industrial or Academic partners. Starting with an initial list of entities, we use a first set of documents to identify syntactic patterns that are then validated in a supervised learning phase on a set of annotated documents. The complete collection is then explored. This approach is similar to the ones used in data extraction from semi-structured documents (wrappers) and do not need any linguistic resources neither a large set for training. As our collection of documents would evolve over years, we hope that the performance of the extraction would improve with the increased size of the training set.", ["Supervised learning"]], ["On the Outage Capacity of a Practical Decoder Using Channel Estimation Accuracy", "The optimal decoder achieving the outage capacity under imperfect channel estimation is investigated. First, by searching into the family of nearest neighbor decoders, which can be easily implemented on most practical coded modulation systems, we derive a decoding metric that minimizes the average of the transmission error probability over all channel estimation errors. This metric, for arbitrary memoryless channels, achieves the capacity of a composite (more noisy) channel. Next, according to the notion of estimation-induced outage capacity (EIO capacity) introduced in our previous work, we characterize maximal achievable information rates associated to the proposed decoder. The performance of the proposed decoding metric over uncorrelated Rayleigh fading MIMO channels is compared to both the classical mismatched maximum-likelihood (ML) decoder and the theoretical limits given by the EIO capacity (i.e. the best decoder in presence of channel estimation errors). Numerical results show that the derived metric provides significant gains, in terms of achievable information rates and bit error rate (BER), in a bit interleaved coded modulation (BICM) framework, without introducing any additional decoding complexity.", ["Rayleigh fading", "Bit error rate", "Memorylessness", "Noise (electronics)", "Probability", "Channel state information", "Modulation", "Maximum likelihood", "Fading", "MIMO", "Decoder", "Bit", "Uncorrelated", "Interleaving"]], ["M\\'ethodologie de mod\\'elisation et d'impl\\'ementation d'adaptateurs spatio-temporels", "The re-use of pre-designed blocks is a well-known concept of the software development. This technique has been applied to System-on-Chip (SoC) design whose complexity and heterogeneity are growing. The re-use is made thanks to high level components, called virtual components (IP), available in more or less flexible forms. These components are dedicated blocks: digital signal processing (DCT, FFT), telecommunications (Viterbi, TurboCodes),... These blocks rest on a model of fixed architecture with very few degrees of personalization. This rigidity is particularly true for the communication interface whose orders of acquisition and production of data, the temporal behavior and protocols of exchanges are fixed. The successful integration of such an IP requires that the designer (1) synchronizes the components (2) converts the protocols between \"incompatible\" blocks (3) temporizes the data to guarantee the temporal constraints and the order of the data. This phase remains however very manual and source of errors. Our approach proposes a formal modeling, based on an original Ressource Compatibility Graph. The synthesis flow is based on a set of transformations of the initial graph to lead to an interface architecture allowing the space-time adaptation of the data exchanges between several components.", ["Communication", "Digital signal processing", "Digital signal", "Personalization", "Mathematical model", "Spacetime", "Homogeneity and heterogeneity", "Telecommunication", "Software development", "Signal processing", "Architecture", "Fast Fourier transform", "System-on-a-chip"]], ["Cache Analysis of Non-uniform Distribution Sorting Algorithms", "We analyse the average-case cache performance of distribution sorting algorithms in the case when keys are independently but not necessarily uniformly distributed. The analysis is for both `in-place' and `out-of-place' distribution sorting algorithms and is more accurate than the analysis presented in \\cite{RRESA00}. In particular, this new analysis yields tighter upper and lower bounds when the keys are drawn from a uniform distribution. We use this analysis to tune the performance of the integer sorting algorithm MSB radix sort when it is used to sort independent uniform floating-point numbers (floats). Our tuned MSB radix sort algorithm comfortably outperforms a cache-tuned implementations of bucketsort \\cite{RR99} and Quicksort when sorting uniform floats from $[0, 1)$.", ["Sorting algorithm", "Radix sort", "Quicksort", "Integer", "Locality of reference", "Algorithm", "Cache", "Upper and lower bounds", "Floating point"]], ["Variations on Kak's Three Stage Quantum Cryptography Protocol", "This paper introduces a variation on Kak's three-stage quanutm key distribution protocol which allows for defence against the man in the middle attack. In addition, we introduce a new protocol, which also offers similar resiliance against such an attack.", ["Man-in-the-middle attack"]], ["Dualheap Sort Algorithm: An Inherently Parallel Generalization of Heapsort", "A generalization of the heapsort algorithm is proposed. At the expense of about 50% more comparison and move operations for typical cases, the dualheap sort algorithm offers several advantages over heapsort: improved cache performance, better performance if the input happens to be already sorted, and easier parallel implementations.", ["Heapsort"]], ["Capacity Scaling for MIMO Two-Way Relaying", "A multiple input multiple output (MIMO) two-way relay channel is considered, where two sources want to exchange messages with each other using multiple relay nodes, and both the sources and relay nodes are equipped with multiple antennas. Both the sources are assumed to have equal number of antennas and have perfect channel state information (CSI) for all the channels of the MIMO two-way relay channel, whereas, each relay node is either assumed to have CSI for its transmit and receive channel (the coherent case) or no CSI for any of the channels (the non-coherent case). The main results in this paper are on the scaling behavior of the capacity region of the MIMO two-way relay channel with increasing number of relay nodes. In the coherent case, the capacity region of the MIMO two-way relay channel is shown to scale linearly with the number of antennas at source nodes and logarithmically with the number of relay nodes. In the non-coherent case, the capacity region is shown to scale linearly with the number of antennas at the source nodes and logarithmically with the signal to noise ratio.", ["Signal-to-noise ratio", "MIMO", "Channel state information", "Signal (electronics)", "Coherence (physics)"]], ["Reducing the Error Floor", "We discuss how the loop calculus approach of [Chertkov, Chernyak '06], enhanced by the pseudo-codeword search algorithm of [Chertkov, Stepanov '06] and the facet-guessing idea from [Dimakis, Wainwright '06], improves decoding of graph based codes in the error-floor domain. The utility of the new, Linear Programming based, decoding is demonstrated via analysis and simulations of the model $[155,64,20]$ code.", ["Code word", "Algorithm", "Calculus", "Graph (mathematics)", "Linear programming"]], ["Outage Behavior of Discrete Memoryless Channels Under Channel Estimation Errors", "Classically, communication systems are designed assuming perfect channel state information at the receiver and/or transmitter. However, in many practical situations, only an estimate of the channel is available that differs from the true channel. We address this channel mismatch scenario by using the notion of estimation-induced outage capacity, for which we provide an associated coding theorem and its strong converse, assuming a discrete memoryless channel. We illustrate our ideas via numerical simulations for transmissions over Ricean fading channels under a quality of service (QoS) constraint using rate-limited feedback channel and maximum likelihood (ML) channel estimation. Our results provide intuitive insights on the impact of the channel estimate and the channel characteristics (SNR, Ricean K-factor, training sequence length, feedback rate, etc.) on the mean outage capacity.", ["Channel state information", "Maximum likelihood", "Quality of service", "Memorylessness", "Communication", "Channel model", "Feedback", "Syncword", "Transmitter"]], ["Remote laboratories: new technology and standard based architecture", "E-Laboratories are important components of e- learning environments, especially in scientific and technical disciplines. First widespread E-Labs consisted in proposing simulations of real systems (virtual labs), as building remote labs (remote control of real systems) was difficult by lack of industrial standards and common protocols. Nowadays, robotics and automation technologies make easier the interfacing of systems with computers. In this frame, many researchers (such as those mentioned in [1]) focus on how to set up such a remote control. But, only a few of them deal with the educational point of view of the problem. This paper outlines our current research and reflection about remote laboratory modelling.", ["Architecture", "Laboratory", "Automation", "Remote control"]], ["A Generic Deployment Framework for Grid Computing and Distributed Applications", "Deployment of distributed applications on large systems, and especially on grid infrastructures, becomes a more and more complex task. Grid users spend a lot of time to prepare, install and configure middleware and application binaries on nodes, and eventually start their applications. The problem is that the deployment process is composed of many heterogeneous tasks that have to be orchestrated in a specific correct order. As a consequence, the automatization of the deployment process is currently very difficult to reach. To address this problem, we propose in this paper a generic deployment framework allowing to automatize the execution of heterogeneous tasks composing the whole deployment process. Our approach is based on a reification as software components of all required deployment mechanisms or existing tools. Grid users only have to describe the configuration to deploy in a simple natural language instead of programming or scripting how the deployment process is executed. As a toy example, this framework is used to deploy CORBA component-based applications and OpenCCM middleware on one thousand nodes of the French Grid5000 infrastructure.", ["Common Object Request Broker Architecture", "Middleware", "Homogeneity and heterogeneity", "Reification (computer science)", "Natural language", "Distributed computing", "Toy", "Infrastructure", "Grid computing", "Computer software", "Computer programming", "Component-based software engineering", "Scripting language"]], ["Application of a design space exploration tool to enhance interleaver generation", "This paper presents a methodology to efficiently explore the design space of communication adapters. In most digital signal processing (DSP) applications, the overall performance of the system is significantly affected by communication architectures, as a consequence the designers need specifically optimized adapters. By explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs a C description of Input/Output data scheduling, and user requirements (throughput, latency, parallelism...), and formalizes communication constraints through a Resource Constraints Graph (RCG). Design space exploration is then performed through associated tools, to synthesize a STAR component under time-to-market constraints. The proposed approach has been tested to design an industrial data mixing block example: an Ultra-Wideband interleaver.", ["Digital signal processing", "Ultra-wideband", "Graph theory", "Digital signal", "Communication", "Spacetime", "Interleaving", "Wideband", "Input/output", "Signal processing", "Data", "Throughput", "Architecture", "Parallel computing", "Digital signal processor"]], ["N-Body Simulations on GPUs", "Commercial graphics processors (GPUs) have high compute capacity at very low cost, which makes them attractive for general purpose scientific computing. In this paper we show how graphics processors can be used for N-body simulations to obtain improvements in performance over current generation CPUs. We have developed a highly optimized algorithm for performing the O(N^2) force calculations that constitute the major part of stellar and molecular dynamics simulations. In some of the calculations, we achieve sustained performance of nearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to specialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the cost. Furthermore, the wide availability of GPUs has significant implications for cluster computing and distributed computing efforts like Folding@Home.", ["Molecular dynamics", "N-body simulation", "Distributed computing", "Computer cluster", "FLOPS", "Central processing unit", "Computing"]], ["On the Performance of Joint Fingerprint Embedding and Decryption Scheme", "Till now, few work has been done to analyze the performances of joint fingerprint embedding and decryption schemes. In this paper, the security of the joint fingerprint embedding and decryption scheme proposed by Kundur et al. is analyzed and improved. The analyses include the security against unauthorized customer, the security against authorized customer, the relationship between security and robustness, the relationship between secu-rity and imperceptibility and the perceptual security. Based these analyses, some means are proposed to strengthen the system, such as multi-key encryp-tion and DC coefficient encryption. The method can be used to analyze other JFD schemes. It is expected to provide valuable information to design JFD schemes.", ["DC bias", "Security"]], ["Group Testing with Random Pools: optimal two-stage algorithms", "We study Probabilistic Group Testing of a set of N items each of which is defective with probability p. We focus on the double limit of small defect probability, p<<1, and large number of variables, N>>1, taking either p->0 after $N\\to\\infty$ or $p=1/N^{\\beta}$ with $\\beta\\in(0,1/2)$. In both settings the optimal number of tests which are required to identify with certainty the defectives via a two-stage procedure, $\\bar T(N,p)$, is known to scale as $Np|\\log p|$. Here we determine the sharp asymptotic value of $\\bar T(N,p)/(Np|\\log p|)$ and construct a class of two-stage algorithms over which this optimal value is attained. This is done by choosing a proper bipartite regular graph (of tests and variable nodes) for the first stage of the detection. Furthermore we prove that this optimal value is also attained on average over a random bipartite graph where all variables have the same degree, while the tests have Poisson-distributed degrees. Finally, we improve the existing upper and lower bound for the optimal number of tests in the case $p=1/N^{\\beta}$ with $\\beta\\in[1/2,1)$.", ["Poisson distribution", "Bipartite graph", "Regular graph", "Algorithm", "Asymptote", "Graph (mathematics)", "Probability", "Upper and lower bounds", "Randomness"]], ["Closed-Form Density of States and Localization Length for a Non-Hermitian Disordered System", "We calculate the Lyapunov exponent for the non-Hermitian Zakharov-Shabat eigenvalue problem corresponding to the attractive non-linear Schroedinger equation with a Gaussian random pulse as initial value function. Using an extension of the Thouless formula to non-Hermitian random operators, we calculate the corresponding average density of states. We analyze two cases, one with circularly symmetric complex Gaussian pulses and the other with real Gaussian pulses. We discuss the implications in the context of the information transmission through non-linear optical fibers.", ["Lyapunov exponent", "Eigenvalues and eigenvectors", "Optical fiber", "Hermitian matrix", "Density of states", "Exponentiation", "Density", "Linear", "Circular symmetry", "Function (mathematics)"]], ["EasyVoice: Integrating voice synthesis with Skype", "This paper presents EasyVoice, a system that integrates voice synthesis with Skype. EasyVoice allows a person with voice disabilities to talk with another person located anywhere in the world, removing an important obstacle that affect these people during a phone or VoIP-based conversation.", ["Speech synthesis", "Voice over Internet Protocol", "Skype"]], ["WiFi Epidemiology: Can Your Neighbors' Router Make Yours Sick?", "In densely populated urban areas WiFi routers form a tightly interconnected proximity network that can be exploited as a substrate for the spreading of malware able to launch massive fraudulent attack and affect entire urban areas WiFi networks. In this paper we consider several scenarios for the deployment of malware that spreads solely over the wireless channel of major urban areas in the US. We develop an epidemiological model that takes into consideration prevalent security flaws on these routers. The spread of such a contagion is simulated on real-world data for geo-referenced wireless routers. We uncover a major weakness of WiFi networks in that most of the simulated scenarios show tens of thousands of routers infected in as little time as two weeks, with the majority of the infections occurring in the first 24 to 48 hours. We indicate possible containment and prevention measure to limit the eventual harm of such an attack.", ["Malware", "Wi-Fi", "Epidemiology"]], ["Une s\\'emantique observationnelle du mod\\`ele des bo\\^ites pour la r\\'esolution de programmes logiques (version \\'etendue)", "This report specifies an observational semantics and gives an original presentation of the Byrd's box model. The approach accounts for the semantics of Prolog tracers independently of a particular implementation. Traces are, in general, considered as rather obscure and difficult to use. The proposed formal presentation of a trace constitutes a simple and pedagogical approach for teaching Prolog or for implementing Prolog tracers. It constitutes a form of declarative specification for the tracers. Our approach highlights qualities of the box model which made its success, but also its drawbacks and limits. As a matter of fact, the presented semantics is only one example to illustrate general problems relating to tracers and observing processes. Observing processes know, from observed processes, only their traces. The issue is then to be able to reconstitute by the sole analysis of the trace the main part of the observed process, and if possible, without any loss of information.", ["Prolog"]], ["A solution for actors' viewpoints representation with collaborative product development", "As product complexity and marketing competition increase, a collaborative product development is necessary for companies which develop high quality products in short lead-times. To support product actors from different fields, disciplines, and locations, wishing to exchange and share information, the representation of the actors' viewpoints is the underlying requirement of the collaborative product development. The actors' viewpoints approach was designed to provide an organisational framework following the actors' perspectives in the collaboration, and their relationships, could be explicitly gathered and formatted. The approach acknowledges the inevitability of multiple integration of product information as different views, promotes gathering of actors' interests, and encourages retrieved adequate information while providing support for integration through PLM and/or SCM collaboration. In this paper, a solution for neutral viewpoints representation is proposed. The product, process, and organisation information models are seriatim discussed. A series of issues referring to the viewpoints representation are discussed in detail. Based on XML standard, taking cyclone vessel as an example, an application case of part of product information modelling is stated.", ["XML", "Marketing", "New product development", "Information model"]], ["Asymptotic Analysis of General Multiuser Detectors in MIMO DS-CDMA Channels", "We analyze a MIMO DS-CDMA channel with a general multiuser detector including a nonlinear multiuser detector, using the replica method. In the many-user, limit the MIMO DS-CDMA channel with the multiuser detector is decoupled into a bank of single-user SIMO Gaussian channels if a spatial spreading scheme is employed. On the other hand, it is decoupled into a bank of single-user MIMO Gaussian channels if a spatial spreading scheme is not employed. The spectral efficiency of the MIMO DS-CDMA channel with the spatial spreading scheme is comparable with that of the MIMO DS-CDMA channel using an optimal space-time block code without the spatial spreading scheme. In the case of the QPSK data modulation scheme the spectral efficiency of the MIMO DS-CDMA channel with the MMSE detector shows {\\it waterfall} behavior and is very close to the corresponding sum capacity when the system load is just below the transition point of the {\\it waterfall} behavior. Our result implies that the performance of a multiuser detector taking the data modulation scheme into consideration can be far superior to that of linear multiuser detectors.", ["Nonlinear system", "Modulation", "MIMO", "Code division multiple access", "Block code", "QPSK", "Spacetime"]], ["A tutorial on conformal prediction", "Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability $\\epsilon$, together with a method that makes a prediction $\\hat{y}$ of a label $y$, it produces a set of labels, typically containing $\\hat{y}$, that also contains $y$ with probability $1-\\epsilon$. Conformal prediction can be applied to any method for producing $\\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right $1-\\epsilon$ of the time, even though they are based on an accumulating dataset rather than on independent datasets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in \"Algorithmic Learning in a Random World\", by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).", ["Tikhonov regularization", "Independence (probability theory)", "Regression analysis", "Normal distribution", "Probability", "Probability distribution", "Data set", "Prediction", "Linear model", "Errors and residuals in statistics"]], ["Lower bounds on the minimum average distance of binary codes", "New lower bounds on the minimum average Hamming distance of binary codes are derived. The bounds are obtained using linear programming approach.", ["Hamming distance", "Linear programming"]], ["A simple generalization of the ElGamal cryptosystem to non-abelian groups II", "This is a study of the MOR cryptosystem using the special linear group over finite fields. The automorphism group of the special linear group is analyzed for this purpose. At our current state of knowledge, I show that the MOR cryptosystem has better security than the ElGamal cryptosystem over finite fields.", ["Special linear group", "Finite set", "Automorphism group", "Nonabelian group", "Automorphism", "Finite field", "Matrix group", "Abelian group", "Middle of the road (music)", "ElGamal encryption"]], ["A Sequent Calculus for Modelling Interferences", "A logic calculus is presented that is a conservative extension of linear logic. The motivation beneath this work concerns lazy evaluation, true concurrency and interferences in proof search. The calculus includes two new connectives to deal with multisequent structures and has the cut-elimination property. Extensions are proposed that give first results concerning our objectives.", ["Conservative extension", "Linear logic", "Lazy evaluation", "Sequent", "Cut-elimination theorem", "Calculus", "Logic", "Motivation"]], ["Optimal Replica Placement in Tree Networks with QoS and Bandwidth Constraints and the Closest Allocation Policy", "This paper deals with the replica placement problem on fully homogeneous tree networks known as the Replica Placement optimization problem. The client requests are known beforehand, while the number and location of the servers are to be determined. We investigate the latter problem using the Closest access policy when adding QoS and bandwidth constraints. We propose an optimal algorithm in two passes using dynamic programming.", ["Dynamic programming", "Quality of service", "Asymptotically optimal algorithm", "Algorithm", "Mathematical optimization", "Server (computing)", "Optimization problem"]], ["On Canonical Forms of Complete Problems via First-order Projections", "The class of problems complete for NP via first-order reductions is known to be characterized by existential second-order sentences of a fixed form. All such sentences are built around the so-called generalized IS-form of the sentence that defines Independent-Set. This result can also be understood as that every sentence that defines a NP-complete problem P can be decomposed in two disjuncts such that the first one characterizes a fragment of P as hard as Independent-Set and the second the rest of P. That is, a decomposition that divides every such sentence into a quotient and residue modulo Independent-Set. In this paper, we show that this result can be generalized over a wide collection of complexity classes, including the so-called nice classes. Moreover, we show that such decomposition can be done for any complete problem with respect to the given class, and that two such decompositions are non-equivalent in general. Interestingly, our results are based on simple and well-known properties of first-order reductions.ow that this result can be generalized over a wide collection of complexity classes, including the so-called nice classes. Moreover, we show that such decomposition can be done for any complete problem with respect to the given class, and that two such decompositions are non-equivalent in general. Interestingly, our results are based on simple and well-known properties of first-order reductions.", ["NP-complete", "Modular arithmetic", "First-order logic", "Set (mathematics)", "Complexity class"]], ["The Impact of Channel Feedback on Opportunistic Relay Selection for Hybrid-ARQ in Wireless Networks", "This paper presents a decentralized relay selection protocol for a dense wireless network and describes channel feedback strategies that improve its performance. The proposed selection protocol supports hybrid automatic-repeat-request transmission where relays forward parity information to the destination in the event of a decoding error. Channel feedback is employed for refining the relay selection process and for selecting an appropriate transmission mode in a proposed adaptive modulation transmission framework. An approximation of the throughput of the proposed adaptive modulation strategy is presented, and the dependence of the throughput on system parameters such as the relay contention probability and the adaptive modulation switching point is illustrated via maximization of this approximation. Simulations show that the throughput of the proposed selection strategy is comparable to that yielded by a centralized selection approach that relies on geographic information.", ["Wireless network", "Decentralization", "Link adaptation", "Modulation", "Feedback", "Automatic repeat request", "Probability"]], ["NP by means of lifts and shadows", "We show that every NP problem is polynomially equivalent to a simple combinatorial problem: the membership problem for a special class of digraphs. These classes are defined by means of shadows (projections) and by finitely many forbidden colored (lifted) subgraphs. Our characterization is motivated by the analysis of syntactical subclasses with the full computational power of NP, which were first studied by Feder and Vardi. Our approach applies to many combinatorial problems and it induces the characterization of coloring problems (CSP) defined by means of shadows. This turns out to be related to homomorphism dualities. We prove that a class of digraphs (relational structures) defined by finitely many forbidden colored subgraphs (i.e. lifted substructures) is a CSP class if and only if all the the forbidden structures are homomorphically equivalent to trees. We show a surprising richness of coloring problems when restricted to most frequent graph classes. Using results of Ne\\v{s}et\\v{r}il and Ossona de Mendez for bounded expansion classes (which include bounded degree and proper minor closed classes) we prove that the restriction of every class defined as the shadow of finitely many colored subgraphs equals to the restriction of a coloring (CSP) class.", ["Combinatorics", "If and only if", "Homomorphism", "Graph (mathematics)"]], ["Hypocomputation", "Hypercomputational formal theories will, clearly, be both structurally and foundationally different from the formal theories underpinning computational theories. However, many of the maps that might guide us into this strange realm have been lost. So little work has been done recently in the area of metamathematics, and so many of the previous results have been folded into other theories, that we are in danger of loosing an appreciation of the broader structure of formal theories. As an aid to those looking to develop hypercomputational theories, we will briefly survey the known landmarks both inside and outside the borders of computational theory. We will not focus in this paper on why the structure of formal theory looks the way it does. Instead we will focus on what this structure looks like, moving from hypocomputational, through traditional computational theories, and then beyond to hypercomputational theories.", ["Theory of computation", "Computational chemistry"]], ["Tight Bounds on the Average Length, Entropy, and Redundancy of Anti-Uniform Huffman Codes", "In this paper we consider the class of anti-uniform Huffman codes and derive tight lower and upper bounds on the average length, entropy, and redundancy of such codes in terms of the alphabet size of the source. The Fibonacci distributions are introduced which play a fundamental role in AUH codes. It is shown that such distributions maximize the average length and the entropy of the code for a given alphabet size. Another previously known bound on the entropy for given average length follows immediately from our results.", ["Entropy", "Alphabet"]], ["Approximately-Universal Space-Time Codes for the Parallel, Multi-Block and Cooperative-Dynamic-Decode-and-Forward Channels", "Explicit codes are constructed that achieve the diversity-multiplexing gain tradeoff of the cooperative-relay channel under the dynamic decode-and-forward protocol for any network size and for all numbers of transmit and receive antennas at the relays. A particularly simple code construction that makes use of the Alamouti code as a basic building block is provided for the single relay case. Along the way, we prove that space-time codes previously constructed in the literature for the block-fading and parallel channels are approximately universal, i.e., they achieve the DMT for any fading distribution. It is shown how approximate universality of these codes leads to the first DMT-optimum code construction for the general, MIMO-OFDM channel.", ["Antenna (radio)", "Fading", "MIMO", "Multiplexing", "Spacetime"]], ["There Exist some Omega-Powers of Any Borel Rank", "Omega-powers of finitary languages are languages of infinite words (omega-languages) in the form V^omega, where V is a finitary language over a finite alphabet X. They appear very naturally in the characterizaton of regular or context-free omega-languages. Since the set of infinite words over a finite alphabet X can be equipped with the usual Cantor topology, the question of the topological complexity of omega-powers of finitary languages naturally arises and has been posed by Niwinski (1990), Simonnet (1992) and Staiger (1997). It has been recently proved that for each integer n > 0, there exist some omega-powers of context free languages which are Pi^0_n-complete Borel sets, that there exists a context free language L such that L^omega is analytic but not Borel, and that there exists a finitary language V such that V^omega is a Borel set of infinite rank. But it was still unknown which could be the possible infinite Borel ranks of omega-powers. We fill this gap here, proving the following very surprising result which shows that omega-powers exhibit a great topological complexity: for each non-null countable ordinal alpha, there exist some Sigma^0_alpha-complete omega-powers, and some Pi^0_alpha-complete omega-powers.", ["Cantor space", "Integer", "Countable set", "Infinity", "Topology", "Finite set", "Ordinal number", "Borel set", "Omega", "Georg Cantor", "Alphabet", "Context-free language", "Finitary"]], ["stdchk: A Checkpoint Storage System for Desktop Grid Computing", "Checkpointing is an indispensable technique to provide fault tolerance for long-running high-throughput applications like those running on desktop grids. This paper argues that a dedicated checkpoint storage system, optimized to operate in these environments, can offer multiple benefits: reduce the load on a traditional file system, offer high-performance through specialization, and, finally, optimize data management by taking into account checkpoint application semantics. Such a storage system can present a unifying abstraction to checkpoint operations, while hiding the fact that there are no dedicated resources to store the checkpoint data. We prototype stdchk, a checkpoint storage system that uses scavenged disk space from participating desktops to build a low-cost storage system, offering a traditional file system interface for easy integration with applications. This paper presents the stdchk architecture, key performance optimizations, support for incremental checkpointing, and increased data availability. Our evaluation confirms that the stdchk approach is viable in a desktop grid setting and offers a low cost storage system with desirable performance characteristics: high write throughput and reduced storage space and network effort to save checkpoint images.", ["Data management", "Application checkpointing", "File system", "Throughput", "Grid computing", "Data", "Semantics", "Prototype", "Computer data storage", "Computing", "Fault-tolerant design", "Computer network", "Architecture"]], ["Experimental Algorithm for the Maximum Independent Set Problem", "We develop an experimental algorithm for the exact solving of the maximum independent set problem. The algorithm consecutively finds the maximal independent sets of vertices in an arbitrary undirected graph such that the next such set contains more elements than the preceding one. For this purpose, we use a technique, developed by Ford and Fulkerson for the finite partially ordered sets, in particular, their method for partition of a poset into the minimum number of chains with finding the maximum antichain. In the process of solving, a special digraph is constructed, and a conjecture is formulated concerning properties of such digraph. This allows to offer of the solution algorithm. Its theoretical estimation of running time equals to is $O(n^{8})$, where $n$ is the number of graph vertices. The offered algorithm was tested by a program on random graphs. The testing the confirms correctness of the algorithm.", ["Undirected graph", "Independent set (graph theory)", "Directed graph", "Graph (mathematics)", "Antichain", "Partially ordered set", "Conjecture", "Random graph", "Finite set", "Time complexity", "Partition of a set", "Maximal element", "Algorithm"]], ["A Collection of Definitions of Intelligence", "This paper is a survey of a large number of informal definitions of ``intelligence'' that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70-odd definitions presented here are, to the authors' knowledge, the largest and most well referenced collection there is.", ["intelligence"]], ["Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers Taking Values in R^Q", "Bounds on the risk play a crucial role in statistical learning theory. They usually involve as capacity measure of the model studied the VC dimension or one of its extensions. In classification, such \"VC dimensions\" exist for models taking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations appropriate for the missing case, the one of models with values in R^Q. This provides us with a new guaranteed risk for M-SVMs which appears superior to the existing one.", ["Machine learning", "Statistics", "VC dimension", "Dimension"]], ["Optimal Constellations for the Low SNR Noncoherent MIMO Block Rayleigh Fading Channel", "Reliable communication over the discrete-input/continuous-output noncoherent multiple-input multiple-output (MIMO) Rayleigh block fading channel is considered when the signal-to-noise ratio (SNR) per degree of freedom is low. Two key problems are posed and solved to obtain the optimum discrete input. In both problems, the average and peak power per space-time slot of the input constellation are constrained. In the first one, the peak power to average power ratio (PPAPR) of the input constellation is held fixed, while in the second problem, the peak power is fixed independently of the average power. In the first PPAPR-constrained problem, the mutual information, which grows as O(SNR^2), is maximized up to second order in SNR. In the second peak-constrained problem, where the mutual information behaves as O(SNR), the structure of constellations that are optimal up to first order, or equivalently, that minimize energy/bit, are explicitly characterized. Furthermore, among constellations that are first-order optimal, those that maximize the mutual information up to second order, or equivalently, the wideband slope, are characterized. In both PPAPR-constrained and peak-constrained problems, the optimal constellations are obtained in closed-form as solutions to non-convex optimizations, and interestingly, they are found to be identical. Due to its special structure, the common solution is referred to as Space Time Orthogonal Rank one Modulation, or STORM. In both problems, it is seen that STORM provides a sharp characterization of the behavior of noncoherent MIMO capacity.", ["Signal-to-noise ratio", "Wideband", "MIMO", "Mutual information", "Modulation", "Bit", "Energy", "Constellation", "Communication", "Spacetime", "Signal (electronics)", "Fading"]], ["Order-Invariant MSO is Stronger than Counting MSO in the Finite", "We compare the expressiveness of two extensions of monadic second-order logic (MSO) over the class of finite structures. The first, counting monadic second-order logic (CMSO), extends MSO with first-order modulo-counting quantifiers, allowing the expression of queries like ``the number of elements in the structure is even''. The second extension allows the use of an additional binary predicate, not contained in the signature of the queried structure, that must be interpreted as an arbitrary linear order on its universe, obtaining order-invariant MSO. While it is straightforward that every CMSO formula can be translated into an equivalent order-invariant MSO formula, the converse had not yet been settled. Courcelle showed that for restricted classes of structures both order-invariant MSO and CMSO are equally expressive, but conjectured that, in general, order-invariant MSO is stronger than CMSO. We affirm this conjecture by presenting a class of structures that is order-invariantly definable in MSO but not definable in CMSO.", ["Second-order logic", "Binary relation", "Quantification", "Unary operation", "First-order logic", "Finite set", "Logic", "Total order", "Formula", "Modular arithmetic", "Conjecture"]], ["Pruning Processes and a New Characterization of Convex Geometries", "We provide a new characterization of convex geometries via a multivariate version of an identity that was originally proved by Maneva, Mossel and Wainwright for certain combinatorial objects arising in the context of the k-SAT problem. We thus highlight the connection between various characterizations of convex geometries and a family of removal processes studied in the literature on random structures.", ["SAT", "Combinatorics"]], ["Secure Nested Codes for Type II Wiretap Channels", "This paper considers the problem of secure coding design for a type II wiretap channel, where the main channel is noiseless and the eavesdropper channel is a general binary-input symmetric-output memoryless channel. The proposed secure error-correcting code has a nested code structure. Two secure nested coding schemes are studied for a type II Gaussian wiretap channel. The nesting is based on cosets of a good code sequence for the first scheme and on cosets of the dual of a good code sequence for the second scheme. In each case, the corresponding achievable rate-equivocation pair is derived based on the threshold behavior of good code sequences. The two secure coding schemes together establish an achievable rate-equivocation region, which almost covers the secrecy capacity-equivocation region in this case study. The proposed secure coding scheme is extended to a type II binary symmetric wiretap channel. A new achievable perfect secrecy rate, which improves upon the previously reported result by Thangaraj et al., is derived for this channel.", ["Forward error correction", "Memorylessness", "Telephone tapping", "Eavesdropping"]], ["Multiple Access Channels with Generalized Feedback and Confidential Messages", "This paper considers the problem of secret communication over a multiple access channel with generalized feedback. Two trusted users send independent confidential messages to an intended receiver, in the presence of a passive eavesdropper. In this setting, an active cooperation between two trusted users is enabled through using channel feedback in order to improve the communication efficiency. Based on rate-splitting and decode-and-forward strategies, achievable secrecy rate regions are derived for both discrete memoryless and Gaussian channels. Results show that channel feedback improves the achievable secrecy rates.", ["Channel access method", "Eavesdropping", "Communication", "Feedback", "Memorylessness"]], ["Dynamic Exploration of Networks: from general principles to the traceroute process", "Dynamical processes taking place on real networks define on them evolving subnetworks whose topology is not necessarily the same of the underlying one. We investigate the problem of determining the emerging degree distribution, focusing on a class of tree-like processes, such as those used to explore the Internet's topology. A general theory based on mean-field arguments is proposed, both for single-source and multiple-source cases, and applied to the specific example of the traceroute exploration of networks. Our results provide a qualitative improvement in the understanding of dynamical sampling and of the interplay between dynamics and topology in large networks like the Internet.", ["Topology", "Traceroute", "Internet"]], ["Java Components Vulnerabilities - An Experimental Classification Targeted at the OSGi Platform", "The OSGi Platform finds a growing interest in two different applications domains: embedded systems, and applications servers. However, the security properties of this platform are hardly studied, which is likely to hinder its use in production systems. This is all the more important that the dynamic aspect of OSGi-based applications, that can be extended at runtime, make them vulnerable to malicious code injection. We therefore perform a systematic audit of the OSGi platform so as to build a vulnerability catalog that intends to reference OSGi Vulnerabilities originating in the Core Specification, and in behaviors related to the use of the Java language. Standard Services are not considered. To support this audit, a Semi-formal Vulnerability Pattern is defined, that enables to uniquely characterize fundamental properties for each vulnerability, to include verbose description in the pattern, to reference known security protections, and to track the implementation status of the proof-of-concept OSGi Bundles that exploit the vulnerability. Based on the analysis of the catalog, a robust OSGi Platform is built, and recommendations are made to enhance the OSGi Specifications.", ["OSGi", "Code injection", "Vulnerability (computing)", "Malware", "Java (programming language)", "Audit", "Embedded system", "Security", "Proof of concept"]], ["Design of optimal convolutional codes for joint decoding of correlated sources in wireless sensor networks", "We consider a wireless sensors network scenario where two nodes detect correlated sources and deliver them to a central collector via a wireless link. Differently from the Slepian-Wolf approach to distributed source coding, in the proposed scenario the sensing nodes do not perform any pre-compression of the sensed data. Original data are instead independently encoded by means of low-complexity convolutional codes. The decoder performs joint decoding with the aim of exploiting the inherent correlation between the transmitted sources. Complexity at the decoder is kept low thanks to the use of an iterative joint decoding scheme, where the output of each decoder is fed to the other decoder's input as a-priori information. For such scheme, we derive a novel analytical framework for evaluating an upper bound of joint-detection packet error probability and for deriving the optimum coding scheme. Experimental results confirm the validity of the analytical framework, and show that recursive codes allow a noticeable performance gain with respect to non-recursive coding schemes. Moreover, the proposed recursive coding scheme allows to approach the ideal Slepian-Wolf scheme performance in AWGN channel, and to clearly outperform it over fading channels on account of diversity gain due to correlation of information.", ["Convolutional code", "Correlation and dependence", "Additive white Gaussian noise", "Complexity", "Wireless sensor network", "Diversity scheme", "Sensor", "Data compression", "Probability", "Decoder", "A priori and a posteriori", "Iteration", "Data", "Recursion"]], ["Opportunistic Scheduling and Beamforming for MIMO-SDMA Downlink Systems with Linear Combining", "Opportunistic scheduling and beamforming schemes are proposed for multiuser MIMO-SDMA downlink systems with linear combining in this work. Signals received from all antennas of each mobile terminal (MT) are linearly combined to improve the {\\em effective} signal-to-noise-interference ratios (SINRs). By exploiting limited feedback on the effective SINRs, the base station (BS) schedules simultaneous data transmission on multiple beams to the MTs with the largest effective SINRs. Utilizing the extreme value theory, we derive the asymptotic system throughputs and scaling laws for the proposed scheduling and beamforming schemes with different linear combining techniques. Computer simulations confirm that the proposed schemes can substantially improve the system throughput.", ["Beamforming", "Signal-to-noise ratio", "Linear combination", "MIMO", "Computer simulation", "Value theory", "Extreme value theory", "Data transmission", "Feedback", "Throughput", "Downlink", "Linear", "Base station"]], ["Minimum Sum Edge Colorings of Multicycles", "In the minimum sum edge coloring problem, we aim to assign natural numbers to edges of a graph, so that adjacent edges receive different numbers, and the sum of the numbers assigned to the edges is minimum. The {\\em chromatic edge strength} of a graph is the minimum number of colors required in a minimum sum edge coloring of this graph. We study the case of multicycles, defined as cycles with parallel edges, and give a closed-form expression for the chromatic edge strength of a multicycle, thereby extending a theorem due to Berge. It is shown that the minimum sum can be achieved with a number of colors equal to the chromatic index. We also propose simple algorithms for finding a minimum sum edge coloring of a multicycle. Finally, these results are generalized to a large family of minimum cost coloring problems.", ["Edge coloring", "Closed-form expression", "Food coloring"]], ["Approximations of Lovasz extensions and their induced interaction index", "The Lovasz extension of a pseudo-Boolean function $f : \\{0,1\\}^n \\to R$ is defined on each simplex of the standard triangulation of $[0,1]^n$ as the unique affine function $\\hat f : [0,1]^n \\to R$ that interpolates $f$ at the $n+1$ vertices of the simplex. Its degree is that of the unique multilinear polynomial that expresses $f$. In this paper we investigate the least squares approximation problem of an arbitrary Lovasz extension $\\hat f$ by Lovasz extensions of (at most) a specified degree. We derive explicit expressions of these approximations. The corresponding approximation problem for pseudo-Boolean functions was investigated by Hammer and Holzman (1992) and then solved explicitly by Grabisch, Marichal, and Roubens (2000), giving rise to an alternative definition of Banzhaf interaction index. Similarly we introduce a new interaction index from approximations of $\\hat f$ and we present some of its properties. It turns out that its corresponding power index identifies with the power index introduced by Grabisch and Labreuche (2001).", ["Boolean function", "Affine transformation", "Least squares", "Function (mathematics)", "Triangulation", "Multilinear map", "Simplex", "Polynomial"]], ["Bid Optimization for Internet Graphical Ad Auction Systems via Special Ordered Sets", "This paper describes an optimization model for setting bid levels for certain types of advertisements on web pages. This model is non-convex, but we are able to obtain optimal or near-optimal solutions rapidly using branch and cut open-source software. The financial benefits obtained using the prototype system have been substantial.", ["Mathematical optimization", "Prototype", "Open-source software"]], ["A Comparison of Push and Pull Techniques for Ajax", "Ajax applications are designed to have high user interactivity and low user-perceived latency. Real-time dynamic web data such as news headlines, stock tickers, and auction updates need to be propagated to the users as soon as possible. However, Ajax still suffers from the limitations of the Web's request/response architecture which prevents servers from pushing real-time dynamic web data. Such applications usually use a pull style to obtain the latest updates, where the client actively requests the changes based on a predefined interval. It is possible to overcome this limitation by adopting a push style of interaction where the server broadcasts data when a change occurs on the server side. Both these options have their own trade-offs. This paper explores the fundamental limits of browser-based applications and analyzes push solutions for Ajax technology. It also shows the results of an empirical study comparing push and pull.", ["Auction", "Ajax (programming)", "Server (computing)", "Web browser", "World Wide Web", "Architecture", "Client (computing)", "Interactivity"]], ["End-to-End Available Bandwidth Measurement Tools : A Comparative Evaluation of Performances", "In recent years, there has been a strong interest in measuring the available bandwidth of network paths. Several methods and techniques have been proposed and various measurement tools have been developed and evaluated. However, there have been few comparative studies with regards to the actual performance of these tools. This paper presents a study of available bandwidth measurement techniques and undertakes a comparative analysis in terms of accuracy, intrusiveness and response time of active probing tools. Finally, measurement errors and the uncertainty of the tools are analysed and overall conclusions made.", ["Observational error"]], ["Multi-criteria scheduling of pipeline workflows", "Mapping workflow applications onto parallel platforms is a challenging problem, even for simple application patterns such as pipeline graphs. Several antagonist criteria should be optimized, such as throughput and latency (or a combination). In this paper, we study the complexity of the bi-criteria mapping problem for pipeline graphs on communication homogeneous platforms. In particular, we assess the complexity of the well-known chains-to-chains problem for different-speed processors, which turns out to be NP-hard. We provide several efficient polynomial bi-criteria heuristics, and their relative performance is evaluated through extensive simulations.", ["Heuristic", "Polynomial", "NP-hard", "Workflow", "Parallel computing", "Communication", "Throughput", "Complexity"]], ["Self-Stabilizing Wavelets and r-Hops Coordination", "We introduce a simple tool called the wavelet (or, r-wavelet) scheme. Wavelets deals with coordination among processes which are at most r hops away of each other. We present a selfstabilizing solution for this scheme. Our solution requires no underlying structure and works in arbritrary anonymous networks, i.e., no process identifier is required. Moreover, our solution works under any (even unfair) daemon. Next, we use the wavelet scheme to design self-stabilizing layer clocks. We show that they provide an efficient device in the design of local coordination problems at distance r, i.e., r-barrier synchronization and r-local resource allocation (LRA) such as r-local mutual exclusion (LME), r-group mutual exclusion (GME), and r-Reader/Writers. Some solutions to the r-LRA problem (e.g., r-LME) also provide transformers to transform algorithms written assuming any r-central daemon into algorithms working with any distributed daemon.", ["Mutual exclusion", "Process identifier", "Daemon (computing)", "Algorithm", "Hops", "Resource allocation"]], ["Encounter-based worms: Analysis and Defense", "Encounter-based network is a frequently-disconnected wireless ad-hoc network requiring immediate neighbors to store and forward aggregated data for information disseminations. Using traditional approaches such as gateways or firewalls for deterring worm propagation in encounter-based networks is inappropriate. We propose the worm interaction approach that relies upon automated beneficial worm generation aiming to alleviate problems of worm propagations in such networks. To understand the dynamic of worm interactions and its performance, we mathematically model worm interactions based on major worm interaction factors including worm interaction types, network characteristics, and node characteristics using ordinary differential equations and analyze their effects on our proposed metrics. We validate our proposed model using extensive synthetic and trace-driven simulations. We find that, all worm interaction factors significantly affect the pattern of worm propagations. For example, immunization linearly decreases the infection of susceptible nodes while on-off behavior only impacts the duration of infection. Using realistic mobile network measurements, we find that encounters are bursty, multi-group and non-uniform. The trends from the trace-driven simulations are consistent with the model, in general. Immunization and timely deployment seem to be the most effective to counter the worm attacks in such scenarios while cooperation may help in a specific case. These findings provide insight that we hope would aid to develop counter-worm protocols in future encounter-based networks.", ["Network traffic measurement", "Firewall (computing)", "Wireless ad hoc network", "Store and forward", "Wireless", "Computer network", "Ad hoc", "Cellular network", "Communications protocol", "Data"]], ["Scheduling multiple divisible loads on a linear processor network", "Min, Veeravalli, and Barlas have recently proposed strategies to minimize the overall execution time of one or several divisible loads on a heterogeneous linear network, using one or more installments. We show on a very simple example that their approach does not always produce a solution and that, when it does, the solution is often suboptimal. We also show how to find an optimal schedule for any instance, once the number of installments per load is given. Then, we formally state that any optimal schedule has an infinite number of installments under a linear cost model as the one assumed in the original papers. Therefore, such a cost model cannot be used to design practical multi-installment strategies. Finally, through extensive simulations we confirmed that the best solution is always produced by the linear programming approach, while solutions of the original papers can be far away from the optimal.", ["Linear programming", "Homogeneity and heterogeneity", "Barlas", "Central processing unit", "Simulation"]], ["PSPACE Bounds for Rank-1 Modal Logics", "For lack of general algorithmic methods that apply to wide classes of logics, establishing a complexity bound for a given modal logic is often a laborious task. The present work is a step towards a general theory of the complexity of modal logics. Our main result is that all rank-1 logics enjoy a shallow model property and thus are, under mild assumptions on the format of their axiomatisation, in PSPACE. This leads to a unified derivation of tight PSPACE-bounds for a number of logics including K, KD, coalition logic, graded modal logic, majority logic, and probabilistic modal logic. Our generic algorithm moreover finds tableau proofs that witness pleasant proof-theoretic properties including a weak subformula property. This generality is made possible by a coalgebraic semantics, which conveniently abstracts from the details of a given model class and thus allows covering a broad range of logics in a uniform way.", ["Modal logic", "PSPACE", "Proof theory", "Semantics", "Axiomatic system", "Mathematical proof", "Logic", "Algorithm", "Well-formed formula", "Theory", "Variety (universal algebra)"]], ["Getting More From Your Multicore: Exploiting OpenMP From An Open Source Numerical Scripting Language", "We introduce SLIRP, a module generator for the S-Lang numerical scripting language, with a focus on its vectorization capabilities. We demonstrate how both SLIRP and S-Lang were easily adapted to exploit the inherent parallelism of high-level mathematical languages with OpenMP, allowing general users to employ tightly-coupled multiprocessors in scriptable research calculations while requiring no special knowledge of parallel programming. Motivated by examples in the ISIS astrophysical modeling & analysis tool, performance figures are presented for several machine and compiler configurations, demonstrating beneficial speedups for real-world operations.", ["S-Lang (programming library)", "Open source", "OpenMP", "Coupling (computer programming)", "Scripting language", "Astrophysics", "Mathematics", "Multiprocessing", "Parallel computing", "Compiler"]], ["Some Quantitative Aspects of Fractional Computability", "Motivated by results on generic-case complexity in group theory, we apply the ideas of effective Baire category and effective measure theory to study complexity classes of functions which are \"fractionally computable\" by a partial algorithm. For this purpose it is crucial to specify an allowable effective density, $\\delta$, of convergence for a partial algorithm. The set $\\mathcal{FC}(\\delta)$ consists of all total functions $ f: \\Sigma^\\ast \\to \\{0,1 \\}$ where $\\Sigma$ is a finite alphabet with $|\\Sigma| \\ge 2$ which are \"fractionally computable at density $\\delta$\". The space $\\mathcal{FC}(\\delta) $ is effectively of the second category while any fractional complexity class, defined using $\\delta$ and any computable bound $\\beta$ with respect to an abstract Blum complexity measure, is effectively meager. A remarkable result of Kautz and Miltersen shows that relative to an algorithmically random oracle $A$, the relativized class $\\mathcal{NP}^A$ does not have effective polynomial measure zero in $\\mathcal{E}^A$, the relativization of strict exponential time. We define the class $\\mathcal{UFP}^A$ of all languages which are fractionally decidable in polynomial time at ``a uniform rate'' by algorithms with an oracle for $A$. We show that this class does have effective polynomial measure zero in $\\mathcal{E}^A$ for every oracle $A$. Thus relaxing the requirement of polynomial time decidability to hold only for a fraction of possible inputs does not compensate for the power of nondeterminism in the case of random oracles.", ["Correctness (computer science)", "Complexity class", "Blum axioms", "Time complexity", "Random oracle", "NP (complexity)", "Computability", "Polynomial time", "Algorithm", "Polynomial", "Decidability (logic)", "Baire space", "Finite set", "Null set", "Nondeterministic algorithm", "Algorithmically random sequence", "Group theory", "Measure (mathematics)", "Complexity", "Oracle machine", "Randomness", "Oracle", "Alphabet"]], ["Radix Sorting With No Extra Space", "It is well known that n integers in the range [1,n^c] can be sorted in O(n) time in the RAM model using radix sorting. More generally, integers in any range [1,U] can be sorted in O(n sqrt{loglog n}) time. However, these algorithms use O(n) words of extra memory. Is this necessary? We present a simple, stable, integer sorting algorithm for words of size O(log n), which works in O(n) time and uses only O(1) words of extra memory on a RAM model. This is the integer sorting case most useful in practice. We extend this result with same bounds to the case when the keys are read-only, which is of theoretical interest. Another interesting question is the case of arbitrary c. Here we present a black-box transformation from any RAM sorting algorithm to a sorting algorithm which uses only O(1) extra space and has the same running time. This settles the complexity of in-place sorting in terms of the complexity of sorting.", ["Sorting algorithm", "Random access machine", "Algorithm", "Black box", "Radix", "Integer", "Random-access memory"]], ["The Domino Problem of the Hyperbolic Plane Is Undecidable", "In this paper, we prove that the general tiling problem of the hyperbolic plane is undecidable by proving a slightly stronger version using only a regular polygon as the basic shape of the tiles. The problem was raised by a paper of Raphael Robinson in 1971, in his famous simplified proof that the general tiling problem is undecidable for the Euclidean plane, initially proved by Robert Berger in 1966.", ["Regular polygon", "Euclidean geometry", "Aperiodic tiling", "Euclidean space", "Raphael M. Robinson", "Polygon", "Hyperbolic geometry", "Tessellation", "Plane (geometry)", "Decision problem"]], ["Hilbert++ Manual", "We present here an installation guide, a hand-on mini-tutorial through examples, and the theoretical foundations of the Hilbert++ code.", ["hilbert"]], ["Heuristics for Network Coding in Wireless Networks", "Multicast is a central challenge for emerging multi-hop wireless architectures such as wireless mesh networks, because of its substantial cost in terms of bandwidth. In this report, we study one specific case of multicast: broadcasting, sending data from one source to all nodes, in a multi-hop wireless network. The broadcast we focus on is based on network coding, a promising avenue for reducing cost; previous work of ours showed that the performance of network coding with simple heuristics is asymptotically optimal: each transmission is beneficial to nearly every receiver. This is for homogenous and large networks of the plan. But for small, sparse or for inhomogeneous networks, some additional heuristics are required. This report proposes such additional new heuristics (for selecting rates) for broadcasting with network coding. Our heuristics are intended to use only simple local topology information. We detail the logic of the heuristics, and with experimental results, we illustrate the behavior of the heuristics, and demonstrate their excellent performance.", ["Wireless network", "Multicast", "Wireless", "Topology", "Logic", "Wireless mesh network", "Mesh networking", "Heuristic", "Transmission (telecommunications)", "Forward error correction", "Computer network", "Node (networking)", "Asymptotically optimal algorithm"]], ["User driven applications - new design paradigm", "Programs for complicated engineering and scientific tasks always have to deal with a problem of showing numerous graphical results. The limits of the screen space and often opposite requirements from different users are the cause of the infinite discussions between designers and users, but the source of this ongoing conflict is not in the level of interface design, but in the basic principle of current graphical output: user may change some views and details, but in general the output view is absolutely defined and fixed by the developer. Author was working for several years on the algorithm that will allow eliminating this problem thus allowing stepping from designer-driven applications to user-driven. Such type of applications in which user is deciding what, when and how to show on the screen, is the dream of scientists and engineers working on the analysis of the most complicated tasks. The new paradigm is based on movable and resizable graphics, and such type of graphics can be widely used not only for scientific and engineering applications.", ["Algorithm", "Science", "Design paradigm"]], ["Unison as a Self-Stabilizing Wave Stream Algorithm in Asynchronous Anonymous Networks", "How to pass from local to global scales in anonymous networks? How to organize a selfstabilizing propagation of information with feedback. From the Angluin impossibility results, we cannot elect a leader in a general anonymous network. Thus, it is impossible to build a rooted spanning tree. Many problems can only be solved by probabilistic methods. In this paper we show how to use Unison to design a self-stabilizing barrier synchronization in an anonymous network. We show that the commuication structure of this barrier synchronization designs a self-stabilizing wave-stream, or pipelining wave, in anonymous networks. We introduce two variants of Wave: the strong waves and the wavelets. A strong wave can be used to solve the idempotent r-operator parametrized computation problem. A wavelet deals with k-distance computation. We show how to use Unison to design a self-stabilizing wave stream, a self-stabilizing strong wave stream and a self-stabilizing wavelet stream.", ["Idempotence", "Pipeline (computing)", "Wavelet", "Spanning tree", "Feedback", "Computation", "Anonymous P2P"]], ["Theory of Finite or Infinite Trees Revisited", "We present in this paper a first-order axiomatization of an extended theory $T$ of finite or infinite trees, built on a signature containing an infinite set of function symbols and a relation $\\fini(t)$ which enables to distinguish between finite or infinite trees. We show that $T$ has at least one model and prove its completeness by giving not only a decision procedure, but a full first-order constraint solver which gives clear and explicit solutions for any first-order constraint satisfaction problem in $T$. The solver is given in the form of 16 rewriting rules which transform any first-order constraint $\\phi$ into an equivalent disjunction $\\phi$ of simple formulas such that $\\phi$ is either the formula $\\true$ or the formula $\\false$ or a formula having at least one free variable, being equivalent neither to $\\true$ nor to $\\false$ and where the solutions of the free variables are expressed in a clear and explicit way. The correctness of our rules implies the completeness of $T$. We also describe an implementation of our algorithm in CHR (Constraint Handling Rules) and compare the performance with an implementation in C++ and that of a recent decision procedure for decomposable theories.", ["Constraint satisfaction problem", "Function (mathematics)", "Constraint programming", "Infinite set", "Signature (logic)", "Free variables and bound variables", "Logical disjunction", "First-order logic", "Constraint satisfaction", "Algorithm", "Decision problem", "Axiomatic system", "Finite set", "Completeness", "Model theory", "Variable (mathematics)", "Infinity", "Formula", "Phi"]], ["A Robust Linguistic Platform for Efficient and Domain specific Web Content Analysis", "Web semantic access in specific domains calls for specialized search engines with enhanced semantic querying and indexing capacities, which pertain both to information retrieval (IR) and to information extraction (IE). A rich linguistic analysis is required either to identify the relevant semantic units to index and weight them according to linguistic specific statistical distribution, or as the basis of an information extraction process. Recent developments make Natural Language Processing (NLP) techniques reliable enough to process large collections of documents and to enrich them with semantic annotations. This paper focuses on the design and the development of a text processing platform, Ogmios, which has been developed in the ALVIS project. The Ogmios platform exploits existing NLP modules and resources, which may be tuned to specific domains and produces linguistically annotated documents. We show how the three constraints of genericity, domain semantic awareness and performance can be handled all together.", ["Information extraction", "Information retrieval", "Natural language processing", "Web search engine", "Ogmios", "Linguistics", "Statistics", "Linguistic description", "Semantics", "Generic programming", "Word processing", "Probability distribution", "World Wide Web"]], ["2-State 3-Symbol Universal Turing Machines Do Not Exist", "In this brief note, we give a simple information-theoretic proof that 2-state 3-symbol universal Turing machines cannot possibly exist, unless one loosens the definition of \"universal\".", ["Information theory", "Symbol"]], ["Non-atomic Games for Multi-User Systems", "In this contribution, the performance of a multi-user system is analyzed in the context of frequency selective fading channels. Using game theoretic tools, a useful framework is provided in order to determine the optimal power allocation when users know only their own channel (while perfect channel state information is assumed at the base station). We consider the realistic case of frequency selective channels for uplink CDMA. This scenario illustrates the case of decentralized schemes, where limited information on the network is available at the terminal. Various receivers are considered, namely the Matched filter, the MMSE filter and the optimum filter. The goal of this paper is to derive simple expressions for the non-cooperative Nash equilibrium as the number of mobiles becomes large and the spreading length increases. To that end two asymptotic methodologies are combined. The first is asymptotic random matrix theory which allows us to obtain explicit expressions of the impact of all other mobiles on any given tagged mobile. The second is the theory of non-atomic games which computes good approximations of the Nash equilibrium as the number of mobiles grows.", ["Matched filter", "Channel state information", "Nash equilibrium", "Minimum mean square error", "Random matrix", "Frequency", "Matrix (mathematics)", "Code division multiple access", "Game theory", "Uplink", "Base station"]], ["Location and Spectral Estimation of Weak Wave Packets on Noise Background", "The method of location and spectral estimation of weak signals on a noise background is being considered. The method is based on the optimized on order and noise dispersion autoregressive model of a sought signal. A new approach of model order determination is being offered. Available estimation of the noise dispersion is close to the real one. The optimized model allows to define function of empirical data spectral and dynamic features changes. The analysis of the signal as dynamic invariant in respect of the linear shift transformation yields the function of model consistency. Use of these both functions enables to detect short-time and nonstationary wave packets at signal to noise ratio as from -20 dB and above.", ["Signal-to-noise ratio", "Decibel", "Autoregressive model", "Wave", "Signal (electronics)", "Noise", "Spectral density estimation", "Data", "Network packet", "Empirical"]], ["Selection Relaying at Low Signal to Noise Ratios", "Performance of cooperative diversity schemes at Low Signal to Noise Ratios (LSNR) was recently studied by Avestimehr et. al. [1] who emphasized the importance of diversity gain over multiplexing gain at low SNRs. It has also been pointed out that continuous energy transfer to the channel is necessary for achieving the max-flow min-cut bound at LSNR. Motivated by this we propose the use of Selection Decode and Forward (SDF) at LSNR and analyze its performance in terms of the outage probability. We also propose an energy optimization scheme which further brings down the outage probability.", ["Mathematical optimization", "Cooperative diversity", "Multiplexing", "Energy", "Probability", "Signal-to-noise ratio", "Max-flow min-cut theorem"]], ["Directed Feedback Vertex Set is Fixed-Parameter Tractable", "We resolve positively a long standing open question regarding the fixed-parameter tractability of the parameterized Directed Feedback Vertex Set problem. In particular, we propose an algorithm which solves this problem in $O(8^kk!*poly(n))$.", ["Parameter", "Feedback vertex set", "Operation Tractable", "Feedback", "Algorithm"]], ["A Generalized Sampling Theorem for Frequency Localized Signals", "A generalized sampling theorem for frequency localized signals is presented. The generalization in the proposed model of sampling is twofold: (1) It applies to various prefilters effecting a \"soft\" bandlimitation, (2) an approximate reconstruction from sample values rather than a perfect one is obtained (though the former might be \"practically perfect\" in many cases). For an arbitrary finite-energy signal the frequency localization is performed by a prefilter realizing a crosscorrelation with a function of prescribed properties. The range of the filter, the so-called localization space, is described in some detail. Regular sampling is applied and a reconstruction formula is given. For the reconstruction error a general error estimate is derived and connections between a critical sampling interval and notions of \"soft bandwidth\" for the prefilter are indicated. Examples based on the sinc-function, Gaussian functions and B-splines are discussed.", ["Sampling theorem", "Cross-correlation", "Frequency", "Sinc function", "Sampling (signal processing)", "Signal (electronics)", "Energy", "Sampling rate"]], ["Interference Alignment and the Degrees of Freedom for the K User Interference Channel", "While the best known outerbound for the K user interference channel states that there cannot be more than K/2 degrees of freedom, it has been conjectured that in general the constant interference channel with any number of users has only one degree of freedom. In this paper, we explore the spatial degrees of freedom per orthogonal time and frequency dimension for the K user wireless interference channel where the channel coefficients take distinct values across frequency slots but are fixed in time. We answer five closely related questions. First, we show that K/2 degrees of freedom can be achieved by channel design, i.e. if the nodes are allowed to choose the best constant, finite and nonzero channel coefficient values. Second, we show that if channel coefficients can not be controlled by the nodes but are selected by nature, i.e., randomly drawn from a continuous distribution, the total number of spatial degrees of freedom for the K user interference channel is almost surely K/2 per orthogonal time and frequency dimension. Thus, only half the spatial degrees of freedom are lost due to distributed processing of transmitted and received signals on the interference channel. Third, we show that interference alignment and zero forcing suffice to achieve all the degrees of freedom in all cases. Fourth, we show that the degrees of freedom $D$ directly lead to an $\\mathcal{O}(1)$ capacity characterization of the form $C(SNR)=D\\log(1+SNR)+\\mathcal{O}(1)$ for the multiple access channel, the broadcast channel, the 2 user interference channel, the 2 user MIMO X channel and the 3 user interference channel with M>1 antennas at each node. Fifth, we characterize the degree of freedom benefits from cognitive sharing of messages on the 3 user interference channel.", ["Alignment (political party)", "Degrees of freedom (mechanics)", "Frequency", "MIMO", "Orthogonality", "Almost surely", "Degrees of freedom (physics and chemistry)", "Wireless", "Antenna (radio)", "Cognition", "Distributed computing", "Coefficient", "Channel access method", "Signal-to-noise ratio", "Dimension"]], ["Pricing Options on Defaultable Stocks", "In this note, we develop stock option price approximations for a model which takes both the risk o default and the stochastic volatility into account. We also let the intensity of defaults be influenced by the volatility. We show that it might be possible to infer the risk neutral default intensity from the stock option prices. Our option price approximation has a rich implied volatility surface structure and fits the data implied volatility well. Our calibration exercise shows that an effective hazard rate from bonds issued by a company can be used to explain the implied volatility skew of the implied volatility of the option prices issued by the same company.", ["Option (finance)", "Volatility smile", "Stochastic volatility", "Risk neutral", "Implied volatility", "Volatility (finance)", "Bond (finance)", "Stochastic", "Default (finance)", "Hazard rate", "Stock", "Risk"]], ["Performance Analysis of Publish/Subscribe Systems", "The Desktop Grid offers solutions to overcome several challenges and to answer increasingly needs of scientific computing. Its technology consists mainly in exploiting resources, geographically dispersed, to treat complex applications needing big power of calculation and/or important storage capacity. However, as resources number increases, the need for scalability, self-organisation, dynamic reconfigurations, decentralisation and performance becomes more and more essential. Since such properties are exhibited by P2P systems, the convergence of grid computing and P2P computing seems natural. In this context, this paper evaluates the scalability and performance of P2P tools for discovering and registering services. Three protocols are used for this purpose: Bonjour, Avahi and Free-Pastry. We have studied the behaviour of theses protocols related to two criteria: the elapsed time for registrations services and the needed time to discover new services. Our aim is to analyse these results in order to choose the best protocol we can use in order to create a decentralised middleware for desktop grid.", ["Grid computing", "Computational science", "Thesis", "Middleware", "Decentralization", "Self-organization", "Technology"]], ["Robust Audio Watermarking Against the D/A and A/D conversions", "Audio watermarking has played an important role in multimedia security. In many applications using audio watermarking, D/A and A/D conversions (denoted by DA/AD in this paper) are often involved. In previous works, however, the robustness issue of audio watermarking against the DA/AD conversions has not drawn sufficient attention yet. In our extensive investigation, it has been found that the degradation of a watermarked audio signal caused by the DA/AD conversions manifests itself mainly in terms of wave magnitude distortion and linear temporal scaling, making the watermark extraction failed. Accordingly, a DWT-based audio watermarking algorithm robust against the DA/AD conversions is proposed in this paper. To resist the magnitude distortion, the relative energy relationships among different groups of the DWT coefficients in the low-frequency sub-band are utilized in watermark embedding by adaptively controlling the embedding strength. Furthermore, the resynchronization is designed to cope with the linear temporal scaling. The time-frequency localization characteristics of DWT are exploited to save the computational load in the resynchronization. Consequently, the proposed audio watermarking algorithm is robust against the DA/AD conversions, other common audio processing manipulations, and the attacks in StirMark Benchmark for Audio, which has been verified by experiments.", ["Digital watermarking", "Frequency", "Multimedia", "Analog-to-digital converter", "Audio signal processing", "Audio signal", "Deadweight tonnage", "Watermark", "Algorithm", "Energy", "Digital-to-analog converter", "Linear", "Distortion"]], ["The $k$-anonymity Problem is Hard", "The problem of publishing personal data without giving up privacy is becoming increasingly important. An interesting formalization recently proposed is the k-anonymity. This approach requires that the rows in a table are clustered in sets of size at least k and that all the rows in a cluster become the same tuple, after the suppression of some records. The natural optimization problem, where the goal is to minimize the number of suppressed entries, is known to be NP-hard when the values are over a ternary alphabet, k = 3 and the rows length is unbounded. In this paper we give a lower bound on the approximation factor that any polynomial-time algorithm can achive on two restrictions of the problem,namely (i) when the records values are over a binary alphabet and k = 3, and (ii) when the records have length at most 8 and k = 4, showing that these restrictions of the problem are APX-hard.", ["NP-hard", "APX", "NP (complexity)", "Tuple", "Upper and lower bounds", "Algorithm", "Mathematical optimization", "Time complexity", "Polynomial", "Optimization problem", "Alphabet", "Binary numeral system"]], ["Assisted Problem Solving and Decompositions of Finite Automata", "A study of assisted problem solving formalized via decompositions of deterministic finite automata is initiated. The landscape of new types of decompositions of finite automata this study uncovered is presented. Languages with various degrees of decomposability between undecomposable and perfectly decomposable are shown to exist.", ["Problem solving", "Finite-state machine", "Automaton", "Deterministic finite-state machine"]], ["Optimal Strategies for Gaussian Jamming in Block-Fading Channels under Delay and Power Constraints", "Without assuming any knowledge on source's codebook and its output signals, we formulate a Gaussian jamming problem in block fading channels as a two-player zero sum game. The outage probability is adopted as an objective function, over which transmitter aims at minimization and jammer aims at maximization by selecting their power control strategies. Optimal power control strategies for each player are obtained under both short-term and long-term power constraints. For the latter case, we first prove the non-existence of a Nash equilibrium, and then provide a complete solution for both maxmin and minimax problems. Numerical results demonstrate a sharp difference between the outage probabilities of the minimax and maxmin solutions.", ["Probability", "Nash equilibrium", "Codebook", "Mathematical optimization", "Transmitter", "Function (mathematics)"]], ["Physical Network Coding in Two-Way Wireless Relay Channels", "It has recently been recognized that the wireless networks represent a fertile ground for devising communication modes based on network coding. A particularly suitable application of the network coding arises for the two--way relay channels, where two nodes communicate with each other assisted by using a third, relay node. Such a scenario enables application of \\emph{physical network coding}, where the network coding is either done (a) jointly with the channel coding or (b) through physical combining of the communication flows over the multiple access channel. In this paper we first group the existing schemes for physical network coding into two generic schemes, termed 3--step and 2--step scheme, respectively. We investigate the conditions for maximization of the two--way rate for each individual scheme: (1) the Decode--and--Forward (DF) 3--step schemes (2) three different schemes with two steps: Amplify--and--Forward (AF), JDF and Denoise--and--Forward (DNF). While the DNF scheme has a potential to offer the best two--way rate, the most interesting result of the paper is that, for some SNR configurations of the source--relay links, JDF yields identical maximal two--way rate as the upper bound on the rate for DNF.", ["Communication", "Wireless network", "Wireless", "Forward error correction", "Channel access method", "Computer network", "Node (networking)", "Channel (communications)", "Signal-to-noise ratio", "Modulation"]], ["Blind Estimation of Multiple Carrier Frequency Offsets", "Multiple carrier-frequency offsets (CFO) arise in a distributed antenna system, where data are transmitted simultaneously from multiple antennas. In such systems the received signal contains multiple CFOs due to mismatch between the local oscillators of transmitters and receiver. This results in a time-varying rotation of the data constellation, which needs to be compensated for at the receiver before symbol recovery. This paper proposes a new approach for blind CFO estimation and symbol recovery. The received base-band signal is over-sampled, and its polyphase components are used to formulate a virtual Multiple-Input Multiple-Output (MIMO) problem. By applying blind MIMO system estimation techniques, the system response is estimated and used to subsequently transform the multiple CFOs estimation problem into many independent single CFO estimation problems. Furthermore, an initial estimate of the CFO is obtained from the phase of the MIMO system response. The Cramer-Rao Lower bound is also derived, and the large sample performance of the proposed estimator is compared to the bound.", ["Antenna (radio)", "Phase (waves)", "Frequency", "Constellation", "MIMO", "Receiver (radio)", "Estimator", "Carrier wave", "Polyphase system", "Transmitter"]], ["Fractional Power Control for Decentralized Wireless Networks", "We consider a new approach to power control in decentralized wireless networks, termed fractional power control (FPC). Transmission power is chosen as the current channel quality raised to an exponent -s, where s is a constant between 0 and 1. The choices s = 1 and s = 0 correspond to the familiar cases of channel inversion and constant power transmission, respectively. Choosing s in (0,1) allows all intermediate policies between these two extremes to be evaluated, and we see that usually neither extreme is ideal. We derive closed-form approximations for the outage probability relative to a target SINR in a decentralized (ad hoc or unlicensed) network as well as for the resulting transmission capacity, which is the number of users/m^2 that can achieve this SINR on average. Using these approximations, which are quite accurate over typical system parameter values, we prove that using an exponent of 1/2 minimizes the outage probability, meaning that the inverse square root of the channel strength is a sensible transmit power scaling for networks with a relatively low density of interferers. We also show numerically that this choice of s is robust to a wide range of variations in the network parameters. Intuitively, s=1/2 balances between helping disadvantaged users while making sure they do not flood the network with interference.", ["Wireless network", "Wireless", "Square root", "Ad hoc", "Inverse-square law", "Exponentiation", "Power control", "Power scaling", "Probability"]], ["Precoding for the AWGN Channel with Discrete Interference", "For a state-dependent DMC with input alphabet $\\mathcal{X}$ and state alphabet $\\mathcal{S}$ where the i.i.d. state sequence is known causally at the transmitter, it is shown that by using at most $|\\mathcal{X}||\\mathcal{S}|-|\\mathcal{S}|+1$ out of $|\\mathcal{X}|^{|\\mathcal{S}|}$ input symbols of the Shannon's \\emph{associated} channel, the capacity is achievable. As an example of state-dependent channels with side information at the transmitter, $M$-ary signal transmission over AWGN channel with additive $Q$-ary interference where the sequence of i.i.d. interference symbols is known causally at the transmitter is considered. For the special case where the Gaussian noise power is zero, a sufficient condition, which is independent of interference, is given for the capacity to be $\\log_2 M$ bits per channel use. The problem of maximization of the transmission rate under the constraint that the channel input given any current interference symbol is uniformly distributed over the channel input alphabet is investigated. For this setting, the general structure of a communication system with optimal precoding is proposed.", ["Precoding", "Additive white Gaussian noise", "Claude Shannon", "Noise power", "Uniform distribution (continuous)", "Causality", "Gaussian noise", "Transmitter", "Communication", "Communications system"]], ["The Role of Time in the Creation of Knowledge", "This paper I assume that in humans the creation of knowledge depends on a discrete time, or stage, sequential decision-making process subjected to a stochastic, information transmitting environment. For each time-stage, this environment randomly transmits Shannon type information-packets to the decision-maker, who examines each of them for relevancy and then determines his optimal choices. Using this set of relevant information-packets, the decision-maker adapts, over time, to the stochastic nature of his environment, and optimizes the subjective expected rate-of-growth of knowledge. The decision-maker's optimal actions, lead to a decision function that involves, over time, his view of the subjective entropy of the environmental process and other important parameters at each time-stage of the process. Using this model of human behavior, one could create psychometric experiments using computer simulation and real decision-makers, to play programmed games to measure the resulting human performance.", ["Human behavior", "Psychometrics", "Computer simulation", "Subjectivity", "Entropy", "Decision theory", "Computer", "Stochastic", "Decision making"]], ["Location-Aided Fast Distributed Consensus in Wireless Networks", "Existing works on distributed consensus explore linear iterations based on reversible Markov chains, which contribute to the slow convergence of the algorithms. It has been observed that by overcoming the diffusive behavior of reversible chains, certain nonreversible chains lifted from reversible ones mix substantially faster than the original chains. In this paper, we investigate the idea of accelerating distributed consensus via lifting Markov chains, and propose a class of Location-Aided Distributed Averaging (LADA) algorithms for wireless networks, where nodes' coarse location information is used to construct nonreversible chains that facilitate distributed computing and cooperative processing. First, two general pseudo-algorithms are presented to illustrate the notion of distributed averaging through chain-lifting. These pseudo-algorithms are then respectively instantiated through one LADA algorithm on grid networks, and one on general wireless networks. For a $k\\times k$ grid network, the proposed LADA algorithm achieves an $\\epsilon$-averaging time of $O(k\\log(\\epsilon^{-1}))$. Based on this algorithm, in a wireless network with transmission range $r$, an $\\epsilon$-averaging time of $O(r^{-1}\\log(\\epsilon^{-1}))$ can be attained through a centralized algorithm. Subsequently, we present a fully-distributed LADA algorithm for wireless networks, which utilizes only the direction information of neighbors to construct nonreversible chains. It is shown that this distributed LADA algorithm achieves the same scaling law in averaging time as the centralized scheme. Finally, we propose a cluster-based LADA (C-LADA) algorithm, which, requiring no central coordination, provides the additional benefit of reduced message complexity compared with the distributed LADA algorithm.", ["Distributed computing", "Computing", "Consensus (computer science)", "Wireless", "Markov chain", "Wireless network", "Power law", "Algorithm", "Consensus decision-making"]], ["Phase space methods and psychoacoustic models in lossy transform coding", "I present a method for lossy transform coding of digital audio that uses the Weyl symbol calculus for constructing the encoding and decoding transformation. The method establishes a direct connection between a time-frequency representation of the signal dependent threshold of masked noise and the encode/decode pair. The formalism also offers a time-frequency measure of perceptual entropy.", ["Entropy (information theory)", "Phase space", "Perception", "Psychoacoustics", "Time-frequency representation", "Calculus", "Noise", "Lossy compression", "Entropy", "Digital audio", "Transform coding", "Signal (electronics)"]], ["Weighted Popular Matchings", "We study the problem of assigning jobs to applicants. Each applicant has a weight and provides a preference list ranking a subset of the jobs. A matching M is popular if there is no other matching M' such that the weight of the applicants who prefer M' over M exceeds the weight of those who prefer M over M'. This paper gives efficient algorithms to find a popular matching if one exists.", ["Algorithm", "Computational complexity theory"]], ["From Royal Road to Epistatic Road for Variable Length Evolution Algorithm", "Although there are some real world applications where the use of variable length representation (VLR) in Evolutionary Algorithm is natural and suitable, an academic framework is lacking for such representations. In this work we propose a family of tunable fitness landscapes based on VLR of genotypes. The fitness landscapes we propose possess a tunable degree of both neutrality and epistasis; they are inspired, on the one hand by the Royal Road fitness landscapes, and the other hand by the NK fitness landscapes. So these landscapes offer a scale of continuity from Royal Road functions, with neutrality and no epistasis, to landscapes with a large amount of epistasis and no redundancy. To gain insight into these fitness landscapes, we first use standard tools such as adaptive walks and correlation length. Second, we evaluate the performances of evolutionary algorithms on these landscapes for various values of the neutral and the epistatic parameters; the results allow us to correlate the performances with the expected degrees of neutrality and epistasis.", ["Epistasis", "Evolutionary algorithm", "Correlation and dependence", "Correlation function (statistical mechanics)", "Algorithm", "Royal Road"]], ["Determinacy in a synchronous pi-calculus", "The S-pi-calculus is a synchronous pi-calculus which is based on the SL model. The latter is a relaxation of the Esterel model where the reaction to the absence of a signal within an instant can only happen at the next instant. In the present work, we present and characterise a compositional semantics of the S-pi-calculus based on suitable notions of labelled transition system and bisimulation. Based on this semantic framework, we explore the notion of determinacy and the related one of (local) confluence.", ["State transition system", "Bisimulation", "Pi-calculus", "Calculus", "Determinacy", "Esterel", "Principle of compositionality", "Pi", "Semantics"]], ["On a Non-Context-Free Extension of PDL", "Over the last 25 years, a lot of work has been done on seeking for decidable non-regular extensions of Propositional Dynamic Logic (PDL). Only recently, an expressive extension of PDL, allowing visibly pushdown automata (VPAs) as a formalism to describe programs, was introduced and proven to have a satisfiability problem complete for deterministic double exponential time. Lately, the VPA formalism was extended to so called k-phase multi-stack visibly pushdown automata (k-MVPAs). Similarly to VPAs, it has been shown that the language of k-MVPAs have desirable effective closure properties and that the emptiness problem is decidable. On the occasion of introducing k-MVPAs, it has been asked whether the extension of PDL with k-MVPAs still leads to a decidable logic. This question is answered negatively here. We prove that already for the extension of PDL with 2-phase MVPAs with two stacks satisfiability becomes \\Sigma_1^1-complete.", ["Many-one reduction", "Pushdown automaton", "Time complexity", "Formal system", "Double exponential function", "Automaton", "Satisfiability", "Decidability (logic)", "Closure (mathematics)", "Dynamic logic (modal logic)", "Logic"]], ["Optimal Linear Precoding Strategies for Wideband Non-Cooperative Systems based on Game Theory-Part I: Nash Equilibria", "In this two-parts paper we propose a decentralized strategy, based on a game-theoretic formulation, to find out the optimal precoding/multiplexing matrices for a multipoint-to-multipoint communication system composed of a set of wideband links sharing the same physical resources, i.e., time and bandwidth. We assume, as optimality criterion, the achievement of a Nash equilibrium and consider two alternative optimization problems: 1) the competitive maximization of mutual information on each link, given constraints on the transmit power and on the spectral mask imposed by the radio spectrum regulatory bodies; and 2) the competitive maximization of the transmission rate, using finite order constellations, under the same constraints as above, plus a constraint on the average error probability. In Part I of the paper, we start by showing that the solution set of both noncooperative games is always nonempty and contains only pure strategies. Then, we prove that the optimal precoding/multiplexing scheme for both games leads to a channel diagonalizing structure, so that both matrix-valued problems can be recast in a simpler unified vector power control game, with no performance penalty. Thus, we study this simpler game and derive sufficient conditions ensuring the uniqueness of the Nash equilibrium. Interestingly, although derived under stronger constraints, incorporating for example spectral mask constraints, our uniqueness conditions have broader validity than previously known conditions. Finally, we assess the goodness of the proposed decentralized strategy by comparing its performance with the performance of a Pareto-optimal centralized scheme. To reach the Nash equilibria of the game, in Part II, we propose alternative distributed algorithms, along with their convergence conditions.", ["Precoding", "Mathematical optimization", "Mutual information", "Nash equilibrium", "Pareto efficiency", "Multiplexing", "Matrix (mathematics)", "Empty set", "Pure strategy", "Game theory", "Algorithm", "Probability", "Spectral mask", "Communication", "Radio", "Radio spectrum", "Decentralization", "Limit of a sequence"]], ["Unfolding Orthogonal Terrains", "It is shown that every orthogonal terrain, i.e., an orthogonal (right-angled) polyhedron based on a rectangle that meets every vertical line in a segment, has a grid unfolding: its surface may be unfolded to a single non-overlapping piece by cutting along grid edges defined by coordinate planes through every vertex.", ["Polyhedron"]], ["Where are Bottlenecks in NK Fitness Landscapes?", "Usually the offspring-parent fitness correlation is used to visualize and analyze some caracteristics of fitness landscapes such as evolvability. In this paper, we introduce a more general representation of this correlation, the Fitness Cloud (FC). We use the bottleneck metaphor to emphasise fitness levels in landscape that cause local search process to slow down. For a local search heuristic such as hill-climbing or simulated annealing, FC allows to visualize bottleneck and neutrality of landscapes. To confirm the relevance of the FC representation we show where the bottlenecks are in the well-know NK fitness landscape and also how to use neutrality information from the FC to combine some neutral operator with local search heuristic.", ["Fitness landscape", "Evolvability", "Correlation and dependence", "Simulated annealing", "Heuristic", "Annealing (metallurgy)", "Local search (optimization)", "Bottleneck"]], ["Scuba Search : when selection meets innovation", "We proposed a new search heuristic using the scuba diving metaphor. This approach is based on the concept of evolvability and tends to exploit neutrality in fitness landscape. Despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push the evolvability to increase. The search process switches between two phases: Conquest-of-the-Waters and Invasion-of-the-Land. A comparative study of the new algorithm and standard local search heuristics on the NKq-landscapes has shown advantage and limit of the scuba search. To enlighten qualitative differences between neutral search processes, the space is changed into a connected graph to visualize the pathways that the search is likely to follow.", ["Fitness landscape", "Innovation", "Connectivity (graph theory)", "Algorithm", "Scuba diving", "Metaphor", "Evolvability", "Heuristic", "Evolution"]], ["Another view of the Gaussian algorithm", "We introduce here a rewrite system in the group of unimodular matrices, \\emph{i.e.}, matrices with integer entries and with determinant equal to $\\pm 1$. We use this rewrite system to precisely characterize the mechanism of the Gaussian algorithm, that finds shortest vectors in a two--dimensional lattice given by any basis. Putting together the algorithmic of lattice reduction and the rewrite system theory, we propose a new worst--case analysis of the Gaussian algorithm. There is already an optimal worst--case bound for some variant of the Gaussian algorithm due to Vall\\'ee \\cite {ValGaussRevisit}. She used essentially geometric considerations. Our analysis generalizes her result to the case of the usual Gaussian algorithm. An interesting point in our work is its possible (but not easy) generalization to the same problem in higher dimensions, in order to exhibit a tight upper-bound for the number of iterations of LLL--like reduction algorithms in the worst case. Moreover, our method seems to work for analyzing other families of algorithms. As an illustration, the analysis of sorting algorithms are briefly developed in the last section of the paper.", ["Matrix (mathematics)", "Determinant", "Case analysis", "Integer", "Rewriting", "Systems theory", "Algorithm", "Unimodular matrix", "Haar measure", "Best, worst and average case"]], ["Dial a Ride from k-forest", "The k-forest problem is a common generalization of both the k-MST and the dense-$k$-subgraph problems. Formally, given a metric space on $n$ vertices $V$, with $m$ demand pairs $\\subseteq V \\times V$ and a ``target'' $k\\le m$, the goal is to find a minimum cost subgraph that connects at least $k$ demand pairs. In this paper, we give an $O(\\min\\{\\sqrt{n},\\sqrt{k}\\})$-approximation algorithm for $k$-forest, improving on the previous best ratio of $O(n^{2/3}\\log n)$ by Segev & Segev. We then apply our algorithm for k-forest to obtain approximation algorithms for several Dial-a-Ride problems. The basic Dial-a-Ride problem is the following: given an $n$ point metric space with $m$ objects each with its own source and destination, and a vehicle capable of carrying at most $k$ objects at any time, find the minimum length tour that uses this vehicle to move each object from its source to destination. We prove that an $\\alpha$-approximation algorithm for the $k$-forest problem implies an $O(\\alpha\\cdot\\log^2n)$-approximation algorithm for Dial-a-Ride. Using our results for $k$-forest, we get an $O(\\min\\{\\sqrt{n},\\sqrt{k}\\}\\cdot\\log^2 n)$- approximation algorithm for Dial-a-Ride. The only previous result known for Dial-a-Ride was an $O(\\sqrt{k}\\log n)$-approximation by Charikar & Raghavachari; our results give a different proof of a similar approximation guarantee--in fact, when the vehicle capacity $k$ is large, we give a slight improvement on their results.", ["Metric space", "Approximation algorithm", "Subgraph", "Natural logarithm", "Algorithm", "Charikar", "Vertex (geometry)"]], ["Sphere Lower Bound for Rotated Lattice Constellations in Fading Channels", "We study the error probability performance of rotated lattice constellations in frequency-flat Nakagami-$m$ block-fading channels. In particular, we use the sphere lower bound on the underlying infinite lattice as a performance benchmark. We show that the sphere lower bound has full diversity. We observe that optimally rotated lattices with largest known minimum product distance perform very close to the lower bound, while the ensemble of random rotations is shown to lack diversity and perform far from it.", ["Frequency", "Sphere", "Upper and lower bounds"]], ["How to use the Scuba Diving metaphor to solve problem with neutrality ?", "We proposed a new search heuristic using the scuba diving metaphor. This approach is based on the concept of evolvability and tends to exploit neutrality which exists in many real-world problems. Despite the fact that natural evolution does not directly select for evolvability, the basic idea behind the scuba search heuristic is to explicitly push evolvability to increase. A comparative study of the scuba algorithm and standard local search heuristics has shown the advantage and the limitation of the scuba search. In order to tune neutrality, we use the NKq fitness landscapes and a family of travelling salesman problems (TSP) where cities are randomly placed on a lattice and where travel distance between cities is computed with the Manhattan metric. In this last problem the amount of neutrality varies with the city concentration on the grid ; assuming the concentration below one, this TSP reasonably remains a NP-hard problem.", ["Evolution", "Scuba diving", "Evolvability", "Fitness landscape", "Taxicab geometry", "NP-hard", "Metaphor", "Diving", "Algorithm", "Manhattan"]], ["Clustering and Feature Selection using Sparse Principal Component Analysis", "In this paper, we study the application of sparse principal component analysis (PCA) to clustering and feature selection problems. Sparse PCA seeks sparse factors, or linear combinations of the data variables, explaining a maximum amount of variance in the data while having only a limited number of nonzero coefficients. PCA is often used as a simple clustering technique and sparse factors allow us here to interpret the clusters in terms of a reduced set of variables. We begin with a brief introduction and motivation on sparse PCA and detail our implementation of the algorithm in d'Aspremont et al. (2005). We then apply these results to some classic clustering and feature selection problems arising in biology.", ["Principal component analysis", "Feature selection", "Cluster analysis", "Linear combination", "Variance", "Biology", "Variable (mathematics)"]], ["Model Selection Through Sparse Maximum Likelihood Estimation", "We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added l_1-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive l_1-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright & Jordan (2006)), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.", ["Gene", "Determinant", "Partition function (statistical mechanics)", "Maximum likelihood", "Graphical model", "Interior point method", "Normal distribution", "Algorithm", "Analysis of algorithms", "Gene expression", "Recursion"]], ["Optimal Solutions for Sparse Principal Component Analysis", "Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefficients, with total complexity O(n^3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n^3) per pattern. We discuss applications in subset selection and sparse recovery and show on artificial examples and biological data that our algorithm does provide globally optimal solutions in many cases.", ["Principal component analysis", "Covariance matrix", "Engineering", "Subset", "Linear combination", "Machine learning", "Covariance", "Variance", "Matrix (mathematics)", "Global optimization", "Algorithm", "Greedy algorithm", "Variable (mathematics)", "Sample mean and sample covariance", "Linear", "Mathematical optimization", "Coefficient"]], ["Workspace Analysis of the Parallel Module of the VERNE Machine", "The paper addresses geometric aspects of a spatial three-degree-of-freedom parallel module, which is the parallel module of a hybrid serial-parallel 5-axis machine tool. This parallel module consists of a moving platform that is connected to a fixed base by three non-identical legs. Each leg is made up of one prismatic and two pairs of spherical joint, which are connected in a way that the combined effects of the three legs lead to an over-constrained mechanism with complex motion. This motion is defined as a simultaneous combination of rotation and translation. A method for computing the complete workspace of the VERNE parallel module for various tool lengths is presented. An algorithm describing this method is also introduced.", ["Rotation", "Machine tool", "Sphere", "Geometry", "Ball joint", "Computing", "Algorithm", "Space"]], ["A Multi Interface Grid Discovery System", "Discovery Systems (DS) can be considered as entry points for global loosely coupled distributed systems. An efficient Discovery System in essence increases the performance, reliability and decision making capability of distributed systems. With the rapid increase in scale of distributed applications, existing solutions for discovery systems are fast becoming either obsolete or incapable of handling such complexity. They are particularly ineffective when handling service lifetimes and providing up-to-date information, poor at enabling dynamic service access and they can also impose unwanted restrictions on interfaces to widely available information repositories. In this paper we present essential the design characteristics, an implementation and a performance analysis for a discovery system capable of overcoming these deficiencies in large, globally distributed environments.", ["Decision making", "Profiling (computer programming)"]], ["Mobile Computing in Physics Analysis - An Indicator for eScience", "This paper presents the design and implementation of a Grid-enabled physics analysis environment for handheld and other resource-limited computing devices as one example of the use of mobile devices in eScience. Handheld devices offer great potential because they provide ubiquitous access to data and round-the-clock connectivity over wireless links. Our solution aims to provide users of handheld devices the capability to launch heavy computational tasks on computational and data Grids, monitor the jobs status during execution, and retrieve results after job completion. Users carry their jobs on their handheld devices in the form of executables (and associated libraries). Users can transparently view the status of their jobs and get back their outputs without having to know where they are being executed. In this way, our system is able to act as a high-throughput computing environment where devices ranging from powerful desktop machines to small handhelds can employ the power of the Grid. The results shown in this paper are readily applicable to the wider eScience community.", ["Mobile device", "Mobile computing", "Wireless", "Desktop computer", "Mobile phone", "Physics", "Computer", "Computer monitor", "Ubiquitous computing", "Library", "Throughput"]], ["DIANA Scheduling Hierarchies for Optimizing Bulk Job Scheduling", "The use of meta-schedulers for resource management in large-scale distributed systems often leads to a hierarchy of schedulers. In this paper, we discuss why existing meta-scheduling hierarchies are sometimes not sufficient for Grid systems due to their inability to re-organise jobs already scheduled locally. Such a job re-organisation is required to adapt to evolving loads which are common in heavily used Grid infrastructures. We propose a peer-to-peer scheduling model and evaluate it using case studies and mathematical modelling. We detail the DIANA (Data Intensive and Network Aware) scheduling algorithm and its queue management system for coping with the load distribution and for supporting bulk job scheduling. We demonstrate that such a system is beneficial for dynamic, distributed and self-organizing resource management and can assist in optimizing load or job distribution in complex Grid infrastructures.", ["Job scheduler", "Peer-to-peer", "Mathematical model", "Distributed computing", "Hierarchy", "Self-organization", "Mathematics", "Resource management", "Scheduling (computing)", "Scheduling algorithm"]], ["A process algebra based framework for promise theory", "We present a process algebra based approach to formalize the interactions of computing devices such as the representation of policies and the resolution of conflicts. As an example we specify how promises may be used in coming to an agreement regarding a simple though practical transportation problem.", ["Process calculus"]], ["Semantic Information Retrieval from Distributed Heterogeneous Data Sources", "Information retrieval from distributed heterogeneous data sources remains a challenging issue. As the number of data sources increases more intelligent retrieval techniques, focusing on information content and semantics, are required. Currently ontologies are being widely used for managing semantic knowledge, especially in the field of bioinformatics. In this paper we describe an ontology assisted system that allows users to query distributed heterogeneous data sources by hiding details like location, information structure, access pattern and semantic structure of the data. Our goal is to provide an integrated view on biomedical information sources for the Health-e-Child project with the aim to overcome the lack of sufficient semantic-based reformulation techniques for querying distributed data sources. In particular, this paper examines the problem of query reformulation across biomedical data sources, based on merged ontologies and the underlying heterogeneous descriptions of the respective data sources.", ["Information retrieval", "Ontology", "Bioinformatics", "Ontology (information science)", "Medical research", "Data", "Knowledge", "Semantics", "Data structure"]], ["Experiences of Engineering Grid-Based Medical Software", "Objectives: Grid-based technologies are emerging as potential solutions for managing and collaborating distributed resources in the biomedical domain. Few examples exist, however, of successful implementations of Grid-enabled medical systems and even fewer have been deployed for evaluation in practice. The objective of this paper is to evaluate the use in clinical practice of a Grid-based imaging prototype and to establish directions for engineering future medical Grid developments and their subsequent deployment. Method: The MammoGrid project has deployed a prototype system for clinicians using the Grid as its information infrastructure. To assist in the specification of the system requirements (and for the first time in healthgrid applications), use-case modelling has been carried out in close collaboration with clinicians and radiologists who had no prior experience of this modelling technique. A critical qualitative and, where possible, quantitative analysis of the MammoGrid prototype is presented leading to a set of recommendations from the delivery of the first deployed Grid-based medical imaging application. Results: We report critically on the application of software engineering techniques in the specification and implementation of the MammoGrid project and show that use-case modelling is a suitable vehicle for representing medical requirements and for communicating effectively with the clinical community. This paper also discusses the practical advantages and limitations of applying the Grid to real-life clinical applications and presents the consequent lessons learned.", ["Software engineering", "Medical imaging", "Infrastructure", "Medical research", "Prototype", "Specification (technical standard)", "Engineering"]], ["Managing Separation of Concerns in Grid Applications Through Architectural Model Transformations", "Grids enable the aggregation, virtualization and sharing of massive heterogeneous and geographically dispersed resources, using files, applications and storage devices, to solve computation and data intensive problems, across institutions and countries via temporary collaborations called virtual organizations (VO). Most implementations result in complex superposition of software layers, often delivering low quality of service and quality of applications. As a consequence, Grid-based applications design and development is increasingly complex, and the use of most classical engineering practices is unsuccessful. Not only is the development of such applications a time-consuming, error prone and expensive task, but also the resulting applications are often hard-coded for specific Grid configurations, platforms and infra-structures. Having neither guidelines nor rules in the design of a Grid-based application is a paradox since there are many existing architectural approaches for distributed computing, which could ease and promote rigorous engineering methods based on the re-use of software components. It is our belief that ad-hoc and semi-formal engineer-ing approaches, in current use, are insufficient to tackle tomorrows Grid develop-ments requirements. Because Grid-based applications address multi-disciplinary and complex domains (health, military, scientific computation), their engineering requires rigor and control. This paper therefore advocates a formal model-driven engineering process and corresponding design framework and tools for building the next generation of Grids.", ["Model-driven engineering", "Computing", "Computation", "Engineering", "Homogeneity and heterogeneity", "Distributed computing", "Computational science", "Component-based software engineering", "Separation of concerns", "Ad hoc", "Quality of service", "Design", "Architecture"]], ["PhantomOS: A Next Generation Grid Operating System", "Grid Computing has made substantial advances in the past decade; these are primarily due to the adoption of standardized Grid middleware. However Grid computing has not yet become pervasive because of some barriers that we believe have been caused by the adoption of middleware centric approaches. These barriers include: scant support for major types of applications such as interactive applications; lack of flexible, autonomic and scalable Grid architectures; lack of plug-and-play Grid computing and, most importantly, no straightforward way to setup and administer Grids. PhantomOS is a project which aims to address many of these barriers. Its goal is the creation of a user friendly pervasive Grid computing platform that facilitates the rapid deployment and easy maintenance of Grids whilst providing support for major types of applications on Grids of almost any topology. In this paper we present the detailed system architecture and an overview of its implementation.", ["Grid computing", "Topology", "Middleware", "Operating system", "Plug and play", "Systems architecture", "Computing", "Computing platform", "Scalability", "Architecture"]], ["The Requirements for Ontologies in Medical Data Integration: A Case Study", "Evidence-based medicine is critically dependent on three sources of information: a medical knowledge base, the patients medical record and knowledge of available resources, including where appropriate, clinical protocols. Patient data is often scattered in a variety of databases and may, in a distributed model, be held across several disparate repositories. Consequently addressing the needs of an evidence-based medicine community presents issues of biomedical data integration, clinical interpretation and knowledge management. This paper outlines how the Health-e-Child project has approached the challenge of requirements specification for (bio-) medical data integration, from the level of cellular data, through disease to that of patient and population. The approach is illuminated through the requirements elicitation and analysis of Juvenile Idiopathic Arthritis (JIA), one of three diseases being studied in the EC-funded Health-e-Child project.", ["Evidence-based medicine", "Disease", "Medicine", "Idiopathic", "Arthritis", "Juvenile idiopathic arthritis", "Medical record", "Knowledge management", "Clinical trial protocol", "Data integration", "Medical research", "Ontology", "Distributed computing"]], ["p-Adic Degeneracy of the Genetic Code", "Degeneracy of the genetic code is a biological way to minimize effects of the undesirable mutation changes. Degeneration has a natural description on the 5-adic space of 64 codons $\\mathcal{C}_5 (64) = \\{n_0 + n_1 5 + n_2 5^2 : n_i = 1, 2, 3, 4 \\} ,$ where $n_i$ are digits related to nucleotides as follows: C = 1, A = 2, T = U = 3, G = 4. The smallest 5-adic distance between codons joins them into 16 quadruplets, which under 2-adic distance decay into 32 doublets. p-Adically close codons are assigned to one of 20 amino acids, which are building blocks of proteins, or code termination of protein synthesis. We shown that genetic code multiplets are made of the p-adic nearest codons.", ["Mutation", "Nucleotide", "Genetic code", "Protein biosynthesis", "Multiple birth", "Protein", "Amino acid", "Degeneracy (biology)", "Carbon", "Degeneration", "Genetics"]], ["Performance of Linear Field Reconstruction Techniques with Noise and Uncertain Sensor Locations", "We consider a wireless sensor network, sampling a bandlimited field, described by a limited number of harmonics. Sensor nodes are irregularly deployed over the area of interest or subject to random motion; in addition sensors measurements are affected by noise. Our goal is to obtain a high quality reconstruction of the field, with the mean square error (MSE) of the estimate as performance metric. In particular, we analytically derive the performance of several reconstruction/estimation techniques based on linear filtering. For each technique, we obtain the MSE, as well as its asymptotic expression in the case where the field number of harmonics and the number of sensors grow to infinity, while their ratio is kept constant. Through numerical simulations, we show the validity of the asymptotic analysis, even for a small number of sensors. We provide some novel guidelines for the design of sensor networks when many parameters, such as field bandwidth, number of sensors, reconstruction quality, sensor motion characteristics, and noise level of the measures, have to be traded off.", ["Wireless sensor network", "Noise (electronics)", "Wireless", "Harmonic", "Sensor", "Asymptotic analysis", "Bandlimiting", "Infinity", "Mean squared error", "Reconstruction Era of the United States", "Performance metric", "Randomness"]], ["A New Family of Unitary Space-Time Codes with a Fast Parallel Sphere Decoder Algorithm", "In this paper we propose a new design criterion and a new class of unitary signal constellations for differential space-time modulation for multiple-antenna systems over Rayleigh flat-fading channels with unknown fading coefficients. Extensive simulations show that the new codes have significantly better performance than existing codes. We have compared the performance of our codes with differential detection schemes using orthogonal design, Cayley differential codes, fixed-point-free group codes and product of groups and for the same bit error rate, our codes allow smaller signal to noise ratio by as much as 10 dB. The design of the new codes is accomplished in a systematic way through the optimization of a performance index that closely describes the bit error rate as a function of the signal to noise ratio. The new performance index is computationally simple and we have derived analytical expressions for its gradient with respect to constellation parameters. Decoding of the proposed constellations is reduced to a set of one-dimensional closest point problems that we solve using parallel sphere decoder algorithms. This decoding strategy can also improve efficiency of existing codes.", ["Free group", "Computational complexity theory", "Bit", "Decoder", "Antenna (radio)", "Signal-to-noise ratio", "Constellation", "Decibel", "Algorithm", "Spacetime", "MIMO", "Mathematical optimization", "Fixed-point arithmetic", "Gradient", "Bit error rate", "Function (mathematics)", "Direct product of groups", "Orthogonality", "Unitary authority"]], ["Very fast watermarking by reversible contrast mapping", "Reversible contrast mapping (RCM) is a simple integer transform that applies to pairs of pixels. For some pairs of pixels, RCM is invertible, even if the least significant bits (LSBs) of the transformed pixels are lost. The data space occupied by the LSBs is suitable for data hiding. The embedded information bit-rates of the proposed spatial domain reversible watermarking scheme are close to the highest bit-rates reported so far. The scheme does not need additional data compression, and, in terms of mathematical complexity, it appears to be the lowest complexity one proposed up to now. A very fast lookup table implementation is proposed. Robustness against cropping can be ensured as well.", ["Least significant bit", "Mathematics", "Integer", "Bit", "Data compression", "Lookup table"]], ["A New Generalization of Chebyshev Inequality for Random Vectors", "In this article, we derive a new generalization of Chebyshev inequality for random vectors. We demonstrate that the new generalization is much less conservative than the classical generalization.", ["Chebyshev's inequality"]], ["The Cyborg Astrobiologist: Porting from a wearable computer to the Astrobiology Phone-cam", "We have used a simple camera phone to significantly improve an `exploration system' for astrobiology and geology. This camera phone will make it much easier to develop and test computer-vision algorithms for future planetary exploration. We envision that the `Astrobiology Phone-cam' exploration system can be fruitfully used in other problem domains as well.", ["Geology", "Astrobiology", "Camera", "Timeline of Solar System exploration", "Wearable computer", "Camera phone", "Computer", "Algorithm", "Cam", "Cyborg"]], ["On the Minimum Number of Transmissions in Single-Hop Wireless Coding Networks", "The advent of network coding presents promising opportunities in many areas of communication and networking. It has been recently shown that network coding technique can significantly increase the overall throughput of wireless networks by taking advantage of their broadcast nature. In wireless networks, each transmitted packet is broadcasted within a certain area and can be overheard by the neighboring nodes. When a node needs to transmit packets, it employs the opportunistic coding approach that uses the knowledge of what the node's neighbors have heard in order to reduce the number of transmissions. With this approach, each transmitted packet is a linear combination of the original packets over a certain finite field. In this paper, we focus on the fundamental problem of finding the optimal encoding for the broadcasted packets that minimizes the overall number of transmissions. We show that this problem is NP-complete over GF(2) and establish several fundamental properties of the optimal solution. We also propose a simple heuristic solution for the problem based on graph coloring and present some empirical results for random settings.", ["Graph coloring", "NP-complete", "Finite field", "Wireless network", "Wireless", "Communication", "Network packet", "Heuristic", "Empirical", "Linear combination", "Throughput", "Forward error correction", "Computer network", "Finite set"]], ["Scheduling in Data Intensive and Network Aware (DIANA) Grid Environments", "In Grids scheduling decisions are often made on the basis of jobs being either data or computation intensive: in data intensive situations jobs may be pushed to the data and in computation intensive situations data may be pulled to the jobs. This kind of scheduling, in which there is no consideration of network characteristics, can lead to performance degradation in a Grid environment and may result in large processing queues and job execution delays due to site overloads. In this paper we describe a Data Intensive and Network Aware (DIANA) meta-scheduling approach, which takes into account data, processing power and network characteristics when making scheduling decisions across multiple sites. Through a practical implementation on a Grid testbed, we demonstrate that queue and execution times of data-intensive jobs can be significantly improved when we introduce our proposed DIANA scheduler. The basic scheduling decisions are dictated by a weighting factor for each potential target location which is a calculated function of network characteristics, processing cycles and data location and size. The job scheduler provides a global ranking of the computing resources and then selects an optimal one on the basis of this overall access and execution cost. The DIANA approach considers the Grid as a combination of active network elements and takes network characteristics as a first class criterion in the scheduling decision matrix along with computation and data. The scheduler can then make informed decisions by taking into account the changing state of the network, locality and size of the data and the pool of available processing cycles.", ["Computing", "Job scheduler", "Matrix (mathematics)", "Computation", "Scheduling (computing)", "Data"]], ["Optimal Linear Precoding Strategies for Wideband Non-Cooperative Systems based on Game Theory-Part II: Algorithms", "In this two-part paper, we address the problem of finding the optimal precoding/multiplexing scheme for a set of non-cooperative links sharing the same physical resources, e.g., time and bandwidth. We consider two alternative optimization problems: P.1) the maximization of mutual information on each link, given constraints on the transmit power and spectral mask; and P.2) the maximization of the transmission rate on each link, using finite order constellations, under the same constraints as in P.1, plus a constraint on the maximum average error probability on each link. Aiming at finding decentralized strategies, we adopted as optimality criterion the achievement of a Nash equilibrium and thus we formulated both problems P.1 and P.2 as strategic noncooperative (matrix-valued) games. In Part I of this two-part paper, after deriving the optimal structure of the linear transceivers for both games, we provided a unified set of sufficient conditions that guarantee the uniqueness of the Nash equilibrium. In this Part II, we focus on the achievement of the equilibrium and propose alternative distributed iterative algorithms that solve both games. Specifically, the new proposed algorithms are the following: 1) the sequential and simultaneous iterative waterfilling based algorithms, incorporating spectral mask constraints; 2) the sequential and simultaneous gradient projection based algorithms, establishing an interesting link with variational inequality problems. Our main contribution is to provide sufficient conditions for the global convergence of all the proposed algorithms which, although derived under stronger constraints, incorporating for example spectral mask constraints, have a broader validity than the convergence conditions known in the current literature for the sequential iterative waterfilling algorithm.", ["Nash equilibrium", "Mathematical optimization", "Matrix (mathematics)", "Game theory", "Mutual information", "Algorithm", "Calculus of variations", "Gradient", "Iteration", "Variational inequality", "Spectral mask", "Multiplexing", "Decentralization", "Precoding", "Linear", "Probability", "Limit of a sequence"]], ["Risk Analysis in Robust Control -- Making the Case for Probabilistic Robust Control", "This paper offers a critical view of the \"worst-case\" approach that is the cornerstone of robust control design. It is our contention that a blind acceptance of worst-case scenarios may lead to designs that are actually more dangerous than designs based on probabilistic techniques with a built-in risk factor. The real issue is one of modeling. If one accepts that no mathematical model of uncertainties is perfect then a probabilistic approach can lead to more reliable control even if it cannot guarantee stability for all possible cases. Our presentation is based on case analysis. We first establish that worst-case is not necessarily \"all-encompassing.\" In fact, we show that for some uncertain control problems to have a conventional robust control solution it is necessary to make assumptions that leave out some feasible cases. Once we establish that point, we argue that it is not uncommon for the risk of unaccounted cases in worst-case design to be greater than that of the accepted risk in a probabilistic approach. With an example, we quantify the risks and show that worst-case can be significantly more risky. Finally, we join our analysis with existing results on computational complexity and probabilistic robustness to argue that the deterministic worst-case analysis is not necessarily the better tool.", ["Computational complexity theory", "Mathematics", "Mathematical model", "Randomized algorithm", "Best, worst and average case", "Robust control", "Determinism", "Probability", "Complexity"]], ["Are there Hilbert-style Pure Type Systems?", "For many a natural deduction style logic there is a Hilbert-style logic that is equivalent to it in that it has the same theorems (i.e. valid judgements with empty contexts). For intuitionistic logic, the axioms of the equivalent Hilbert-style logic can be propositions which are also known as the types of the combinators I, K and S. Hilbert-style versions of illative combinatory logic have formulations with axioms that are actual type statements for I, K and S. As pure type systems (PTSs)are, in a sense, equivalent to systems of illative combinatory logic, it might be thought that Hilbert-style PTSs (HPTSs) could be based in a similar way. This paper shows that some PTSs have very trivial equivalent HPTSs, with only the axioms as theorems and that for many PTSs no equivalent HPTS can exist. Most commonly used PTSs belong to these two classes. For some PTSs however, including lambda* and the PTS at the basis of the proof assistant Coq, there is a nontrivial equivalent HPTS, with axioms that are type statements for I, K and S.", ["Interactive theorem proving", "Natural deduction", "Intuitionistic logic", "Combinatory logic", "Deductive reasoning", "Axiom", "Hilbert system", "David Hilbert", "Proposition", "Illative case", "Lambda", "Logic", "Mathematical proof", "Pure type system", "Coq"]], ["The Nash Equilibrium Revisited: Chaos and Complexity Hidden in Simplicity", "The Nash Equilibrium is a much discussed, deceptively complex, method for the analysis of non-cooperative games. If one reads many of the commonly available definitions the description of the Nash Equilibrium is deceptively simple in appearance. Modern research has discovered a number of new and important complex properties of the Nash Equilibrium, some of which remain as contemporary conundrums of extraordinary difficulty and complexity. Among the recently discovered features which the Nash Equilibrium exhibits under various conditions are heteroclinic Hamiltonian dynamics, a very complex asymptotic structure in the context of two-player bi-matrix games and a number of computationally complex or computationally intractable features in other settings. This paper reviews those findings and then suggests how they may inform various market prediction strategies.", ["Hamiltonian mechanics", "Nash equilibrium", "Computational complexity theory", "Complexity", "Hamiltonian (quantum mechanics)"]], ["Segmentation and Context of Literary and Musical Sequences", "We test a segmentation algorithm, based on the calculation of the Jensen-Shannon divergence between probability distributions, to two symbolic sequences of literary and musical origin. The first sequence represents the successive appearance of characters in a theatrical play, and the second represents the succession of tones from the twelve-tone scale in a keyboard sonata. The algorithm divides the sequences into segments of maximal compositional divergence between them. For the play, these segments are related to changes in the frequency of appearance of different characters and in the geographical setting of the action. For the sonata, the segments correspond to tonal domains and reveal in detail the characteristic tonal progression of such kind of musical composition.", ["Frequency", "Algorithm", "Sonata", "Musical composition", "Probability"]], ["Spectrum Sensing in Cognitive Radios Based on Multiple Cyclic Frequencies", "Cognitive radios sense the radio spectrum in order to find unused frequency bands and use them in an agile manner. Transmission by the primary user must be detected reliably even in the low signal-to-noise ratio (SNR) regime and in the face of shadowing and fading. Communication signals are typically cyclostationary, and have many periodic statistical properties related to the symbol rate, the coding and modulation schemes as well as the guard periods, for example. These properties can be exploited in designing a detector, and for distinguishing between the primary and secondary users' signals. In this paper, a generalized likelihood ratio test (GLRT) for detecting the presence of cyclostationarity using multiple cyclic frequencies is proposed. Distributed decision making is employed by combining the quantized local test statistics from many secondary users. User cooperation allows for mitigating the effects of shadowing and provides a larger footprint for the cognitive radio system. Simulation examples demonstrate the resulting performance gains in the low SNR regime and the benefits of cooperative detection.", ["Likelihood-ratio test", "Signal-to-noise ratio", "Radio spectrum", "Cognitive radio", "Noise", "Symbol rate", "Signal (electronics)", "Transmission (telecommunications)", "Modulation", "Frequency", "Statistics", "Radio", "Cognition", "Simulation"]], ["Theorem proving support in programming language semantics", "We describe several views of the semantics of a simple programming language as formal documents in the calculus of inductive constructions that can be verified by the Coq proof system. Covered aspects are natural semantics, denotational semantics, axiomatic semantics, and abstract interpretation. Descriptions as recursive functions are also provided whenever suitable, thus yielding a a verification condition generator and a static analyser that can be run inside the theorem prover for use in reflective proofs. Extraction of an interpreter from the denotational semantics is also described. All different aspects are formally proved sound with respect to the natural semantics specification.", ["Denotational semantics", "Abstract interpretation", "Axiomatic semantics", "Semantics of programming languages", "Coq", "Recursion", "Calculus", "Formal verification", "Axiom", "Theorem", "Semantics", "Mathematical proof", "Programming language", "Automated theorem proving"]], ["Resource Allocation for Wireless Fading Relay Channels: Max-Min Solution", "As a basic information-theoretic model for fading relay channels, the parallel relay channel is first studied, for which lower and upper bounds on the capacity are derived. For the parallel relay channel with degraded subchannels, the capacity is established, and is further demonstrated via the Gaussian case, for which the synchronized and asynchronized capacities are obtained. The capacity achieving power allocation at the source and relay nodes among the subchannels is characterized. The fading relay channel is then studied, for which resource allocations that maximize the achievable rates are obtained for both the full-duplex and half-duplex cases. Capacities are established for fading relay channels that satisfy certain conditions.", ["Full-duplex", "Information theory", "Half-duplex"]], ["When Network Coding and Dirty Paper Coding meet in a Cooperative Ad Hoc Network", "We develop and analyze new cooperative strategies for ad hoc networks that are more spectrally efficient than classical DF cooperative protocols. Using analog network coding, our strategies preserve the practical half-duplex assumption but relax the orthogonality constraint. The introduction of interference due to non-orthogonality is mitigated thanks to precoding, in particular Dirty Paper coding. Combined with smart power allocation, our cooperation strategies allow to save time and lead to more efficient use of bandwidth and to improved network throughput with respect to classical RDF/PDF.", ["Portable Document Format", "Forward error correction", "Orthogonality", "Precoding", "Resource Description Framework", "Spectral efficiency", "Half-duplex", "Throughput", "Analog signal", "Bandwidth (computing)"]], ["On Cognitive Interference Networks", "We study the high-power asymptotic behavior of the sum-rate capacity of multi-user interference networks with an equal number of transmitters and receivers. We assume that each transmitter is cognizant of the message it wishes to convey to its corresponding receiver and also of the messages that a subset of the other transmitters wish to send. The receivers are assumed not to be able to cooperate in any way so that they must base their decision on the signal they receive only. We focus on the network's pre-log, which is defined as the limiting ratio of the sum-rate capacity to half the logarithm of the transmitted power. We present both upper and lower bounds on the network's pre-log. The lower bounds are based on a linear partial-cancellation scheme which entails linearly transforming Gaussian codebooks so as to eliminate the interference in a subset of the receivers. Inter alias, the bounds give a complete characterization of the networks and side-information settings that result in a full pre-log, i.e., in a pre-log that is equal to the number of transmitters (and receivers) as well as a complete characterization of networks whose pre-log is equal to the full pre-log minus one. They also fully characterize networks where the full pre-log can only be achieved if each transmitter knows the messages of all users, i.e., when the side-information is \"full\".", ["Subset", "Asymptotic analysis", "Logarithm", "Upper and lower bounds", "Transmitter"]], ["The star trellis decoding of Reed-Solomon codes", "The new method for Reed-Solomon codes decoding is introduced. The method is based on the star trellis decoding of the binary image of Reed-Solomon codes.", ["Binary image", "Reed-Solomon"]], ["Noisy Sorting Without Resampling", "In this paper we study noisy sorting without re-sampling. In this problem there is an unknown order $a_{\\pi(1)} < ... < a_{\\pi(n)}$ where $\\pi$ is a permutation on $n$ elements. The input is the status of $n \\choose 2$ queries of the form $q(a_i,x_j)$, where $q(a_i,a_j) = +$ with probability at least $1/2+\\ga$ if $\\pi(i) > \\pi(j)$ for all pairs $i \\neq j$, where $\\ga > 0$ is a constant and $q(a_i,a_j) = -q(a_j,a_i)$ for all $i$ and $j$. It is assumed that the errors are independent. Given the status of the queries the goal is to find the maximum likelihood order. In other words, the goal is find a permutation $\\sigma$ that minimizes the number of pairs $\\sigma(i) > \\sigma(j)$ where $q(\\sigma(i),\\sigma(j)) = -$. The problem so defined is the feedback arc set problem on distributions of inputs, each of which is a tournament obtained as a noisy perturbations of a linear order. Note that when $\\ga < 1/2$ and $n$ is large, it is impossible to recover the original order $\\pi$. It is known that the weighted feedback are set problem on tournaments is NP-hard in general. Here we present an algorithm of running time $n^{O(\\gamma^{-4})}$ and sampling complexity $O_{\\gamma}(n \\log n)$ that with high probability solves the noisy sorting without re-sampling problem. We also show that if $a_{\\sigma(1)},a_{\\sigma(2)},...,a_{\\sigma(n)}$ is an optimal solution of the problem then it is ``close'' to the original order. More formally, with high probability it holds that $\\sum_i |\\sigma(i) - \\pi(i)| = \\Theta(n)$ and $\\max_i |\\sigma(i) - \\pi(i)| = \\Theta(\\log n)$. Our results are of interest in applications to ranking, such as ranking in sports, or ranking of search items based on comparisons by experts.", ["Maximum likelihood", "NP-hard", "Algorithm", "Probability", "Permutation", "Sigma", "Feedback", "Total order", "Pi", "Independence (probability theory)", "Logarithm"]], ["Exploration via design and the cost of uncertainty in keyword auctions", "We present a deterministic exploration mechanism for sponsored search auctions, which enables the auctioneer to learn the relevance scores of advertisers, and allows advertisers to estimate the true value of clicks generated at the auction site. This exploratory mechanism deviates only minimally from the mechanism being currently used by Google and Yahoo! in the sense that it retains the same pricing rule, similar ranking scheme, as well as, similar mathematical structure of payoffs. In particular, the estimations of the relevance scores and true-values are achieved by providing a chance to lower ranked advertisers to obtain better slots. This allows the search engine to potentially test a new pool of advertisers, and correspondingly, enables new advertisers to estimate the value of clicks/leads generated via the auction. Both these quantities are unknown a priori, and their knowledge is necessary for the auction to operate efficiently. We show that such an exploration policy can be incorporated without any significant loss in revenue for the auctioneer. We compare the revenue of the new mechanism to that of the standard mechanism at their corresponding symmetric Nash equilibria and compute the cost of uncertainty, which is defined as the relative loss in expected revenue per impression. We also bound the loss in efficiency, as well as, in user experience due to exploration, under the same solution concept (i.e. SNE). Thus the proposed exploration mechanism learns the relevance scores while incorporating the incentive constraints from the advertisers who are selfish and are trying to maximize their own profits, and therefore, the exploration is essentially achieved via mechanism design. We also discuss variations of the new mechanism such as truthful implementations.", ["Google", "A priori and a posteriori", "Solution concept", "Mathematics", "Auction", "Nash equilibrium", "Mechanism design", "Web search engine", "Yahoo!"]], ["For-profit mediators in sponsored search advertising", "A mediator is a well-known construct in game theory, and is an entity that plays on behalf of some of the agents who choose to use its services, while the rest of the agents participate in the game directly. We initiate a game theoretic study of sponsored search auctions, such as those used by Google and Yahoo!, involving {\\em incentive driven} mediators. We refer to such mediators as {\\em for-profit} mediators, so as to distinguish them from mediators introduced in prior work, who have no monetary incentives, and are driven by the altruistic goal of implementing certain desired outcomes. We show that in our model, (i) players/advertisers can improve their payoffs by choosing to use the services of the mediator, compared to directly participating in the auction; (ii) the mediator can obtain monetary benefit by managing the advertising burden of its group of advertisers; and (iii) the payoffs of the mediator and the advertisers it plays for are compatible with the incentive constraints from the advertisers who do dot use its services. A simple intuition behind the above result comes from the observation that the mediator has more information about and more control over the bid profile than any individual advertiser, allowing her to reduce the payments made to the auctioneer, while still maintaining incentive constraints. Further, our results indicate that there are significant opportunities for diversification in the internet economy and we should expect it to continue to develop richer structure, with room for different types of agents to coexist.", ["Game theory", "Intuition (knowledge)", "Advertising", "Yahoo!", "Internet", "Mediation", "Auction"]], ["Projection semantics for rigid loops", "A rigid loop is a for-loop with a counter not accessible to the loop body or any other part of a program. Special instructions for rigid loops are introduced on top of the syntax of the program algebra PGA. Two different semantic projections are provided and proven equivalent. One of these is taken to have definitional status on the basis of two criteria: `normative semantic adequacy' and `indicative algorithmic adequacy'.", ["Algorithm", "For loop", "Semantics", "Algebra", "Syntax"]], ["High-resolution distributed sampling of bandlimited fields with low-precision sensors", "The problem of sampling a discrete-time sequence of spatially bandlimited fields with a bounded dynamic range, in a distributed, communication-constrained, processing environment is addressed. A central unit, having access to the data gathered by a dense network of fixed-precision sensors, operating under stringent inter-node communication constraints, is required to reconstruct the field snapshots to maximum accuracy. Both deterministic and stochastic field models are considered. For stochastic fields, results are established in the almost-sure sense. The feasibility of having a flexible tradeoff between the oversampling rate (sensor density) and the analog-to-digital converter (ADC) precision, while achieving an exponential accuracy in the number of bits per Nyquist-interval per snapshot is demonstrated. This exposes an underlying ``conservation of bits'' principle: the bit-budget per Nyquist-interval per snapshot (the rate) can be distributed along the amplitude axis (sensor-precision) and space (sensor density) in an almost arbitrary discrete-valued manner, while retaining the same (exponential) distortion-rate characteristics. Achievable information scaling laws for field reconstruction over a bounded region are also derived: With N one-bit sensors per Nyquist-interval, $\\Theta(\\log N)$ Nyquist-intervals, and total network bitrate $R_{net} = \\Theta((\\log N)^2)$ (per-sensor bitrate $\\Theta((\\log N)/N)$), the maximum pointwise distortion goes to zero as $D = O((\\log N)^2/N)$ or $D = O(R_{net} 2^{-\\beta \\sqrt{R_{net}}})$. This is shown to be possible with only nearest-neighbor communication, distributed coding, and appropriate interpolation algorithms. For a fixed, nonzero target distortion, the number of fixed-precision sensors and the network rate needed is always finite.", ["Analog-to-digital converter", "Bit rate", "Sensor", "Bit", "Oversampling", "Bandlimiting", "Algorithm", "Dynamic range", "Sampling (signal processing)", "Logarithm", "Analog signal", "Discrete mathematics", "Discrete time", "Data", "Interpolation", "Communication", "Stochastic", "Image resolution", "Time series"]], ["The Effect of Noise Correlation in AF Relay Networks", "In wireless relay networks, noise at the relays can be correlated possibly due to common interference or noise propagation from preceding hops. In this work we consider a parallel relay network with noise correlation. For the relay strategy of amplify-and-forward (AF), we determine the optimal rate maximizing relay gains when correlation knowledge is available at the relays. The effect of correlation on the performance of the relay networks is analyzed for the cases where full knowledge of correlation is available at the relays and when there is no knowledge about the correlation structure. Interestingly we find that, on the average, noise correlation is beneficial regardless of whether the relays know the noise covariance matrix or not. However, the knowledge of correlation can greatly improve the performance. Typically, the performance improvement from correlation knowledge increases with the relay power and the number of relays. With perfect correlation knowledge the system is capable of canceling interference if the number of interferers is less than the number of relays. For a dual-hop multiple access parallel network, we obtain closed form expressions for the maximum sum-rate and the optimal relay strategy. The relay optimization for networks with three hops is also considered. For any relay gains for the first stage relays, this represents a parallel relay network with correlated noise. Based on the result of two hop networks with noise correlation, we propose an algorithm for solving the relay optimization problem for three-hop networks.", ["Algorithm", "Covariance matrix", "Covariance", "Wireless", "Mathematical optimization", "Optimization problem", "Hops", "Matrix (mathematics)", "Channel access method"]], ["Delayed Correlations in Inter-Domain Network Traffic", "To observe the evolution of network traffic correlations we analyze the eigenvalue spectra and eigenvectors statistics of delayed correlation matrices of network traffic counts time series. Delayed correlation matrix D is composed of the correlations between one variable in the multivariable time series and another at a time delay \\tau . Inverse participation ratio (IPR) of eigenvectors of D deviates substantially from the IPR of eigenvectors of the equal time correlation matrix C. We relate this finding to the localization and discuss its importance for network congestion control. The time-lagged correlation pattern between network time series is preserved over a long time, up to 100\\tau, where \\tau=300 sec. The largest eigenvalue \\lambda_{max} of D and the corresponding IPR oscillate with two characteristic periods of 3\\tau and 6\\tau . The existence of delayed correlations between network time series fits well into the long range dependence (LRD) property of the network traffic. The ability to monitor and control the long memory processes is crucial since they impact the network performance. Injecting the random traffic counts between non-randomly correlated time series, we were able to break the picture of periodicity of \\lambda_{max}. In addition, we investigated influence of the periodic injections on both largest eigenvalue and the IPR, and addressed relevance of these indicators for the LRD and self-similarity of the network traffic.", ["Long-range dependency", "Self-similarity", "Oscillation", "Eigenvalues and eigenvectors", "Evolution", "Correlation matrix", "Lambda", "Network congestion", "Matrix (mathematics)", "Congestion control"]], ["Better Algorithms and Bounds for Directed Maximum Leaf Problems", "The {\\sc Directed Maximum Leaf Out-Branching} problem is to find an out-branching (i.e. a rooted oriented spanning tree) in a given digraph with the maximum number of leaves. In this paper, we improve known parameterized algorithms and combinatorial bounds on the number of leaves in out-branchings. We show that \\begin{itemize} \\item every strongly connected digraph $D$ of order $n$ with minimum in-degree at least 3 has an out-branching with at least $(n/4)^{1/3}-1$ leaves; \\item if a strongly connected digraph $D$ does not contain an out-branching with $k$ leaves, then the pathwidth of its underlying graph is $O(k\\log k)$; \\item it can be decided in time $2^{O(k\\log^2 k)}\\cdot n^{O(1)}$ whether a strongly connected digraph on $n$ vertices has an out-branching with at least $k$ leaves. \\end{itemize} All improvements use properties of extremal structures obtained after applying local search and of some out-branching decompositions.", ["Directed graph", "Path decomposition", "Spanning tree", "Graph (mathematics)", "Combinatorics", "Natural logarithm", "Algorithm"]], ["Worst-Case Interactive Communication and Enhancing Sensor Network Lifetime", "We are concerned with the problem of maximizing the worst-case lifetime of a data-gathering wireless sensor network consisting of a set of sensor nodes directly communicating with a base-station.We propose to solve this problem by modeling sensor node and base-station communication as the interactive communication between multiple correlated informants (sensor nodes) and a recipient (base-station). We provide practical and scalable interactive communication protocols for data gathering in sensor networks and demonstrate their efficiency compared to traditional approaches. In this paper, we first develop a formalism to address the problem of worst-case interactive communication between a set of multiple correlated informants and a recipient. We realize that there can be different objectives to achieve in such a communication scenario and compute the optimal number of messages and bits exchanged to realize these objectives. Then, we propose to adapt these results in the context of single-hop data-gathering sensor networks. Finally, based on this proposed formalism, we propose a clustering based communication protocol for large sensor networks and demonstrate its superiority over a traditional clustering protocol.", ["Wireless sensor network", "Communications protocol", "Sensor node", "Computer network", "Data", "Interactivity", "Scalability", "Communication"]], ["Logic, Design & Organization of PTVD-SHAM; A Parallel Time Varying & Data Super-helical Access Memory", "This paper encompasses a super helical memory system's design, 'Boolean logic & image-logic' as a theoretical concept of an invention-model to 'store time-data' in terms of anticipating the best memory location ever for data/time. A waterfall effect is deemed to assist the process of potential-difference output-switch into diverse logic states in quantum dot computational methods via utilizing coiled carbon nanotubes (CCNTs) and carbon nanotube field effect transistors (CNFETs). A 'quantum confinement' is thus derived for a flow of particles in a categorized quantum well substrate with a normalized capacitance rectifying high B-field flux into electromagnetic induction. Multi-access of coherent sequences of 'qubit addressing' is gained in any magnitude as pre-defined for the orientation of array displacement. Briefly, Gaussian curvature of k<0 is debated in aim of specifying the 2D electron gas characteristics in scenarios where data is stored in short intervals versus long ones e.g. when k'>(k<0) for greater CCNT diameters, space-time continuum is folded by chance for the particle. This benefits from Maxwell-Lorentz theory in Minkowski's space-time viewpoint alike to crystal oscillators for precise data timing purposes and radar systems e.g., time varying self-clocking devices in diverse geographic locations. This application could also be optional for data depository versus extraction, in the best supercomputer system's locations, autonomously. For best performance in minimizing current limiting mechanisms including electromigration, a multilevel metallization and implant process forming elevated sources/drains for the circuit's staircase pyramidal construction, is discussed accordingly.", ["Carbon nanotube", "Quantum dot", "Quantum well", "Boolean algebra", "Qubit", "Electromagnetic induction", "Potential well", "Electron", "Flux", "Electromigration", "Spacetime", "Capacitance", "Logic", "Carbon", "Free electron model", "Gas", "Self-clocking signal", "Supercomputer", "Magnetic field", "Radar", "Transistor", "Field-effect transistor", "Curvature", "Crystal oscillator", "Helix", "Crystal", "Gaussian curvature", "Rectifier", "Coherence (physics)"]], ["Expressing an NP-Complete Problem as the Solvability of a Polynomial Equation", "We demonstrate a polynomial approach to express the decision version of the directed Hamiltonian Cycle Problem (HCP), which is NP-Complete, as the Solvability of a Polynomial Equation with a constant number of variables, within a bounded real space. We first introduce four new Theorems for a set of periodic Functions with irrational periods, based on which we then use a trigonometric substitution, to show how the HCP can be expressed as the Solvability of a single polynomial Equation with a constant number of variables. The feasible solution of each of these variables is bounded within two real numbers. We point out what future work is necessary to prove that P=NP.", ["Real number", "Polynomial", "Trigonometry", "NP-complete", "Candidate solution", "Equation", "P versus NP problem"]], ["Singular curves and cusp points in the joint space of 3-RPR parallel manipulators", "This paper investigates the singular curves in two-dimensional slices of the joint space of a family of planar parallel manipulators. It focuses on special points, referred to as cusp points, which may appear on these curves. Cusp points play an important role in the kinematic behavior of parallel manipulators since they make possible a nonsingular change of assembly mode. The purpose of this study is twofold. First, it reviews an important previous work, which, to the authors' knowledge, has never been exploited yet. Second, it determines the cusp points in any two-dimensional slice of the joint space. First results show that the number of cusp points may vary from zero to eight. This work finds applications in both design and trajectory planning.", ["Trajectory", "Kinematics"]], ["Graph-Based Decoding in the Presence of ISI", "We propose an approximation of maximum-likelihood detection in ISI channels based on linear programming or message passing. We convert the detection problem into a binary decoding problem, which can be easily combined with LDPC decoding. We show that, for a certain class of channels and in the absence of coding, the proposed technique provides the exact ML solution without an exponential complexity in the size of channel memory, while for some other channels, this method has a non-diminishing probability of failure as SNR increases. Some analysis is provided for the error events of the proposed technique under linear programming.", ["Linear programming", "Graph (mathematics)", "ML (programming language)", "Low-density parity-check code", "Message passing", "Maximum likelihood", "Signal-to-noise ratio"]], ["Building Decision Procedures in the Calculus of Inductive Constructions", "It is commonly agreed that the success of future proof assistants will rely on their ability to incorporate computations within deduction in order to mimic the mathematician when replacing the proof of a proposition P by the proof of an equivalent proposition P' obtained from P thanks to possibly complex calculations. In this paper, we investigate a new version of the calculus of inductive constructions which incorporates arbitrary decision procedures into deduction via the conversion rule of the calculus. The novelty of the problem in the context of the calculus of inductive constructions lies in the fact that the computation mechanism varies along proof-checking: goals are sent to the decision procedure together with the set of user hypotheses available from the current context. Our main result shows that this extension of the calculus of constructions does not compromise its main properties: confluence, subject reduction, strong normalization and consistency are all preserved.", ["Calculus of constructions", "Calculus", "Decision problem", "Computation", "Normalization property (abstract rewriting)", "Mathematician", "Deductive reasoning", "Proposition", "Mathematical proof", "Consistency", "Hypothesis"]], ["Espaces de repr\\'esentation multidimensionnels d\\'edi\\'es \\`a la visualisation", "In decision-support systems, the visual component is important for On Line Analysis Processing (OLAP). In this paper, we propose a new approach that faces the visualization problem due to data sparsity. We use the results of a Multiple Correspondence Analysis (MCA) to reduce the negative effect of sparsity by organizing differently data cube cells. Our approach does not reduce sparsity, however it tries to build relevant representation spaces where facts are efficiently gathered. In order to evaluate our approach, we propose an homogeneity criterion based on geometric neighborhood of cells. The obtained experimental results have shown the efficiency of our method.", ["Cube", "Online analytical processing", "Geometry", "Decision support system"]], ["Efficient supervised learning in networks with binary synapses", "Recent experimental studies indicate that synaptic changes induced by neuronal activity are discrete jumps between a small number of stable states. Learning in systems with discrete synapses is known to be a computationally hard problem. Here, we study a neurobiologically plausible on-line learning algorithm that derives from Belief Propagation algorithms. We show that it performs remarkably well in a model neuron with binary synapses, and a finite number of `hidden' states per synapse, that has to learn a random classification task. Such system is able to learn a number of associations close to the theoretical limit, in time which is sublinear in system size. This is to our knowledge the first on-line algorithm that is able to achieve efficiently a finite number of patterns learned per binary synapse. Furthermore, we show that performance is optimal for a finite number of hidden states which becomes very small for sparse coding. The algorithm is similar to the standard `perceptron' learning algorithm, with an additional rule for synaptic transitions which occur only if a currently presented pattern is `barely correct'. In this case, the synaptic changes are meta-plastic only (change in hidden states and not in actual synaptic state), stabilizing the synapse in its current state. Finally, we show that a system with two visible states and K hidden states is much more robust to noise than a system with K visible states. We suggest this rule is sufficiently simple to be easily implemented by neurobiological systems or in hardware.", ["Sparse coding", "Synapse", "Neuron", "Machine learning", "Algorithm", "Perceptron", "Neuroscience", "Computational complexity theory", "Sublinear function", "Supervised learning", "Hardware"]], ["Un index de jointure pour les entrep\\^ots de donn\\'ees XML", "XML data warehouses form an interesting basis for decision-support applications that exploit heterogeneous data from multiple sources. However, XML-native database systems currently bear limited performances and it is necessary to research ways to optimize them. In this paper, we propose a new index that is specifically adapted to the multidimensional architecture of XML warehouses and eliminates join operations, while preserving the information contained in the original warehouse. A theoretical study and experimental results demonstrate the efficiency of our index, even when queries are complex.", ["Homogeneity and heterogeneity", "XML", "Jointure", "Warehouse", "Decision support system", "Architecture", "Data", "Database management system"]], ["S\\'election simultan\\'ee d'index et de vues mat\\'erialis\\'ees", "Indices and materialized views are physical structures that accelerate data access in data warehouses. However, these data structures generate some maintenance overhead. They also share the same storage space. The existing studies about index and materialized view selection consider these structures separately. In this paper, we adopt the opposite stance and couple index and materialized view selection to take into account the interactions between them and achieve an efficient storage space sharing. We develop cost models that evaluate the respective benefit of indexing and view materialization. These cost models are then exploited by a greedy algorithm to select a relevant configuration of indices and materialized views. Experimental results show that our strategy performs better than the independent selection of indices and materialized views.", ["Materialized view", "Greedy algorithm", "Index (database)", "Index (computer science)"]], ["Report on Generic Case Complexity", "This article is a short introduction to generic case complexity, which is a recently developed way of measuring the difficulty of a computational problem while ignoring atypical behavior on a small set of inputs. Generic case complexity applies to both recursively solvable and recursively unsolvable problems.", ["Computational problem", "Computational chemistry", "Recursion", "Complexity"]], ["Computability Closure: Ten Years Later", "The notion of computability closure has been introduced for proving the termination of higher-order rewriting with first-order matching by Jean-Pierre Jouannaud and Mitsuhiro Okada in a 1997 draft which later served as a basis for the author's PhD. In this paper, we show how this notion can also be used for dealing with beta-normalized rewriting with matching modulo beta-eta (on patterns \\`a la Miller), rewriting with matching modulo some equational theory, and higher-order data types (types with constructors having functional recursive arguments). Finally, we show how the computability closure can easily be turned into a reduction ordering which, in the higher-order case, contains Jean-Pierre Jouannaud and Albert Rubio's higher-order recursive path ordering and, in the first-order case, is equal to the usual first-order recursive path ordering.", ["Universal algebra", "Modular arithmetic", "Doctor of Philosophy", "First-order logic", "Well-order"]], ["Sequential products in effect categories", "A new categorical framework is provided for dealing with multiple arguments in a programming language with effects, for example in a language with imperative features. Like related frameworks (Monads, Arrows, Freyd categories), we distinguish two kinds of functions. In addition, we also distinguish two kinds of equations. Then, we are able to define a kind of product, that generalizes the usual categorical product. This yields a powerful tool for deriving many results about languages with effects.", ["Imperative programming", "Product (category theory)", "Monad (functional programming)", "Programming language"]], ["Clusters, Graphs, and Networks for Analysing Internet-Web-Supported Communication within a Virtual Community", "The proposal is to use clusters, graphs and networks as models in order to analyse the Web structure. Clusters, graphs and networks provide knowledge representation and organization. Clusters were generated by co-site analysis. The sample is a set of academic Web sites from the countries belonging to the European Union. These clusters are here revisited from the point of view of graph theory and social network analysis. This is a quantitative and structural analysis. In fact, the Internet is a computer network that connects people and organizations. Thus we may consider it to be a social network. The set of Web academic sites represents an empirical social network, and is viewed as a virtual community. The network structural properties are here analysed applying together cluster analysis, graph theory and social network analysis.", ["Graph theory", "Knowledge representation and reasoning", "Social network", "Virtual community", "European Union", "Empirical", "Cluster analysis", "Computer", "Computer network", "Graph (mathematics)", "Communication", "Academia"]], ["Secrecy Capacity Region of Fading Broadcast Channels", "The fading broadcast channel with confidential messages (BCC) is investigated, where a source node has common information for two receivers (receivers 1 and 2), and has confidential information intended only for receiver 1. The confidential information needs to be kept as secret as possible from receiver 2. The channel state information (CSI) is assumed to be known at both the transmitter and the receivers. The secrecy capacity region is first established for the parallel Gaussian BCC, and the optimal source power allocations that achieve the boundary of the secrecy capacity region are derived. In particular, the secrecy capacity region is established for the Gaussian case of the Csiszar-Korner BCC model. The secrecy capacity results are then applied to give the ergodic secrecy capacity region for the fading BCC.", ["Channel state information", "Transmitter"]], ["IRVO: an Interaction Model for designing Collaborative Mixed Reality systems", "This paper presents an interaction model adapted to mixed reality environments known as IRVO (Interacting with Real and Virtual Objects). IRVO aims at modeling the interaction between one or more users and the Mixed Reality system by representing explicitly the objects and tools involved and their relationship. IRVO covers the design phase of the life cycle and models the intended use of the system. In a first part, we present a brief review of related HCI models. The second part is devoted to the IRVO model, its notation and some examples. In the third part, we present how IRVO is used for designing applications and in particular we show how this model can be integrated in a Model-Based Approach (CoCSys) which is currently designed at our lab.", ["Mixed reality"]], ["Fast computing of velocity field for flows in industrial burners and pumps", "In this work we present a technique of fast numerical computation for solutions of Navier-Stokes equations in the case of flows of industrial interest. At first the partial differential equations are translated into a set of nonlinear ordinary differential equations using the geometrical shape of the domain where the flow is developing, then these ODEs are numerically resolved using a set of computations distributed among the available processors. We present some results from simulations on a parallel hardware architecture using native multithreads software and simulating a shared-memory or a distributed-memory environment.", ["Navier-Stokes equations", "Computation", "Flow velocity", "Partial differential equation", "Computing", "Numerical analysis", "Computer software", "Ordinary differential equation", "Differential equation", "Nonlinear system", "Shared memory", "Architecture", "Hardware"]], ["Random subgroups and analysis of the length-based and quotient attacks", "In this paper we discuss generic properties of \"random subgroups\" of a given group G. It turns out that in many groups G (even in most exotic of them) the random subgroups have a simple algebraic structure and they \"sit\" inside G in a very particular way. This gives a strong mathematical foundation for cryptanalysis of several group-based cryptosystems and indicates on how to chose \"strong keys\". To illustrate our technique we analyze the Anshel-Anshel-Goldfeld (AAG) cryptosystem and give a mathematical explanation of recent success of some heuristic length-based attacks on it. Furthermore, we design and analyze a new type of attacks, which we term the quotient attacks. Mathematical methods we develop here also indicate how one can try to choose \"parameters\" in AAG to foil the attacks.", ["Algebraic structure", "Cryptosystem", "Mathematics", "Generic property", "Foundations of mathematics", "Cryptanalysis", "Quotient group"]], ["Properties of polynomial bases used in a line-surface intersection algorithm", "In [5], Srijuntongsiri and Vavasis propose the \"Kantorovich-Test Subdivision algorithm\", or KTS, which is an algorithm for finding all zeros of a polynomial system in a bounded region of the plane. This algorithm can be used to find the intersections between a line and a surface. The main features of KTS are that it can operate on polynomials represented in any basis that satisfies certain conditions and that its efficiency has an upper bound that depends only on the conditioning of the problem and the choice of the basis representing the polynomial system. This article explores in detail the dependence of the efficiency of the KTS algorithm on the choice of basis. Three bases are considered: the power, the Bernstein, and the Chebyshev bases. These three bases satisfy the basis properties required by KTS. Theoretically, Chebyshev case has the smallest upper bound on its running time. The computational results, however, do not show that Chebyshev case performs better than the other two.", ["Polynomial", "Time complexity", "Computational complexity theory", "Algorithm", "Upper and lower bounds", "Supremum"]], ["Programming Telepathy: Implementing Quantum Non-Locality Games", "Quantum pseudo-telepathy is an intriguing phenomenon which results from the application of quantum information theory to communication complexity. To demonstrate this phenomenon researchers in the field of quantum communication complexity devised a number of quantum non-locality games. The setting of these games is as follows: the players are separated so that no communication between them is possible and are given a certain computational task. When the players have access to a quantum resource called entanglement, they can accomplish the task: something that is impossible in a classical setting. To an observer who is unfamiliar with the laws of quantum mechanics it seems that the players employ some sort of telepathy; that is, they somehow exchange information without sharing a communication channel. This paper provides a formal framework for specifying, implementing, and analysing quantum non-locality games.", ["Communication complexity", "Quantum information", "Quantum mechanics", "Quantum pseudo-telepathy", "Information theory", "Communication", "Telepathy", "Quantum entanglement", "Mechanics", "Phenomenon", "Theory", "Classical physics", "Quantum information science", "Urban areas in Sweden"]], ["Sorting and Selection in Posets", "Classical problems of sorting and searching assume an underlying linear ordering of the objects being compared. In this paper, we study a more general setting, in which some pairs of objects are incomparable. This generalization is relevant in applications related to rankings in sports, college admissions, or conference submissions. It also has potential applications in biology, such as comparing the evolutionary fitness of different strains of bacteria, or understanding input-output relations among a set of metabolic reactions or the causal influences among a set of interacting genes or proteins. Our results improve and extend results from two decades ago of Faigle and Tur\\'{a}n. A measure of complexity of a partially ordered set (poset) is its width. Our algorithms obtain information about a poset by queries that compare two elements. We present an algorithm that sorts, i.e. completely identifies, a width w poset of size n and has query complexity O(wn + nlog(n)), which is within a constant factor of the information-theoretic lower bound. We also show that a variant of Mergesort has query complexity O(wn(log(n/w))) and total complexity O((w^2)nlog(n/w)). Faigle and Tur\\'{a}n have shown that the sorting problem has query complexity O(wn(log(n/w))) but did not address its total complexity. For the related problem of determining the minimal elements of a poset, we give efficient deterministic and randomized algorithms with O(wn) query and total complexity, along with matching lower bounds for the query complexity up to a factor of 2. We generalize these results to the k-selection problem of determining the elements of height at most k. We also derive upper bounds on the total complexity of some other problems of a similar flavor.", ["Partially ordered set", "Merge sort", "Selection algorithm", "Information theory", "Biology", "Causality", "Bacteria", "Protein", "Fitness (biology)", "Decision tree model", "Gene", "Algorithm", "Metabolism"]], ["An Architecture Framework for Complex Data Warehouses", "Nowadays, many decision support applications need to exploit data that are not only numerical or symbolic, but also multimedia, multistructure, multisource, multimodal, and/or multiversion. We term such data complex data. Managing and analyzing complex data involves a lot of different issues regarding their structure, storage and processing, and metadata are a key element in all these processes. Such problems have been addressed by classical data warehousing (i.e., applied to \"simple\" data). However, data warehousing approaches need to be adapted for complex data. In this paper, we first propose a precise, though open, definition of complex data. Then we present a general architecture framework for warehousing complex data. This architecture heavily relies on metadata and domain-related knowledge, and rests on the XML language, which helps storing data, metadata and domain-specific knowledge altogether, and facilitates communication between the various warehousing processes.", ["Data warehouse", "Metadata", "XML", "Communication", "Warehouse", "Architecture", "Multimedia", "Data"]], ["Data Mining-based Materialized View and Index Selection in Data Warehouses", "Materialized views and indexes are physical structures for accelerating data access that are casually used in data warehouses. However, these data structures generate some maintenance overhead. They also share the same storage space. Most existing studies about materialized view and index selection consider these structures separately. In this paper, we adopt the opposite stance and couple materialized view and index selection to take view-index interactions into account and achieve efficient storage space sharing. Candidate materialized views and indexes are selected through a data mining process. We also exploit cost models that evaluate the respective benefit of indexing and view materialization, and help select a relevant configuration of indexes and materialized views among the candidates. Experimental results show that our strategy performs better than an independent selection of materialized views and indexes.", ["Materialized view", "Data mining", "Index (database)", "Data warehouse", "Data"]], ["Autonomy with regard to an Attribute", "This paper presents a model of autonomy called autonomy with regard to an attribute applicable to cognitive and not cognitive artificial agents. Three criteria (global / partial, social / nonsocial, absolute / relative) are defined and used to describe the main characteristics of this type of autonomy. A software agent autonomous with regard to the mobility illustrates a possible implementation of this model.", ["Software agent", "Autonomy"]], ["A Tight Lower Bound to the Outage Probability of Discrete-Input Block-Fading Channels", "In this correspondence, we propose a tight lower bound to the outage probability of discrete-input Nakagami-m block-fading channels. The approach permits an efficient method for numerical evaluation of the bound, providing an additional tool for system design. The optimal rate-diversity trade-off for the Nakagami-m block-fading channel is also derived and a tight upper bound is obtained for the optimal coding gain constant.", ["Fading", "Upper and lower bounds", "Trade-off"]], ["Cactus Framework: Black Holes to Gamma Ray Bursts", "Gamma Ray Bursts (GRBs) are intense narrowly-beamed flashes of gamma-rays of cosmological origin. They are among the most scientifically interesting astrophysical systems, and the riddle concerning their central engines and emission mechanisms is one of the most complex and challenging problems of astrophysics today. In this article we outline our petascale approach to the GRB problem and discuss the computational toolkits and numerical codes that are currently in use and that will be scaled up to run on emerging petaflop scale computing platforms in the near future. Petascale computing will require additional ingredients over conventional parallelism. We consider some of the challenges which will be caused by future petascale architectures, and discuss our plans for the future development of the Cactus framework and its applications to meet these challenges in order to profit from these new architectures.", ["Astrophysics", "Petascale", "Gamma ray", "Gamma-ray burst", "Computing", "Cactus", "Parallel computing", "Black hole", "FLOPS"]], ["The Trade-offs with Space Time Cube Representation of Spatiotemporal Patterns", "Space time cube representation is an information visualization technique where spatiotemporal data points are mapped into a cube. Fast and correct analysis of such information is important in for instance geospatial and social visualization applications. Information visualization researchers have previously argued that space time cube representation is beneficial in revealing complex spatiotemporal patterns in a dataset to users. The argument is based on the fact that both time and spatial information are displayed simultaneously to users, an effect difficult to achieve in other representations. However, to our knowledge the actual usefulness of space time cube representation in conveying complex spatiotemporal patterns to users has not been empirically validated. To fill this gap we report on a between-subjects experiment comparing novice users error rates and response times when answering a set of questions using either space time cube or a baseline 2D representation. For some simple questions the error rates were lower when using the baseline representation. For complex questions where the participants needed an overall understanding of the spatiotemporal structure of the dataset, the space time cube representation resulted in on average twice as fast response times with no difference in error rates compared to the baseline. These results provide an empirical foundation for the hypothesis that space time cube representation benefits users when analyzing complex spatiotemporal patterns.", ["Cube", "Hypothesis", "Knowledge", "Spacetime", "Information visualization", "Experiment", "Scientific method", "Space", "Data", "Geospatial analysis", "Empirical"]], ["Interface groups and financial transfer architectures", "Analytic execution architectures have been proposed by the same authors as a means to conceptualize the cooperation between heterogeneous collectives of components such as programs, threads, states and services. Interface groups have been proposed as a means to formalize interface information concerning analytic execution architectures. These concepts are adapted to organization architectures with a focus on financial transfers. Interface groups (and monoids) now provide a technique to combine interface elements into interfaces with the flexibility to distinguish between directions of flow dependent on entity naming. The main principle exploiting interface groups is that when composing a closed system of a collection of interacting components, the sum of their interfaces must vanish in the interface group modulo reflection. This certainly matters for financial transfer interfaces. As an example of this, we specify an interface group and within it some specific interfaces concerning the financial transfer architecture for a part of our local academic organization. Financial transfer interface groups arise as a special case of more general service architecture interfaces.", ["Thread (computer science)", "Homogeneity and heterogeneity", "Architecture"]], ["Fast and Simple Relational Processing of Uncertain Data", "This paper introduces U-relations, a succinct and purely relational representation system for uncertain databases. U-relations support attribute-level uncertainty using vertical partitioning. If we consider positive relational algebra extended by an operation for computing possible answers, a query on the logical level can be translated into, and evaluated as, a single relational algebra query on the U-relation representation. The translation scheme essentially preserves the size of the query in terms of number of operations and, in particular, number of joins. Standard techniques employed in off-the-shelf relational database management systems are effective for optimizing and processing queries on U-relations. In our experiments we show that query evaluation on U-relations scales to large amounts of data with high degrees of uncertainty.", ["Relational algebra", "Relational database", "Database management system", "Information retrieval", "Database", "Relational database management system", "Data"]], ["Sampling Algorithms and Coresets for Lp Regression", "The Lp regression problem takes as input a matrix $A \\in \\Real^{n \\times d}$, a vector $b \\in \\Real^n$, and a number $p \\in [1,\\infty)$, and it returns as output a number ${\\cal Z}$ and a vector $x_{opt} \\in \\Real^d$ such that ${\\cal Z} = \\min_{x \\in \\Real^d} ||Ax -b||_p = ||Ax_{opt}-b||_p$. In this paper, we construct coresets and obtain an efficient two-stage sampling-based approximation algorithm for the very overconstrained ($n \\gg d$) version of this classical problem, for all $p \\in [1, \\infty)$. The first stage of our algorithm non-uniformly samples $\\hat{r}_1 = O(36^p d^{\\max\\{p/2+1, p\\}+1})$ rows of $A$ and the corresponding elements of $b$, and then it solves the Lp regression problem on the sample; we prove this is an 8-approximation. The second stage of our algorithm uses the output of the first stage to resample $\\hat{r}_1/\\epsilon^2$ constraints, and then it solves the Lp regression problem on the new sample; we prove this is a $(1+\\epsilon)$-approximation. Our algorithm unifies, improves upon, and extends the existing algorithms for special cases of Lp regression, namely $p = 1,2$. In course of proving our result, we develop two concepts--well-conditioned bases and subspace-preserving sampling--that are of independent interest.", ["Sampling (music)", "Condition number", "Euclidean vector", "Approximation algorithm"]], ["Sources of Superlinearity in Davenport-Schinzel Sequences", "A generalized Davenport-Schinzel sequence is one over a finite alphabet that contains no subsequences isomorphic to a fixed forbidden subsequence. One of the fundamental problems in this area is bounding (asymptotically) the maximum length of such sequences. Following Klazar, let Ex(\\sigma,n) be the maximum length of a sequence over an alphabet of size n avoiding subsequences isomorphic to \\sigma. It has been proved that for every \\sigma, Ex(\\sigma,n) is either linear or very close to linear; in particular it is O(n 2^{\\alpha(n)^{O(1)}}), where \\alpha is the inverse-Ackermann function and O(1) depends on \\sigma. However, very little is known about the properties of \\sigma that induce superlinearity of \\Ex(\\sigma,n). In this paper we exhibit an infinite family of independent superlinear forbidden subsequences. To be specific, we show that there are 17 prototypical superlinear forbidden subsequences, some of which can be made arbitrarily long through a simple padding operation. Perhaps the most novel part of our constructions is a new succinct code for representing superlinear forbidden subsequences.", ["Ackermann function", "Function (mathematics)", "Isomorphism", "Inverse function", "Finite set", "Sequence", "Sigma", "Subsequence", "Alphabet", "Linear"]], ["Numerical Calculation With Arbitrary Precision", "The vast use of computers on scientific numerical computation makes the awareness of the limited precision that these machines are able to provide us an essential matter. A limited and insufficient precision allied to the truncation and rounding errors may induce the user to incorrect interpretation of his/hers answer. In this work, we have developed a computational package to minimize this kind of error by offering arbitrary precision numbers and calculation. This is very important in Physics where we can work with numbers too small and too big simultaneously.", ["Computation", "Numerical analysis", "Arbitrary-precision arithmetic", "Physics", "Calculation"]], ["On slow-fading non-separable correlation MIMO systems", "In a frequency selective slow-fading channel in a MIMO system, the channel matrix is of the form of a block matrix. We propose a method to calculate the limit of the eigenvalue distribution of block matrices if the size of the blocks tends to infinity. We will also calculate the asymptotic eigenvalue distribution of $HH^*$, where the entries of $H$ are jointly Gaussian, with a correlation of the form $E[h_{pj}\\bar h_{qk}]= \\sum_{s=1}^t \\Psi^{(s)}_{jk}\\hat\\Psi^{(s)}_{pq}$ (where $t$ is fixed and does not increase with the size of the matrix). We will use an operator-valued free probability approach to achieve this goal. Using this method, we derive a system of equations, which can be solved numerically to compute the desired eigenvalue distribution.", ["Eigenvalues and eigenvectors", "Free probability", "Infinity", "Correlation and dependence", "Normal distribution", "Separable space", "Multivariate normal distribution", "Frequency", "Probability", "Matrix (mathematics)", "MIMO", "Block matrix", "Fading", "Probability distribution"]], ["Understanding the Properties of the BitTorrent Overlay", "In this paper, we conduct extensive simulations to understand the properties of the overlay generated by BitTorrent. We start by analyzing how the overlay properties impact the efficiency of BitTorrent. We focus on the average peer set size (i.e., average number of neighbors), the time for a peer to reach its maximum peer set size, and the diameter of the overlay. In particular, we show that the later a peer arrives in a torrent, the longer it takes to reach its maximum peer set size. Then, we evaluate the impact of the maximum peer set size, the maximum number of outgoing connections per peer, and the number of NATed peers on the overlay properties. We show that BitTorrent generates a robust overlay, but that this overlay is not a random graph. In particular, the connectivity of a peer to its neighbors depends on its arriving order in the torrent. We also show that a large number of NATed peers significantly compromise the robustness of the overlay to attacks. Finally, we evaluate the impact of peer exchange on the overlay properties, and we show that it generates a chain-like overlay with a large diameter, which will adversely impact the efficiency of large torrents.", ["Random graph", "BitTorrent (protocol)", "Peer exchange"]], ["The Kinematics of Manipulators Built From Closed Planar Mechanisms", "The paper discusses the kinematics of manipulators builts of planar closed kinematic chains. A special kinematic scheme is extracted from the array of these mechanisms that looks the most promising for the creation of different types of robotic manipulators. The structural features of this manipulator determine a number of its original properties that essentially simplify its control. These features allow the main control problems to be effectively overcome by application of the simple kinematic problems. The workspace and singular configurations of a basic planar manipulator are studied. By using a graphic simulation method, motions of the designed mechanism are examined. A prototype of this mechanism was implemented to verify the proposed approach.", ["Kinematics", "Prototype"]], ["On the Degrees of Freedom in Cognitive Radio Channels", "After receiving useful peer comments, we would like to withdraw this paper.", ["Freedom (newspaper)"]], ["Generalized Solution Concepts in Games with Possibly Unaware Players", "Most work in game theory assumes that players are perfect reasoners and have common knowledge of all significant aspects of the game. In earlier work, we proposed a framework for representing and analyzing games with possibly unaware players, and suggested a generalization of Nash equilibrium appropriate for games with unaware players that we called generalized Nash equilibrium. Here, we use this framework to analyze other solution concepts that have been considered in the game-theory literature, with a focus on sequential equilibrium. We also provide some insight into the notion of generalized Nash equilibrium by proving that it is closely related to the notion of rationalizability when we restrict the analysis to games in normal form and no unawareness is involved.", ["Nash equilibrium", "Game theory", "Literature"]], ["Throughput Scaling Laws for Wireless Networks with Fading Channels", "A network of n communication links, operating over a shared wireless channel, is considered. Fading is assumed to be the dominant factor affecting the strength of the channels between transmitter and receiver terminals. It is assumed that each link can be active and transmit with a constant power P or remain silent. The objective is to maximize the throughput over the selection of active links. By deriving an upper bound and a lower bound, it is shown that in the case of Rayleigh fading (i) the maximum throughput scales like $\\log n$ (ii) the maximum throughput is achievable in a distributed fashion. The upper bound is obtained using probabilistic methods, where the key point is to upper bound the throughput of any random set of active links by a chi-squared random variable. To obtain the lower bound, a decentralized link activation strategy is proposed and analyzed.", ["Random variable", "Rayleigh fading", "Chi-square distribution", "Wireless", "Communication", "Throughput", "Wireless network", "Channel (communications)", "Transmitter"]], ["Removing Manually-Generated Boilerplate from Electronic Texts: Experiments with Project Gutenberg e-Books", "Collaborative work on unstructured or semi-structured documents, such as in literature corpora or source code, often involves agreed upon templates containing metadata. These templates are not consistent across users and over time. Rule-based parsing of these templates is expensive to maintain and tends to fail as new documents are added. Statistical techniques based on frequent occurrences have the potential to identify automatically a large fraction of the templates, thus reducing the burden on the programmers. We investigate the case of the Project Gutenberg corpus, where most documents are in ASCII format with preambles and epilogues that are often copied and pasted or manually typed. We show that a statistical approach can solve most cases though some documents require knowledge of English. We also survey various technical solutions that make our approach applicable to large data sets.", ["ASCII", "Project Gutenberg", "Source code", "Boilerplate (spaceflight)", "Literature", "Metadata", "Text corpus", "Data"]], ["A note on minimal matching covered graphs", "A graph is called matching covered if for its every edge there is a maximum matching containing it. It is shown that minimal matching covered graphs contain a perfect matching.", ["Matching (graph theory)", "Graph (mathematics)"]], ["On trees with a maximum proper partial 0-1 coloring containing a maximum matching", "I prove that in a tree in which the distance between any two endpoints is even, there is a maximum proper partial 0-1 coloring such that the edges colored by 0 form a maximum matching.", ["Matching (graph theory)", "Tree"]], ["Bandlimited Field Reconstruction for Wireless Sensor Networks", "Wireless sensor networks are often used for environmental monitoring applications. In this context sampling and reconstruction of a physical field is one of the most important problems to solve. We focus on a bandlimited field and find under which conditions on the network topology the reconstruction of the field is successful, with a given probability. We review irregular sampling theory, and analyze the problem using random matrix theory. We show that even a very irregular spatial distribution of sensors may lead to a successful signal reconstruction, provided that the number of collected samples is large enough with respect to the field bandwidth. Furthermore, we give the basis to analytically determine the probability of successful field reconstruction.", ["Matrix (mathematics)", "Bandlimiting", "Random matrix", "Sensor", "Topology", "Network topology", "Wireless sensor network", "Reconstruction Era of the United States", "Probability", "Bandwidth (signal processing)", "Sampling (statistics)"]], ["Moveability and Collision Analysis for Fully-Parallel Manipulators", "The aim of this paper is to characterize the moveability of fully-parallel manipulators in the presence of obstacles. Fully parallel manipulators are used in applications where accuracy, stiffness or high speeds and accelerations are required \\cite{Merlet:97}. However, one of its main drawbacks is a relatively small workspace compared to the one of serial manipulators. This is due mainly to the existence of potential internal collisions, and the existence of singularities. In this paper, the notion of free aspect is defined which permits to exhibit domains of the workspace and the joint space free of singularity and collision. The main application of this study is the moveability analysis in the workspace of the manipulator as well as path-planning, control and design.", ["Stiffness"]], ["A Normalizing Intuitionistic Set Theory with Inaccessible Sets", "We propose a set theory strong enough to interpret powerful type theories underlying proof assistants such as LEGO and also possibly Coq, which at the same time enables program extraction from its constructive proofs. For this purpose, we axiomatize an impredicative constructive version of Zermelo-Fraenkel set theory IZF with Replacement and $\\omega$-many inaccessibles, which we call \\izfio. Our axiomatization utilizes set terms, an inductive definition of inaccessible sets and the mutually recursive nature of equality and membership relations. It allows us to define a weakly-normalizing typed lambda calculus corresponding to proofs in \\izfio according to the Curry-Howard isomorphism principle. We use realizability to prove the normalization theorem, which provides a basis for program extraction capability.", ["Zermelo-Fraenkel set theory", "Lambda calculus", "Axiomatic system", "Impredicativity", "Set theory", "Abraham Fraenkel", "Theorem", "Ernst Zermelo", "Isomorphism", "Haskell Curry", "Typed lambda calculus", "Recursive definition", "Intuitionistic logic", "Realizability", "Mathematical proof", "Calculus", "Lego", "Interactive theorem proving"]], ["Working Modes and Aspects in Fully-Parallel Manipulator", "The aim of this paper is to characterize the notion of aspect in the workspace and in the joint space for parallel manipulators. In opposite to the serial manipulators, the parallel manipulators can admit not only multiple inverse kinematic solutions, but also multiple direct kinematic solutions. The notion of aspect introduced for serial manipulators in [Borrel 86], and redefined for parallel manipulators with only one inverse kinematic solution in [Wenger 1997], is redefined for general fully parallel manipulators. Two Jacobian matrices appear in the kinematic relations between the joint-rate and the Cartesian-velocity vectors, which are called the \"inverse kinematics\" and the \"direct kinematics\" matrices. The study of these matrices allow to respectively define the parallel and the serial singularities. The notion of working modes is introduced to separate inverse kinematic solutions. Thus, we can find out domains of the workspace and the joint space exempt of singularity. Application of this study is the moveability analysis in the workspace of the manipulator as well as path-planing and control. This study is illustrated in this paper with a RR-RRR planar parallel manipulator.", ["Inverse kinematics", "Jacobian matrix and determinant", "Velocity", "Cartesian coordinate system", "Matrix (mathematics)", "Euclidean vector", "Kinematics", "Mathematical singularity"]], ["On the error exponent of variable-length block-coding schemes over finite-state Markov channels with feedback", "The error exponent of Markov channels with feedback is studied in the variable-length block-coding setting. Burnashev's classic result is extended and a single letter characterization for the reliability function of finite-state Markov channels is presented, under the assumption that the channel state is causally observed both at the transmitter and at the receiver side. Tools from stochastic control theory are used in order to treat channels with intersymbol interference. In particular the convex analytical approach to Markov decision processes is adopted to handle problems with stopping time horizons arising from variable-length coding schemes.", ["Intersymbol interference", "Control theory", "Exponentiation", "Stochastic control", "Stochastic", "Causality", "Transmitter", "Finite set", "Variable-length code"]], ["The Isoconditioning Loci of A Class of Closed-Chain Manipulators", "The subject of this paper is a special class of closed-chain manipulators. First, we analyze a family of two-degree-of-freedom (dof) five-bar planar linkages. Two Jacobian matrices appear in the kinematic relations between the joint-rate and the Cartesian-velocity vectors, which are called the ``inverse kinematics\" and the \"direct kinematics\" matrices. It is shown that the loci of points of the workspace where the condition number of the direct-kinematics matrix remains constant, i.e., the isoconditioning loci, are the coupler points of the four-bar linkage obtained upon locking the middle joint of the linkage. Furthermore, if the line of centers of the two actuated revolutes is used as the axis of a third actuated revolute, then a three-dof hybrid manipulator is obtained. The isoconditioning loci of this manipulator are surfaces of revolution generated by the isoconditioning curves of the two-dof manipulator, whose axis of symmetry is that of the third actuated revolute.", ["Revolute joint", "Condition number", "Four-bar linkage", "Surface of revolution", "Inverse kinematics", "Jacobian matrix and determinant", "Cartesian coordinate system", "Matrix (mathematics)", "Kinematics", "Reflection symmetry", "Symmetry", "Velocity", "Locus (mathematics)", "Plane (geometry)"]], ["Workspace and Assembly modes in Fully-Parallel Manipulators : A Descriptive Study", "The goal of this paper is to explain, using a typical example, the distribution of the different assembly modes in the workspace and their effective role in the execution of trajectories. The singular and non-singular changes of assembly mode are described and compared to each other. The non-singular change of assembly mode is more deeply analysed and discussed in the context of trajectory planning. In particular, it is shown that, according to the location of the initial and final configurations with respect to the uniqueness domains in the workspace, there are three different cases to consider before planning a linking trajectory.", ["Singular point of an algebraic variety"]], ["Conception Isotropique D'Une Morphologie Parall\\`Ele : Application \\`a L'Usinage", "The aim of this paper is the isotropic design of a hybrid morphology dedicated to 3-axis machining applications. It is necessary to ensure the feasibility of continuous, singularity-free trajectories, as well as a good manipulability in position and velocity. We want to propose an alternative design to conventional serial machine-tools. We compare a serial PPP machine-tool (three prismatic orthogonal axes) with a hybrid architecture which we optimize only the first two axes. The critrerion used for the optimization is the conditioning of the Jacobian matrices. The optimum, namely isotropy, can be obtained which provides our architecture with excellent manipulability properties.", ["Mathematical optimization", "Velocity", "Jacobian matrix and determinant", "Machining", "Isotropy", "Machine tool", "Trajectory", "Architecture", "Orthogonality", "Hybrid (biology)"]], ["A distributed Approach for Access and Visibility Task under Ergonomic Constraints with a Manikin in a Virtual Reality Environment", "This paper presents a new method, based on a multi-agent system and on digital mock-up technology, to assess an efficient path planner for a manikin for access and visibility task under ergonomic constraints. In order to solve this problem, the human operator is integrated in the process optimization to contribute to a global perception of the environment. This operator cooperates, in real-time, with several automatic local elementary agents. The result of this work validates solutions brought by digital mock-up and that can be applied to simulate maintenance task.", ["Multi-agent system", "Mathematical optimization", "Technology", "Perception", "Virtual reality", "Process optimization", "Ergonomics", "Manikin"]], ["A Training based Distributed Non-Coherent Space-Time Coding Strategy", "Unitary space-time modulation is known to be an efficient means to communicate over non-coherent Multiple Input Multiple Output (MIMO) channels. In this letter, differential unitary space-time coding and non-coherent space-time coding for the training based approach of Kim and Tarokh are addressed. For this approach, necessary and sufficient conditions for multi-group decodability are derived in a simple way assuming a Generalized Likelihood Ratio Test receiver and a unitary codebook. Extending Kim and Tarokh's approach for colocated MIMO systems, a novel training based approach to distributed non-coherent space-time coding for wireless relay networks is proposed. An explicit construction of two-group decodable distributed non-coherent space-time codes achieving full cooperative diversity for all even number of relays is provided.", ["Spacetime", "Cooperative diversity", "Wireless", "MIMO", "Codebook"]], ["On complexity of special maximum matchings constructing", "For bipartite graphs the NP-completeness is proved for the problem of existence of maximum matching which removal leads to a graph with given lower(upper)bound for the cardinality of its maximum matching.", ["Matching (graph theory)", "Cardinality", "NP-complete", "Graph (mathematics)", "Bipartite graph", "Upper and lower bounds", "NP (complexity)", "Glossary of graph theory", "Completeness"]], ["Splay Trees, Davenport-Schinzel Sequences, and the Deque Conjecture", "We introduce a new technique to bound the asymptotic performance of splay trees. The basic idea is to transcribe, in an indirect fashion, the rotations performed by the splay tree as a Davenport-Schinzel sequence S, none of whose subsequences are isomorphic to fixed forbidden subsequence. We direct this technique towards Tarjan's deque conjecture and prove that n deque operations require O(n alpha^*(n)) time, where alpha^*(n) is the minimum number of applications of the inverse-Ackermann function mapping n to a constant. We are optimistic that this approach could be directed towards other open conjectures on splay trees such as the traversal and split conjectures.", ["Ackermann function", "Isomorphism", "Robert Tarjan", "Double-ended queue", "Function (mathematics)", "Splay tree", "Big O notation", "Conjecture", "Inverse function", "Subsequence", "Sequence", "Map (mathematics)"]], ["Benefit of Delay on the Diversity-Multiplexing Tradeoffs of MIMO Channels with Partial CSI", "This paper re-examines the well-known fundamental tradeoffs between rate and reliability for the multi-antenna, block Rayleigh fading channel in the high signal to noise ratio (SNR) regime when (i) the transmitter has access to (noiseless) one bit per coherence-interval of causal channel state information (CSI) and (ii) soft decoding delays together with worst-case delay guarantees are acceptable. A key finding of this work is that substantial improvements in reliability can be realized with a very short expected delay and a slightly longer (but bounded) worst-case decoding delay guarantee in communication systems where the transmitter has access to even one bit per coherence interval of causal CSI. While similar in spirit to the recent work on communication systems based on automatic repeat requests (ARQ) where decoding failure is known at the transmitter and leads to re-transmission, here transmit side-information is purely based on CSI. The findings reported here also lend further support to an emerging understanding that decoding delay (related to throughput) and codeword blocklength (related to coding complexity and delays) are distinctly different design parameters which can be tuned to control reliability.", ["Rayleigh fading", "Signal-to-noise ratio", "Channel state information", "Fading", "Signal (electronics)", "Transmission (telecommunications)", "Multiplexing", "Code word", "Antenna (radio)", "MIMO", "Communication", "Telecommunication", "Forward error correction", "Automatic repeat request", "Channel (communications)", "Transmitter", "Bit", "Throughput"]], ["Design of Multistage Decimation Filters Using Cyclotomic Polynomials: Optimization and Design Issues", "This paper focuses on the design of multiplier-less decimation filters suitable for oversampled digital signals. The aim is twofold. On one hand, it proposes an optimization framework for the design of constituent decimation filters in a general multistage decimation architecture. The basic building blocks embedded in the proposed filters belong, for a simple reason, to the class of cyclotomic polynomials (CPs): the first 104 CPs have a z-transfer function whose coefficients are simply {-1,0,+1}. On the other hand, the paper provides a bunch of useful techniques, most of which stemming from some key properties of CPs, for designing the proposed filters in a variety of architectures. Both recursive and non-recursive architectures are discussed by focusing on a specific decimation filter obtained as a result of the optimization algorithm. Design guidelines are provided with the aim to simplify the design of the constituent decimation filters in the multistage chain.", ["Polynomial", "Mathematical optimization", "Transfer function", "Decimation (comics)", "Algorithm", "Oversampling", "Root of unity", "Recursion", "Architecture"]], ["Mod\\'elisation Dynamique d'un Robot Parall\\`ele \\`a 3-DDL : l'Orthoglide", "In this article, we propose a method for calculation of the inverse and direct dynamic models of the Orthoglide, a parallel robot with threedegrees of freedom in translation. These models are calculated starting from the elements of the dynamic model of the kinematic chain structure and equations of Newton-Euler applied to the platform. These models are obtained in explicit form having an interesting physical interpretation.", ["Leonhard Euler", "Mathematical model", "Kinematic chain"]], ["Word statistics in Blogs and RSS feeds: Towards empirical universal evidence", "We focus on the statistics of word occurrences and of the waiting times between such occurrences in Blogs. Due to the heterogeneity of words' frequencies, the empirical analysis is performed by studying classes of \"frequently-equivalent\" words, i.e. by grouping words depending on their frequencies. Two limiting cases are considered: the dilute limit, i.e. for those words that are used less than once a day, and the dense limit for frequent words. In both cases, extreme events occur more frequently than expected from the Poisson hypothesis. These deviations from Poisson statistics reveal non-trivial time correlations between events that are associated with bursts of activities. The distribution of waiting times is shown to behave like a stretched exponential and to have the same shape for different sets of words sharing a common frequency, thereby revealing universal features.", ["Poisson distribution", "Frequency", "Homogeneity and heterogeneity", "Empirical", "RSS", "Statistics", "Probability distribution", "Hypothesis"]], ["Degeneracy study of the forward kinematics of planar 3-RPR parallel manipulators", "This paper investigates two situations in which the forward kinematics of planar 3-RPR parallel manipulators degenerates. These situations have not been addressed before. The first degeneracy arises when the three input joint variables r1, r2 and r3 satisfy a certain relationship. This degeneracy yields a double root of the characteristic polynomial in t, which could be erroneously interpreted as two coalesce assembly modes. But, unlike what arises in non-degenerate cases, this double root yields two sets of solutions for the position coordinates (x, y) of the platform. In the second situation, we show that the forward kinematics degenerates over the whole joint space if the base and platform triangles are congruent and the platform triangle is rotated by 180 deg about one of its sides. For these \"degenerate\" manipulators, which are defined here for the first time, the forward kinematics is reduced to the solution of a 3rd-degree polynomial and a quadratics in sequence. Such manipulators constitute, in turn, a new family of analytic planar manipulators that would be more suitable for industrial applications.", ["Characteristic polynomial", "Quadratic equation", "Congruence (geometry)", "Zero of a function", "Multiplicity (mathematics)", "Coordinate system", "Plane (geometry)", "Variable (mathematics)", "Polynomial"]], ["Kinematic Analysis of a Family of 3R Manipulators", "The workspace topologies of a family of 3-revolute (3R) positioning manipulators are enumerated. The workspace is characterized in a half-cross section by the singular curves. The workspace topology is defined by the number of cusps that appear on these singular curves. The design parameters space is shown to be divided into five domains where all manipulators have the same number of cusps. Each separating surface is given as an explicit expression in the DH-parameters. As an application of this work, we provide a necessary and sufficient condition for a 3R orthogonal manipulator to be cuspidal, i.e. to change posture without meeting a singularity. This condition is set as an explicit expression in the DH parameters.", ["Orthogonality", "Kinematics", "Topology", "Sufficient condition", "Necessary and sufficient condition"]], ["The Computation of All 4R Serial Spherical Wrists With an Isotropic Architecture", "A spherical wrist of the serial type with n revolute (R) joints is said to be isotropic if it can attain a posture whereby the singular values of its Jacobian matrix are all equal to sqrt(n/3). What isotropy brings about is robustness to manufacturing, assembly, and measurement errors, thereby guaranteeing a maximum orientation accuracy. In this paper we investigate the existence of redundant isotropic architectures, which should add to the dexterity of the wrist under design by virtue of its extra degree of freedom. The problem formulation, for, leads to a system of eight quadratic equations with eight unknowns. The Bezout number of this system is thus 2^8=256, its BKK bound being 192. However, the actual number of solutions is shown to be 32. We list all solutions of the foregoing algebraic problem. All these solutions are real, but distinct solutions do not necessarily lead to distinct manipulators. Upon discarding those algebraic solutions that yield no new wrists, we end up with exactly eight distinct architectures, the eight corresponding manipulators being displayed at their isotropic postures.", ["Quadratic equation", "Isotropy", "Jacobian matrix and determinant", "Observational error"]], ["A design oriented study for 3R Orthogonal Manipulators With Geometric Simplifications", "This paper proposes a method to calculate the largest Regular Dextrous Workspace (RDW) of some types of three-revolute orthogonal manipulators that have at least one of their DH parameters equal to zero. Then a new performance index based on the RDW is introduced, the isocontours of this index are plotted in the parameter space of the interesting types of manipulators and finally an inspection of the domains of the parameter spaces is conducted in order to identify the better manipulator architectures. The RDW is a part of the workspace whose shape is regular (cube, cylinder) and the performances (conditioning index) are bounded inside. The groups of 3R orthogonal manipulators studied have interesting kinematic properties such as, a well-connected workspace that is fully reachable with four inverse kinematic solutions and that does not contain any void. This study is of high interest for the design of alternative manipulator geometries.", ["Cube", "Geometry", "Inverse kinematics", "Parameter space", "Orthogonality", "Parameter"]], ["Separable convex optimization problems with linear ascending constraints", "Separable convex optimization problems with linear ascending inequality and equality constraints are addressed in this paper. Under an ordering condition on the slopes of the functions at the origin, an algorithm that determines the optimum point in a finite number of steps is described. The optimum value is shown to be monotone with respect to a partial order on the constraint parameters. Moreover, the optimum value is convex with respect to these parameters. Examples motivated by optimizations for communication systems are used to illustrate the algorithm.", ["Convex optimization", "Algorithm", "Convex set", "Separable space", "Partially ordered set", "Mathematical optimization", "Finite set", "Communication"]], ["Design of a Spherical Wrist with Parallel Architecture: Application to Vertebrae of an Eel Robot", "The design of a spherical wrist with parallel architecture is the object of this article. This study is part of a larger project, which aims to design and to build an eel robot for inspection of immersed piping. The kinematic analysis of the mechanism is presented first to characterize the singular configurations as well as the isotropic configurations. We add the design constraints related to the application, such as (i) the compactness of the mechanism, (ii) the symmetry of the elements in order to ensure static and dynamic balance and (iii) the possibility of the mechanism to fill the elliptic form of the ell sections.", ["Eel", "Kinematics", "Isotropy", "Sphere", "Symmetry", "Compact space", "Vertebral column", "Architecture", "Parallel computing", "Ell", "Robot"]], ["Passive Control Architecture for Virtual Humans", "In the present paper, we introduce a new control architecture aimed at driving virtual humans in interaction with virtual environments, by motion capture. It brings decoupling of functionalities, and also of stability thanks to passivity. We show projections can break passivity, and thus must be used carefully. Our control scheme enables task space and internal control, contact, and joint limits management. Thanks to passivity, it can be easily extended. Besides, we introduce a new tool as for manikin's control, which makes it able to build passive projections, so as to guide the virtual manikin when sharp movements are needed.", ["Motion capture", "Virtual reality", "Manikin", "Architecture", "Human"]], ["An Integrated Crosscutting Concern Migration Strategy and its Application to JHotDraw", "In this paper we propose a systematic strategy for migrating crosscutting concerns in existing object-oriented systems to aspect-based solutions. The proposed strategy consists of four steps: mining, exploration, documentation and refactoring of crosscutting concerns. We discuss in detail a new approach to aspect refactoring that is fully integrated with our strategy, and apply the whole strategy to an object-oriented system, namely the JHotDraw framework. The result of this migration is made available as an open-source project, which is the largest aspect refactoring available to date. We report on our experiences with conducting this case study and reflect on the success and challenges of the migration process, as well as on the feasibility of automatic aspect refactoring.", ["Mining", "Code refactoring"]], ["Worm Epidemics in Wireless Adhoc Networks", "A dramatic increase in the number of computing devices with wireless communication capability has resulted in the emergence of a new class of computer worms which specifically target such devices. The most striking feature of these worms is that they do not require Internet connectivity for their propagation but can spread directly from device to device using a short-range radio communication technology, such as WiFi or Bluetooth. In this paper, we develop a new model for epidemic spreading of these worms and investigate their spreading in wireless ad hoc networks via extensive Monte Carlo simulations. Our studies show that the threshold behaviour and dynamics of worm epidemics in these networks are greatly affected by a combination of spatial and temporal correlations which characterize these networks, and are significantly different from the previously studied epidemics in the Internet.", ["Communication", "Bluetooth", "Wireless ad hoc network", "Computing", "Radio", "Technology", "Wi-Fi", "Ad hoc", "Information and communications technology", "Computer worm", "Epidemic", "Computer", "Monte Carlo method", "Wireless"]], ["Two polynomial algorithms for special maximum matching constructing in trees", "For an arbitrary tree we investigate the problems of constructing a maximum matching which minimizes or maximizes the cardinality of a maximum matching of the graph obtained from original one by its removal and present corresponding polynomial algorithms.", ["Matching (graph theory)", "Algorithm", "Graph (mathematics)", "Cardinality"]], ["A Cultural Market Model", "Social interactions and personal tastes shape our consumption behavior of cultural products. In this study, we present a computational model of a cultural market and we aim to analyze the behavior of the consumer population as an emergent phenomena. Our results suggest that the final market shares of cultural products dramatically depend on consumer heterogeneity and social interaction pressure. Furthermore, the relation between the resulting market shares and social interaction is robust with respect to a wide range of variation in the parameter values and the type of topology.", ["Topology", "Social relation", "Consumer", "Homogeneity and heterogeneity", "Computational model", "Emergence", "Parameter", "Phenomenon"]], ["Memory efficient scheduling of Strassen-Winograd's matrix multiplication algorithm", "We propose several new schedules for Strassen-Winograd's matrix multiplication algorithm, they reduce the extra memory allocation requirements by three different means: by introducing a few pre-additions, by overwriting the input matrices, or by using a first recursive level of classical multiplication. In particular, we show two fully in-place schedules: one having the same number of operations, if the input matrices can be overwritten; the other one, slightly increasing the constant of the leading term of the complexity, if the input matrices are read-only. Many of these schedules have been found by an implementation of an exhaustive search algorithm based on a pebble game.", ["Brute-force search", "Recursion", "Search algorithm", "Dynamic memory allocation", "Algorithm", "Matrix multiplication", "In-place algorithm"]], ["Tripartitions do not always discriminate phylogenetic networks", "Phylogenetic networks are a generalization of phylogenetic trees that allow for the representation of non-treelike evolutionary events, like recombination, hybridization, or lateral gene transfer. In a recent series of papers devoted to the study of reconstructibility of phylogenetic networks, Moret, Nakhleh, Warnow and collaborators introduced the so-called {tripartition metric for phylogenetic networks. In this paper we show that, in fact, this tripartition metric does not satisfy the separation axiom of distances (zero distance means isomorphism, or, in a more relaxed version, zero distance means indistinguishability in some specific sense) in any of the subclasses of phylogenetic networks where it is claimed to do so. We also present a subclass of phylogenetic networks whose members can be singled out by means of their sets of tripartitions (or even clusters), and hence where the latter can be used to define a meaningful metric.", ["Gene", "Hybrid (biology)", "Separation axiom", "Horizontal gene transfer", "Class (biology)", "Axiom", "Isomorphism", "Phylogenetics", "Phylogenetic tree"]], ["Pricing Asian Options for Jump Diffusions", "We construct a sequence of functions that uniformly converge (on compact sets) to the price of Asian option, which is written on a stock whose dynamics follows a jump diffusion, exponentially fast. Each of the element in this sequence solves a parabolic partial differen- tial equation (not an integro-differential equation). As a result we obtain a fast numerical approximation scheme whose accuracy versus speed characteristics can be controlled. We analyze the performance of our numerical algorithm on several examples.", ["Differential equation", "Algorithm", "Uniform convergence", "Equation", "Sequence", "Race and ethnicity in the United States Census", "Limit of a sequence", "Numerical analysis", "Approximation", "Compact space", "Diffusion", "Parabola"]], ["On the Polyphase Decomposition for Design of Generalized Comb Decimation Filters", "Generalized comb filters (GCFs) are efficient anti-aliasing decimation filters with improved selectivity and quantization noise (QN) rejection performance around the so called folding bands with respect to classical comb filters. In this paper, we address the design of GCF filters by proposing an efficient partial polyphase architecture with the aim to reduce the data rate as much as possible after the Sigma-Delta A/D conversion. We propose a mathematical framework in order to completely characterize the dependence of the frequency response of GCFs on the quantization of the multipliers embedded in the proposed filter architecture. This analysis paves the way to the design of multiplier-less decimation architectures. We also derive the impulse response of a sample 3rd order GCF filter used as a reference scheme throughout the paper.", ["Analog-to-digital converter", "Impulse response", "Aliasing", "Quantization (signal processing)", "Quantization error", "Frequency response", "Frequency", "Spatial anti-aliasing", "Noise", "Filter (signal processing)", "Sampling (signal processing)", "Dirac delta function", "Electronic filter", "Decimation (comics)", "Delta-sigma modulation", "Mathematics", "Polyphase system"]], ["Multiple-Description Lattice Vector Quantization", "In this thesis, we construct and analyze multiple-description codes based on lattice vector quantization.", ["Vector quantization", "Lattice (group)"]], ["Mixed Integer Linear Programming For Exact Finite-Horizon Planning In Decentralized Pomdps", "We consider the problem of finding an n-agent joint-policy for the optimal finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new mathematical programming approach for the problem. Our approach is based on two ideas: First, we represent each agent's policy in the sequence-form and not in the tree-form, thereby obtaining a very compact representation of the set of joint-policies. Second, using this compact representation, we solve this problem as an instance of combinatorial optimization for which we formulate a mixed integer linear program (MILP). The optimal solution of the MILP directly yields an optimal joint-policy for the Dec-Pomdp. Computational experience shows that formulating and solving the MILP requires significantly less time to solve benchmark Dec-Pomdp problems than existing algorithms. For example, the multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP whereas existing algorithms require several hours to solve it.", ["Integer linear program", "Combinatorial optimization", "Mathematics", "Combinatorics", "Linear programming", "Finite set", "Mathematical optimization", "Integer", "Algorithm", "Multi-agent system", "Compact space", "Optimization problem", "NEXPTIME"]], ["Rate and Power Allocation for Discrete-Rate Link Adaptation", "Link adaptation, in particular adaptive coded modulation (ACM), is a promising tool for bandwidth-efficient transmission in a fading environment. The main motivation behind employing ACM schemes is to improve the spectral efficiency of wireless communication systems. In this paper, using a finite number of capacity achieving component codes, we propose new transmission schemes employing constant power transmission, as well as discrete and continuous power adaptation, for slowly varying flat-fading channels. We show that the proposed transmission schemes can achieve throughputs close to the Shannon limits of flat-fading channels using only a small number of codes. Specifically, using a fully discrete scheme with just four codes, each associated with four power levels, we achieve a spectral efficiency within 1 dB of the continuous-rate continuous-power Shannon capacity. Furthermore, when restricted to a fixed number of codes, the introduction of power adaptation has significant gains with respect to ASE and probability of no transmission compared to a constant power scheme.", ["Decibel", "Channel capacity", "Probability", "Modulation", "Link adaptation", "Telecommunication", "Spectral efficiency", "Transmission (telecommunications)", "Wireless", "Communication", "Bandwidth (computing)"]], ["A Characterisation of First-Order Constraint Satisfaction Problems", "We describe simple algebraic and combinatorial characterisations of finite relational core structures admitting finitely many obstructions. As a consequence, we show that it is decidable to determine whether a constraint satisfaction problem is first-order definable: we show the general problem to be NP-complete, and give a polynomial-time algorithm in the case of cores. A slight modification of this algorithm provides, for first-order definable CSP's, a simple poly-time algorithm to produce a solution when one exists. As an application of our algebraic characterisation of first order CSP's, we describe a large family of L-complete CSP's.", ["Constraint satisfaction problem", "NP-complete", "L (complexity)", "Combinatorics", "Finite set", "Polynomial", "Time complexity", "Constraint satisfaction", "NP (complexity)", "First-order logic", "Algorithm", "Decidability (logic)"]], ["Multi-physics Extension of OpenFMO Framework", "OpenFMO framework, an open-source software (OSS) platform for Fragment Molecular Orbital (FMO) method, is extended to multi-physics simulations (MPS). After reviewing the several FMO implementations on distributed computer environments, the subsequent development planning corresponding to MPS is presented. It is discussed which should be selected as a scientific software, lightweight and reconfigurable form or large and self-contained form.", ["Open-source software", "Physics", "Distributed computing", "Open source"]], ["A fixed point iteration for computing the matrix logarithm", "In various areas of applied numerics, the problem of calculating the logarithm of a matrix A emerges. Since series expansions of the logarithm usually do not converge well for matrices far away from the identity, the standard numerical method calculates successive square roots. In this article, a new algorithm is presented that relies on the computation of successive matrix exponentials. Convergence of the method is demonstrated for a large class of initial matrices and favorable choices of the initial matrix are discussed.", ["Logarithm of a matrix", "Fixed point (mathematics)", "Numerical analysis", "Fixed point iteration", "Logarithm", "Algorithm", "Exponential function", "Computing", "Square root", "Computation", "Matrix (mathematics)", "Limit of a sequence"]], ["Animation of virtual mannequins, robot-like simulation or motion captures", "In order to optimize the costs and time of design of the new products while improving their quality, concurrent engineering is based on the digital model of these products, the numerical model. However, in order to be able to avoid definitively physical model, old support of the design, without loss of information, new tools must be available. Especially, a tool making it possible to check simply and quickly the maintainability of complex mechanical sets using the numerical model is necessary. Since one decade, our team works on the creation of tool for the generation and the analysis of trajectories of virtual mannequins. The simulation of human tasks can be carried out either by robot-like simulation or by simulation by motion capture. This paper presents some results on the both two methods. The first method is based on a multi-agent system and on a digital mock-up technology, to assess an efficient path planner for a manikin or a robot for access and visibility task taking into account ergonomic constraints or joint and mechanical limits. In order to solve this problem, the human operator is integrated in the process optimization to contribute to a global perception of the environment. This operator cooperates, in real-time, with several automatic local elementary agents. In the case of the second approach, we worked with the CEA and EADS/CCR to solve the constraints related to the evolution of human virtual in its environment on the basis of data resulting from motion capture system. An approach using of the virtual guides was developed to allow to the user the realization of precise trajectory in absence of force feedback. The result of this work validates solutions through the digital mock-up; it can be applied to simulate maintenability and mountability tasks.", ["Multi-agent system", "Engineering", "Animation", "EADS", "Robot", "Motion capture", "Haptic technology", "Computer simulation", "Perception", "Evolution", "Mathematical optimization", "Mockup", "Technology", "Simulation", "Process optimization", "Trajectory", "Maintainability", "Concurrent engineering", "Ergonomics", "Feedback", "Mannequin", "Conceptual model"]], ["A Framework to Illustrate Kinematic Behavior of Mechanisms by Haptic Feedback", "The kinematic properties of mechanisms are well known by the researchers and teachers. The theory based on the study of Jacobian matrices allows us to explain, for example, the singular configuration. However, in many cases, the physical sense of such properties is difficult to explain to students. The aim of this article is to use haptic feedback to render to the user the signification of different kinematic indices. The framework uses a Phantom Omni and a serial and parallel mechanism with two degrees of freedom. The end-effector of both mechanisms can be moved either by classical mouse, or Phantom Omni with or without feedback.", ["Matrix (mathematics)", "Jacobian matrix and determinant", "Feedback", "Haptic technology"]], ["On the ergodic sum-rate performance of CDD in multi-user systems", "The main focus of space-time coding design and analysis for MIMO systems has been so far focused on single-user systems. For single-user systems, transmit diversity schemes suffer a loss in spectral efficiency if the receiver is equipped with more than one antenna, making them unsuitable for high rate transmission. One such transmit diversity scheme is the cyclic delay diversity code (CDD). The advantage of CDD over other diversity schemes such as orthogonal space-time block codes (OSTBC) is that a code rate of one and delay optimality are achieved independent of the number of transmit antennas. In this work we analyze the ergodic rate of a multi-user multiple access channel (MAC) with each user applying such a cyclic delay diversity (CDD) code. We derive closed form expressions for the ergodic sum-rate of multi-user CDD and compare it with the sum-capacity. We study the ergodic rate region and show that in contrast to what is conventionally known regarding the single-user case, transmit diversity schemes are viable candidates for high rate transmission in multi-user systems. Finally, our theoretical findings are illustrated by numerical simulation results.", ["Code rate", "Antenna (radio)", "Orthogonality", "MIMO", "Simulation", "Spectral efficiency", "Ergodic theory", "Computer simulation", "Diversity scheme", "Spacetime"]], ["Distributed Compression and Multiparty Squashed Entanglement", "We study a protocol in which many parties use quantum communication to transfer a shared state to a receiver without communicating with each other. This protocol is a multiparty version of the fully quantum Slepian-Wolf protocol for two senders and arises through the repeated application of the two-sender protocol. We describe bounds on the achievable rate region for the distributed compression problem. The inner bound arises by expressing the achievable rate region for our protocol in terms of its vertices and extreme rays and, equivalently, in terms of facet inequalities. We also prove an outer bound on all possible rates for distributed compression based on the multiparty squashed entanglement, a measure of multiparty entanglement.", ["Communication"]], ["A Comparative Study between Two Three-DOF Parallel Kinematic Machines using Kinetostatic Criteria and Interval Analysis", "This paper addresses the workspace analysis of two 3-DOF translational parallel mechanisms designed for machining applications. The two machines features three fixed linear joints. The joint axes of the first machine are orthogonal whereas these of the second are parallel. In both cases, the mobile platform moves in the Cartesian $x-y-z$ space with fixed orientation. The workspace analysis is conducted on the basis of prescribed kinetostatic performances. Interval analysis based methods are used to compute the dextrous workspace and the largest cube enclosed in this workspace.", ["Orthogonality", "Translation (geometry)", "Machining", "Cartesian coordinate system", "Orientation (vector space)", "Cube"]], ["Multimedia Capacity Analysis of the IEEE 802.11e Contention-based Infrastructure Basic Service Set", "We first propose a simple mathematical analysis framework for the Enhanced Distributed Channel Access (EDCA) function of the recently ratified IEEE 802.11e standard. Our analysis considers the fact that the distributed random access systems exhibit cyclic behavior. The proposed model is valid for arbitrary assignments of AC-specific Arbitration Interframe Space (AIFS) values and Contention Window (CW) sizes and is the first that considers an arbitrary distribution of active Access Categories (ACs) at the stations. Validating the theoretical results via extensive simulations, we show that the proposed analysis accurately captures the EDCA saturation performance. Next, we propose a framework for multimedia capacity analysis of the EDCA function. We calculate an accurate station- and AC-specific queue utilization ratio by appropriately weighing the service time predictions of the cycle time model for different number of active stations. Based on the calculated queue utilization ratio, we design a simple model-based admission control scheme. We show that the proposed call admission control algorithm maintains satisfactory user-perceived quality for coexisting voice and video connections in an infrastructure BSS and does not present over- or under-admission problems of previously proposed models in the literature.", ["Mathematical analysis", "IEEE 802.11e-2005", "Mathematics", "Literature", "Algorithm", "IEEE 802.11", "Infrastructure", "Multimedia", "Institute of Electrical and Electronics Engineers"]], ["The Virtual Manufacturing concept: Scope, Socio-Economic Aspects and Future Trends", "The research area \"Virtual Manufacturing (VM)'' is the use of information technology and computer simulation to model real world manufacturing processes for the purpose of analysing and understanding them. As automation technologies such as CAD/CAM have substantially shortened the time required to design products, Virtual Manufacturing will have a similar effect on the manufacturing phase thanks to the modelling, simulation and optimisation of the product and the processes involved in its fabrication. After a description of Virtual Manufacturing (definitions and scope), we present some socio-economic factors of VM and finaly some \"hot topics'' for the future are proposed.", ["Computer simulation", "Computer-aided design", "Simulation", "Information technology", "Design", "Computer-aided technologies", "Manufacturing", "Computer-aided manufacturing", "Socioeconomics"]], ["A Classification of 3R Orthogonal Manipulators by the Topology of their Workspace", "A classification of a family of 3-revolute (3R) positining manipulators is established. This classification is based on the topology of their workspace. The workspace is characterized in a half-cross section by the singular curves. The workspace topology is defined by the number of cusps and nodes that appear on these singular curves. The design parameters space is shown to be divided into nine domains of distinct workspace topologies, in which all manipulators have similar global kinematic properties. Each separating surface is given as an explicit expression in the DH-parameters.", ["Kinematics", "Topology"]], ["OA@MPS - a colourful view", "The open access agenda of the Max Planck Society, initiator of the Berlin Declaration, envisions the support of both the green way and the golden way to open access. For the implementation of the green way the Max Planck Society through its newly established unit (Max Planck Digital Library) follows the idea of providing a centralized technical platform for publications and a local support for editorial issues. With regard to the golden way, the Max Planck Society fosters the development of open access publication models and experiments new publishing concepts like the Living Reviews journals.", ["Max Planck Society", "Open access (publishing)", "Berlin"]], ["Covering a line segment with variable radius discs", "The paper addresses the problem of locating sensors with a circular field of view so that a given line segment is under full surveillance, which is termed as the Disc Covering Problem on a Line. The cost of each sensor includes a fixed component, and a variable component that is proportional to the field-of-view area. When only one type of sensor or, in general, one type of disc, is available, then a simple polynomial algorithm solves the problem. When there are different types of sensors in terms of fixed and variable costs, the problem becomes NP-hard. A branch-and-bound algorithm as well as an efficient heuristic are developed. The heuristic very often obtains the optimal solution as shown in extensive computational testing.", ["NP-hard", "Algorithm", "Time complexity", "Sensor", "Radius", "Branch and bound", "Heuristic", "Polynomial", "Field of view", "Line segment"]], ["Robust Hypothesis Testing with a Relative Entropy Tolerance", "This paper considers the design of a minimax test for two hypotheses where the actual probability densities of the observations are located in neighborhoods obtained by placing a bound on the relative entropy between actual and nominal densities. The minimax problem admits a saddle point which is characterized. The robust test applies a nonlinear transformation which flattens the nominal likelihood ratio in the vicinity of one. Results are illustrated by considering the transmission of binary data in the presence of additive noise.", ["Nonlinear system", "Probability density function", "Saddle point", "Additive white Gaussian noise", "Relative entropy", "Entropy", "Minimax", "Hypothesis"]], ["Instrumented Collective Learning Situations (ICLS): the Gap between Theoretical Research and Observed Practices", "According to socio-constructivism approach, collective situations are promoted to favor learning in classroom, at a distance or in a blended educational context. So, many Information and Communication Technologies (ICT) are provided to teachers but there are no clear studies about the way they are used and perceived. Our research is based on the hypothesis that practices of educational actors (instructional designers and tutors) are far away from theoretical results of research in education technologies. In this paper, we consider a precise kind of situation: Instrumented Collective Learning Situations (ICLS). By a survey on 13 fields in higher education in France, Switzerland and Canada, we present how ICLS are designed and how teachers used them. Conclusions give an indication on the gap between the way information technologies are prescribed and the way they are actually used and perceived by teachers.", ["Constructivism (learning theory)", "Higher education", "Information technology", "Switzerland", "France", "Communication", "Information and communications technology", "Canada"]], ["Building a Cooperative Communications System", "In this paper, we present the results from over-the-air experiments of a complete implementation of an amplify and forward cooperative communications system. Our custom OFDM-based physical layer uses a distributed version of the Alamouti block code, where the relay sends one branch of Alamouti encoded symbols. First we show analytically and experimentally that amplify and forward protocols are unaffected by carrier frequency offsets at the relay. This result allows us to use a conventional Alamouti receiver without change for the distributed relay system. Our full system implementation shows gains up to 5.5dB in peak power constrained networks. Thus, we can conclusively state that even the simplest form of relaying can lead to significant gains in practical implementations.", ["Terrestrial television", "Decibel", "Block code", "Orthogonal frequency-division multiplexing", "Physical Layer"]], ["Situations d'apprentissage collectives instrument\\'ees : \\'etude de pratiques dans l'enseignement sup\\'erieur", "Currently, educational platforms propose many tools of communication, production, labour division or collective work management in order to support collective activities. But it is not guaranteed that actors (instructional designers, tutors or learner) are really using them. Our work, describe characteristics of instrumented learning situations (ICLS) in the higher education. Our intention is to determine: if ICLS are really existing; which form they take (in terms of scenario, tools, type of activity...) ; if recommendations resulting from research tasks are taken into account by instructional designers and if the instructional designer prescribed activities are really follow by learners or tutors? To answer these questions, we have made a survey about ICLS actors uses.", ["Communication", "Higher education", "Etude"]], ["Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms", "This work aims at optimizing injection networks, which consist in adding a set of long-range links (called bypass links) in mobile multi-hop ad hoc networks so as to improve connectivity and overcome network partitioning. To this end, we rely on small-world network properties, that comprise a high clustering coefficient and a low characteristic path length. We investigate the use of two genetic algorithms (generational and steady-state) to optimize three instances of this topology control problem and present results that show initial evidence of their capacity to solve it.", ["Small-world network", "Clustering coefficient", "Ad hoc", "Genetic algorithm", "Algorithm", "Topology", "Genetics"]], ["p-Adic Modelling of the Genome and the Genetic Code", "The present paper is devoted to foundations of p-adic modelling in genomics. Considering nucleotides, codons, DNA and RNA sequences, amino acids, and proteins as information systems, we have formulated the corresponding p-adic formalisms for their investigations. Each of these systems has its characteristic prime number used for construction of the related information space. Relevance of this approach is illustrated by some examples. In particular, it is shown that degeneration of the genetic code is a p-adic phenomenon. We have also put forward a hypothesis on evolution of the genetic code assuming that primitive code was based on single nucleotides and chronologically first four amino acids. This formalism of p-adic genomic information systems can be implemented in computer programs and applied to various concrete cases.", ["Prime number", "DNA", "RNA", "Genomics", "Genetics", "Evolution", "Computer", "Nucleotide", "Protein", "Amino acid", "Genetic code", "Genome", "Computer program", "Phenomenon"]], ["Universal Reinforcement Learning", "We consider an agent interacting with an unmodeled environment. At each time, the agent makes an observation, takes an action, and incurs a cost. Its actions can influence future observations and costs. The goal is to minimize the long-term average cost. We propose a novel algorithm, known as the active LZ algorithm, for optimal control based on ideas from the Lempel-Ziv scheme for universal data compression and prediction. We establish that, under the active LZ algorithm, if there exists an integer $K$ such that the future is conditionally independent of the past given a window of $K$ consecutive actions and observations, then the average cost converges to the optimum. Experimental results involving the game of Rock-Paper-Scissors illustrate merits of the algorithm.", ["LZ77 and LZ78", "Data compression", "Conditional independence", "Integer", "Algorithm", "Rock-paper-scissors", "Optimal control"]], ["Channel Capacity Estimation using Free Probability Theory", "In many channel measurement applications, one needs to estimate some characteristics of the channels based on a limited set of measurements. This is mainly due to the highly time varying characteristics of the channel. In this contribution, it will be shown how free probability can be used for channel capacity estimation in MIMO systems. Free probability has already been applied in various application fields such as digital communications, nuclear physics and mathematical finance, and has been shown to be an invaluable tool for describing the asymptotic behaviour of many large-dimensional systems. In particular, using the concept of free deconvolution, we provide an asymptotically (w.r.t. the number of observations) unbiased capacity estimator for MIMO channels impaired with noise called the free probability based estimator. Another estimator, called the Gaussian matrix mean based estimator, is also introduced by slightly modifying the free probability based estimator. This estimator is shown to give unbiased estimation of the moments of the channel matrix for any number of observations. Also, the estimator has this property when we extend to MIMO channels with phase off-set and frequency drift, for which no estimator has been provided so far in the literature. It is also shown that both the free probability based and the Gaussian matrix mean based estimator are asymptotically unbiased capacity estimators as the number of transmit antennas go to infinity, regardless of whether phase off-set and frequency drift are present. The limitations in the two estimators are also explained. Simulations are run to assess the performance of the estimators for a low number of antennas and samples to confirm the usefulness of the asymptotic results.", ["Mathematical finance", "Free probability", "Deconvolution", "MIMO", "Frequency", "Probability theory", "Nuclear physics", "Probability", "Channel capacity", "Physics", "Data transmission", "Antenna (radio)", "Infinity", "Finance", "Asymptotic analysis", "Mathematics", "Normal distribution", "Estimator"]], ["Inductive Definition and Domain Theoretic Properties of Fully Abstract", "A construction of fully abstract typed models for PCF and PCF^+ (i.e., PCF + \"parallel conditional function\"), respectively, is presented. It is based on general notions of sequential computational strategies and wittingly consistent non-deterministic strategies introduced by the author in the seventies. Although these notions of strategies are old, the definition of the fully abstract models is new, in that it is given level-by-level in the finite type hierarchy. To prove full abstraction and non-dcpo domain theoretic properties of these models, a theory of computational strategies is developed. This is also an alternative and, in a sense, an analogue to the later game strategy semantics approaches of Abramsky, Jagadeesan, and Malacaria; Hyland and Ong; and Nickau. In both cases of PCF and PCF^+ there are definable universal (surjective) functionals from numerical functions to any given type, respectively, which also makes each of these models unique up to isomorphism. Although such models are non-omega-complete and therefore not continuous in the traditional terminology, they are also proved to be sequentially complete (a weakened form of omega-completeness), \"naturally\" continuous (with respect to existing directed \"pointwise\", or \"natural\" lubs) and also \"naturally\" omega-algebraic and \"naturally\" bounded complete -- appropriate generalisation of the ordinary notions of domain theory to the case of non-dcpos.", ["Domain theory", "Isomorphism", "Finite set", "Continuous function", "Surjective function", "Function (mathematics)", "Sequence", "Abstraction", "Denotational semantics", "Functional (mathematics)", "Semantics", "Up to", "Abstract algebra", "Complete partial order", "Domain of a function", "Hierarchy", "Level (video gaming)"]], ["Stacked OSTBC: Error Performance and Rate Analysis", "It is well known, that the Alamouti scheme is the only space-time code from orthogonal design achieving the capacity of a multiple-input multiple-output (MIMO) wireless communication system with n_T=2 transmit antennas and n_R=1 receive antenna. In this work, we propose the n-times stacked Alamouti scheme for n_T=2n transmit antennas and show that this scheme achieves the capacity in the case of n_R=1 receive antenna. This result may regarded as an extension of the Alamouti case. For the more general case of more than one receive antenna, we show that if the number of transmit antennas is higher than the number of receive antennas we achieve a high portion of the capacity with this scheme. Further, we show that the MIMO capacity is at most twice the rate achieved with the proposed scheme for all SNR. We derive lower and upper bounds for the rate achieved with this scheme and compare it with upper and lower bounds for the capacity. In addition to the capacity analysis based on the assumption of a coherent channel, we analyze the error rate performance of the stacked OSTBC with the optimal ML detector and with the suboptimal lattice-reduction (LR) aided zero-forcing detector. We compare the error rate performance of the stacked OSTBC with spatial multiplexing (SM) and full-diversity achieving schemes. Finally, we illustrate the theoretical results by numerical simulations.", ["MIMO", "Orthogonality", "Spatial multiplexing", "Timecode", "Wireless", "Communication", "Antenna (radio)", "Spacetime", "Multiplexing", "Stacked"]], ["Kinematic and stiffness analysis of the Orthoglide, a PKM with simple, regular workspace and homogeneous performances", "The Orthoglide is a Delta-type PKM dedicated to 3-axis rapid machining applications that was originally developed at IRCCyN in 2000-2001 to meet the advantages of both serial 3-axis machines (regular workspace and homogeneous performances) and parallel kinematic architectures (good dynamic performances and stiffness). This machine has three fixed parallel linear joints that are mounted orthogonally. The geometric parameters of the Orthoglide were defined as function of the size of a prescribed cubic Cartesian workspace that is free of singularities and internal collision. The interesting features of the Orthoglide are a regular Cartesian workspace shape, uniform performances in all directions and good compactness. In this paper, a new method is proposed to analyze the stiffness of overconstrained Delta-type manipulators, such as the Orthoglide. The Orthoglide is then benchmarked according to geometric, kinematic and stiffness criteria: workspace to footprint ratio, velocity and force transmission factors, sensitivity to geometric errors, torsional stiffness and translational stiffness.", ["Orthogonality", "Stiffness", "Geometry", "PK machine gun", "Machining", "Velocity", "Torsion (mechanics)", "Compact space", "Kinematics", "Cartesian coordinate system"]], ["Neutrality and Many-Valued Logics", "In this book, we consider various many-valued logics: standard, linear, hyperbolic, parabolic, non-Archimedean, p-adic, interval, neutrosophic, etc. We survey also results which show the tree different proof-theoretic frameworks for many-valued logics, e.g. frameworks of the following deductive calculi: Hilbert's style, sequent, and hypersequent. We present a general way that allows to construct systematically analytic calculi for a large family of non-Archimedean many-valued logics: hyperrational-valued, hyperreal-valued, and p-adic valued logics characterized by a special format of semantics with an appropriate rejection of Archimedes' axiom. These logics are built as different extensions of standard many-valued logics (namely, Lukasiewicz's, Goedel's, Product, and Post's logics). The informal sense of Archimedes' axiom is that anything can be measured by a ruler. Also logical multiple-validity without Archimedes' axiom consists in that the set of truth values is infinite and it is not well-founded and well-ordered. On the base of non-Archimedean valued logics, we construct non-Archimedean valued interval neutrosophic logic INL by which we can describe neutrality phenomena.", ["Proof theory", "Truth value", "Deductive reasoning", "Axiom", "Archimedes", "Semantics", "David Hilbert", "Infinity", "Sequent", "Logic", "Mathematical proof", "Truth", "Well-order", "Well-founded relation", "Parabola", "Calculus"]], ["Estimation of Small s-t Reliabilities in Acyclic Networks", "In the classical s-t network reliability problem a fixed network G is given including two designated vertices s and t (called terminals). The edges are subject to independent random failure, and the task is to compute the probability that s and t are connected in the resulting network, which is known to be #P-complete. In this paper we are interested in approximating the s-t reliability in case of a directed acyclic original network G. We introduce and analyze a specialized version of the Monte-Carlo algorithm given by Karp and Luby. For the case of uniform edge failure probabilities, we give a worst-case bound on the number of samples that have to be drawn to obtain an epsilon-delta approximation, being sharper than the original upper bound. We also derive a variance reduction of the estimator which reduces the expected number of iterations to perform to achieve the desired accuracy when applied in conjunction with different stopping rules. Initial computational results on two types of random networks (directed acyclic Delaunay graphs and a slightly modified version of a classical random graph) with up to one million vertices are presented. These results show the advantage of the introduced Monte-Carlo approach compared to direct simulation when small reliabilities have to be estimated and demonstrate its applicability on large-scale instances.", ["Random graph", "Expected value", "Variance", "Simulation", "Probability", "Estimator", "Upper and lower bounds", "Continuous function", "Randomness", "P-complete", "Graph (mathematics)", "Independence (probability theory)", "Algorithm", "Approximation", "Classical mechanics"]], ["RS-232 Led Board", "This article demonstrates how to develop a Microchip PIC16F84 based device that supports RS-232 interface with PC. Circuit (LED Board) design and software development will be discussed. PicBasic Pro Compiler from microEngineering Labs, Inc. is used for PIC programming. Development of LED Board Control Console using C/C++ is also briefly discussed. The project requires basic work experience with Microchip PICs, serial communication and programming.", ["RS-232", "Computer software", "PIC microcontroller", "Software development", "Light-emitting diode", "Compiler", "Personal computer", "Serial communication", "C (programming language)", "Communication"]], ["Decentralized sequential change detection using physical layer fusion", "The problem of decentralized sequential detection with conditionally independent observations is studied. The sensors form a star topology with a central node called fusion center as the hub. The sensors make noisy observations of a parameter that changes from an initial state to a final state at a random time where the random change time has a geometric distribution. The sensors amplify and forward the observations over a wireless Gaussian multiple access channel and operate under either a power constraint or an energy constraint. The optimal transmission strategy at each stage is shown to be the one that maximizes a certain Ali-Silvey distance between the distributions for the hypotheses before and after the change. Simulations demonstrate that the proposed analog technique has lower detection delays when compared with existing schemes. Simulations further demonstrate that the energy-constrained formulation enables better use of the total available energy than the power-constrained formulation in the change detection problem.", ["Conditional independence", "Star network", "Topology", "Physical Layer", "Wireless", "Energy", "Channel access method", "Oscilloscope", "Star", "Geometry", "Sensor", "Geometric distribution"]], ["Autonomous tools for Grid management, monitoring and optimization", "We outline design and lines of development of autonomous tools for the computing Grid management, monitoring and optimization. The management is proposed to be based on the notion of utility. Grid optimization is considered to be application-oriented. A generic Grid simulator is proposed as an optimization tool for Grid structure and functionality.", ["Grid computing"]], ["International Standard for a Linguistic Annotation Framework", "This paper describes the Linguistic Annotation Framework under development within ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to serve as a basis for harmonizing existing language resources as well as developing new ones.", ["Linguistics", "International Organization for Standardization", "Annotation", "Language"]], ["A Formal Model of Dictionary Structure and Content", "We show that a general model of lexical information conforms to an abstract model that reflects the hierarchy of information found in a typical dictionary entry. We show that this model can be mapped into a well-formed XML document, and how the XSL transformation language can be used to implement a semantics defined over the abstract model to enable extraction and manipulation of the information in any format.", ["XML", "Dictionary", "Lexicon", "Conceptual model", "Hierarchy"]], ["Statistical mechanical analysis of the linear vector channel in digital communication", "A statistical mechanical framework to analyze linear vector channel models in digital wireless communication is proposed for a large system. The framework is a generalization of that proposed for code-division multiple-access systems in Europhys. Lett. 76 (2006) 1193 and enables the analysis of the system in which the elements of the channel transfer matrix are statistically correlated with each other. The significance of the proposed scheme is demonstrated by assessing the performance of an existing model of multi-input multi-output communication systems.", ["Communication", "Data transmission", "Code division multiple access", "Statistical mechanics", "Transfer function", "Channel access method", "Telecommunication", "Statistics", "Wireless"]], ["Consistency of the group Lasso and multiple kernel learning", "We consider the least-square regression problem with regularization by a block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic model consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied.", ["Functional analysis", "Necessary and sufficient condition", "Euclidean space", "Variable (mathematics)", "David Hilbert", "Dimension", "Norm (mathematics)", "Reproducing kernel Hilbert space", "Infinity", "Least squares", "Regularization (mathematics)"]], ["Faster subsequence recognition in compressed strings", "Computation on compressed strings is one of the key approaches to processing massive data sets. We consider local subsequence recognition problems on strings compressed by straight-line programs (SLP), which is closely related to Lempel--Ziv compression. For an SLP-compressed text of length $\\bar m$, and an uncompressed pattern of length $n$, C{\\'e}gielski et al. gave an algorithm for local subsequence recognition running in time $O(\\bar mn^2 \\log n)$. We improve the running time to $O(\\bar mn^{1.5})$. Our algorithm can also be used to compute the longest common subsequence between a compressed text and an uncompressed pattern in time $O(\\bar mn^{1.5})$; the same problem with a compressed pattern is known to be NP-hard.", ["Data compression", "Algorithm"]], ["Faster exon assembly by sparse spliced alignment", "Assembling a gene from candidate exons is an important problem in computational biology. Among the most successful approaches to this problem is \\emph{spliced alignment}, proposed by Gelfand et al., which scores different candidate exon chains within a DNA sequence of length $m$ by comparing them to a known related gene sequence of length n, $m = \\Theta(n)$. Gelfand et al.\\ gave an algorithm for spliced alignment running in time O(n^3). Kent et al.\\ considered sparse spliced alignment, where the number of candidate exons is O(n), and proposed an algorithm for this problem running in time O(n^{2.5}). We improve on this result, by proposing an algorithm for sparse spliced alignment running in time O(n^{2.25}). Our approach is based on a new framework of \\emph{quasi-local string comparison}.", ["Computational biology", "Exon", "Gene", "DNA", "Algorithm"]], ["A Knowledge-Based Analysis of Global Function Computation", "Consider a distributed system N in which each agent has an input value and each communication link has a weight. Given a global function, that is, a function f whose value depends on the whole network, the goal is for every agent to eventually compute the value f(N). We call this problem global function computation. Various solutions for instances of this problem, such as Boolean function computation, leader election, (minimum) spanning tree construction, and network determination, have been proposed, each under particular assumptions about what processors know about the system and how this knowledge can be acquired. We give a necessary and sufficient condition for the problem to be solvable that generalizes a number of well-known results. We then provide a knowledge-based (kb) program (like those of Fagin, Halpern, Moses, and Vardi) that solves global function computation whenever possible. Finally, we improve the message overhead inherent in our initial kb program by giving a counterfactual belief-based program that also solves the global function computation whenever possible, but where agents send messages only when they believe it is necessary to do so. The latter program is shown to be implemented by a number of well-known algorithms for solving leader election.", ["Boolean function", "Distributed computing", "Spanning tree", "Communication", "Fagin", "Necessary and sufficient condition", "Knowledge", "Computation", "Algorithm"]], ["Zero-automatic queues and product form", "We introduce and study a new model: 0-automatic queues. Roughly, 0-automatic queues are characterized by a special buffering mechanism evolving like a random walk on some infinite group or monoid. The salient result is that all stable 0-automatic queues have a product form stationary distribution and a Poisson output process. When considering the two simplest and extremal cases of 0-automatic queues, we recover the simple M/M/1 queue, and Gelenbe's G-queue with positive and negative customers.", ["Monoid", "Random walk", "Probability distribution", "M/M/1 model", "Poisson distribution", "Stationary distribution"]], ["A Generalized Information Formula as the Bridge between Shannon and Popper", "A generalized information formula related to logical probability and fuzzy set is deduced from the classical information formula. The new information measure accords with to Popper's criterion for knowledge evolution very much. In comparison with square error criterion, the information criterion does not only reflect error of a proposition, but also reflects the particularity of the event described by the proposition. It gives a proposition with less logical probability higher evaluation. The paper introduces how to select a prediction or sentence from many for forecasts and language translations according to the generalized information criterion. It also introduces the rate fidelity theory, which comes from the improvement of the rate distortion theory in the classical information theory by replacing distortion (i.e. average error) criterion with the generalized mutual information criterion, for data compression and communication efficiency. Some interesting conclusions are obtained from the rate-fidelity function in relation to image communication. It also discusses how to improve Popper's theory.", ["Evolution", "Information theory", "Mutual information", "Fuzzy set", "Communication", "Proposition", "Data compression", "Physical information", "Data", "Knowledge", "Karl Popper"]], ["Lattices for Distributed Source Coding: Jointly Gaussian Sources and Reconstruction of a Linear Function", "Consider a pair of correlated Gaussian sources (X1,X2). Two separate encoders observe the two components and communicate compressed versions of their observations to a common decoder. The decoder is interested in reconstructing a linear combination of X1 and X2 to within a mean-square distortion of D. We obtain an inner bound to the optimal rate-distortion region for this problem. A portion of this inner bound is achieved by a scheme that reconstructs the linear function directly rather than reconstructing the individual components X1 and X2 first. This results in a better rate region for certain parameter values. Our coding scheme relies on lattice coding techniques in contrast to more prevalent random coding arguments used to demonstrate achievable rate regions in information theory. We then consider the case of linear reconstruction of K sources and provide an inner bound to the optimal rate-distortion region. Some parts of the inner bound are achieved using the following coding structure: lattice vector quantization followed by \"correlated\" lattice-structured binning.", ["Function (mathematics)", "Linear combination", "Information theory", "Correlation and dependence", "Vector quantization", "Parameter", "Mean", "Linear", "Linear function", "Quantization (signal processing)", "Normal distribution", "Distortion", "Randomness"]], ["Separable and Low-Rank Continuous Games", "In this paper, we study nonzero-sum separable games, which are continuous games whose payoffs take a sum-of-products form. Included in this subclass are all finite games and polynomial games. We investigate the structure of equilibria in separable games. We show that these games admit finitely supported Nash equilibria. Motivated by the bounds on the supports of mixed equilibria in two-player finite games in terms of the ranks of the payoff matrices, we define the notion of the rank of an n-player continuous game and use this to provide bounds on the cardinality of the support of equilibrium strategies. We present a general characterization theorem that states that a continuous game has finite rank if and only if it is separable. Using our rank results, we present an efficient algorithm for computing approximate equilibria of two-player separable games with fixed strategy spaces in time polynomial in the rank of the game.", ["Compact support", "Cardinality", "Nash equilibrium", "Polynomial time", "Matrix (mathematics)", "Polynomial", "Computing", "Disjunctive normal form", "Algorithm", "Finite set", "Separable space", "If and only if"]], ["Quantum Algorithms for Learning and Testing Juntas", "In this article we develop quantum algorithms for learning and testing juntas, i.e. Boolean functions which depend only on an unknown set of k out of n input variables. Our aim is to develop efficient algorithms: - whose sample complexity has no dependence on n, the dimension of the domain the Boolean functions are defined over; - with no access to any classical or quantum membership (\"black-box\") queries. Instead, our algorithms use only classical examples generated uniformly at random and fixed quantum superpositions of such classical examples; - which require only a few quantum examples but possibly many classical random examples (which are considered quite \"cheap\" relative to quantum examples). Our quantum algorithms are based on a subroutine FS which enables sampling according to the Fourier spectrum of f; the FS subroutine was used in earlier work of Bshouty and Jackson on quantum learning. Our results are as follows: - We give an algorithm for testing k-juntas to accuracy $\\epsilon$ that uses $O(k/\\epsilon)$ quantum examples. This improves on the number of examples used by the best known classical algorithm. - We establish the following lower bound: any FS-based k-junta testing algorithm requires $\\Omega(\\sqrt{k})$ queries. - We give an algorithm for learning $k$-juntas to accuracy $\\epsilon$ that uses $O(\\epsilon^{-1} k\\log k)$ quantum examples and $O(2^k \\log(1/\\epsilon))$ random examples. We show that this learning algorithms is close to optimal by giving a related lower bound.", ["Black box", "Upper and lower bounds", "Dimension", "Boolean function", "Uniform distribution (discrete)", "Algorithm", "Quantum algorithm", "Subroutine", "Computational complexity theory", "Randomness", "Quantum superposition"]], ["A Bayesian Framework for Combining Valuation Estimates", "Obtaining more accurate equity value estimates is the starting point for stock selection, value-based indexing in a noisy market, and beating benchmark indices through tactical style rotation. Unfortunately, discounted cash flow, method of comparables, and fundamental analysis typically yield discrepant valuation estimates. Moreover, the valuation estimates typically disagree with market price. Can one form a superior valuation estimate by averaging over the individual estimates, including market price? This article suggests a Bayesian framework for combining two or more estimates into a superior valuation estimate. The framework justifies the common practice of averaging over several estimates to arrive at a final point estimate.", ["Discounted cash flow", "Fundamental analysis", "Point estimation", "Index (economics)", "Bayesian inference", "Stock"]], ["Workspace and Kinematic Analysis of the VERNE machine", "This paper describes the workspace and the inverse and direct kinematic analysis of the VERNE machine, a serial/parallel 5-axis machine tool designed by Fatronik for IRCCyN. This machine is composed of a three-degree-of-freedom (DOF) parallel module and a two-DOF serial tilting table. The parallel module consists of a moving platform that is connected to a fixed base by three non-identical legs. This feature involves (i) a simultaneous combination of rotation and translation for the moving platform, which is balanced by the tilting table and (ii) workspace whose shape and volume vary as a function of the tool length. This paper summarizes results obtained in the context of the European projects NEXT (\"Next Generation of Productions Systems\").", ["Rotation", "Machine tool", "Degrees of freedom (mechanics)", "Kinematics", "Function (mathematics)"]], ["Upper bound of loss probability in an OFDMA system with randomly located users", "For OFDMA systems, we find a rough but easily computed upper bound for the probability of loosing communications by insufficient number of sub-channels on downlink. We consider as random the positions of receiving users in the system as well as the number of sub-channels dedicated to each one. We use recent results of the theory of point processes which reduce our calculations to the first and second moments of the total required number of sub-carriers.", ["Upper and lower bounds", "Downlink", "Orthogonal frequency-division multiple access", "Probability"]], ["e-Science initiatives in Venezuela", "Within the context of the nascent e-Science infrastructure in Venezuela, we describe several web-based scientific applications developed at the Centro Nacional de Calculo Cientifico Universidad de Los Andes (CeCalCULA), Merida, and at the Instituto Venezolano de Investigaciones Cientificas (IVIC), Caracas. The different strategies that have been followed for implementing quantum chemistry and atomic physics applications are presented. We also briefly discuss a damage portal based on dynamic, nonlinear, finite elements of lumped damage mechanics and a biomedical portal developed within the framework of the \\textit{E-Infrastructure shared between Europe and Latin America} (EELA) initiative for searching common sequences and inferring their functions in parasitic diseases such as leishmaniasis, chagas and malaria.", ["Malaria", "Venezuela", "Europe", "Latin America", "Atomic physics", "Chemistry", "Andes", "Latin", "Nonlinear system", "Medical research", "Leishmaniasis", "E-Science", "Quantum chemistry", "Caracas", "Chagas disease", "Tribology", "Mechanics", "Finite element method", "Infrastructure", "Physics", "Parasitism"]], ["The Kinetostatic Optimization of a Novel Prismatic Drive", "The design of a mechanical transmission taking into account the transmitted forces is reported in this paper. This transmission is based on Slide-o-Cam, a cam mechanism with multiple rollers mounted on a common translating follower. The design of Slide-o-Cam, a transmission intended to produce a sliding motion from a turning drive, or vice versa, was reported elsewhere. This transmission provides pure-rolling motion, thereby reducing the friction of rack-and-pinions and linear drives. The pressure angle is a suitable performance index for this transmission because it determines the amount of force transmitted to the load vs. that transmitted to the machine frame. To assess the transmission capability of the mechanism, the Hertz formula is introduced to calculate the stresses on the rollers and on the cams. The final transmission is intended to replace the current ball-screws in the Orthoglide, a three-DOF parallel robot for the production of translational motions, currently under development for machining applications at Ecole Centrale de Nantes.", ["Machining", "Robot", "Friction", "Cam", "Mathematical optimization", "Pinion", "Transmission (mechanics)", "Pressure"]], ["Mumford dendrograms", "An effective $p$-adic encoding of dendrograms is presented through an explicit embedding into the Bruhat-Tits tree for a $p$-adic number field. This field depends on the number of children of a vertex and is a finite extension of the field of $p$-adic numbers. It is shown that fixing $p$-adic representatives of the residue field allows a natural way of encoding strings by identifying a given alphabet with such representatives. A simple $p$-adic hierarchic classification algorithm is derived for $p$-adic numbers, and is applied to strings over finite alphabets. Examples of DNA coding are presented and discussed. Finally, new geometric and combinatorial invariants of time series of $p$-adic dendrograms are developped.", ["Algorithm", "DNA", "Geometry", "Algebraic number field", "Field extension", "Residue field", "Finite set", "Embedding", "Combinatorics", "Alphabet", "Statistical classification"]], ["A Six Degree-Of-Freedom Haptic Device Based On The Orthoglide And A Hybrid Agile Eye", "This paper is devoted to the kinematic design of a new six degree-of-freedom haptic device using two parallel mechanisms. The first one, called orthoglide, provides the translation motions and the second one, called agile eye, produces the rotational motions. These two motions are decoupled to simplify the direct and inverse kinematics, as it is needed for real-time control. To reduce the inertial load, the motors are fixed on the base and a transmission with two universal joints is used to transmit the rotational motions from the base to the end-effector. Two alternative wrists are proposed (i), the agile eye with three degrees of freedom or (ii) a hybrid wrist made by the assembly of a two-dof agile eye with a rotary motor. The last one is optimized to increase its stiffness and to decrease the number of moving parts.", ["Six degrees of freedom", "Inverse kinematics", "Stiffness", "Kinematics", "Inertial frame of reference", "Moving parts"]], ["Analyse Comparative des Manipulateurs 3R \\`a Axes Orthogonaux", "A family of 3R orthogonal manipulators without offset on the third body can be divided into exactly nine workspace topologies. The workspace is characterized in a half-cross section by the singular curves. The workspace topology is defined by the number of cusps and nodes that appear on these singular curves. Based on this classification, we evaluate theses manipulators by the condition number related to the joint space and the proportion of the region with four inverse kinematic solutions compared to a sphere containing all the workspace. This second performance number is in relation with the workspace. We determine finally le topology of workspace to which belong manipulators having the best performance number values.", ["Condition number", "Kinematics", "Inverse kinematics", "Sphere", "Topology", "Thesis", "Orthogonality"]], ["An Exhaustive Study of the Workspace Topologies of all 3R Orthogonal Manipulators with Geometric Simplifications", "This paper analyses the workspace of the three-revolute orthogonal manipulators that have at least one of their DH parameters equal to zero. These manipulators are classified into different groups with similar kinematic properties. The classification criteria are based on the topology of the workspace. Each group is evaluated according to interesting kinematic properties such as the size of the workspace subregion reachable with four inverse kinematic solutions, the existence and the size of voids, and the size of the regions of feasible paths in the workspace.", ["Orthogonality", "Topology", "Inverse kinematics"]], ["Practical Approach to Knowledge-based Question Answering with Natural Language Understanding and Advanced Reasoning", "This research hypothesized that a practical approach in the form of a solution framework known as Natural Language Understanding and Reasoning for Intelligence (NaLURI), which combines full-discourse natural language understanding, powerful representation formalism capable of exploiting ontological information and reasoning approach with advanced features, will solve the following problems without compromising practicality factors: 1) restriction on the nature of question and response, and 2) limitation to scale across domains and to real-life natural language text.", ["Reason", "Ontology", "Ontology (information science)", "Discourse", "Knowledge", "Natural language processing", "Question answering", "Natural language", "Language"]], ["Integration of a Balanced Virtual Manikin in a Virtual Reality Platform aimed at Virtual Prototyping", "The work presented here is aimed at introducing a virtual human controller in a virtual prototyping framework. After a brief introduction describing the problem solved in the paper, we describe the interest as for digital humans in the context of concurrent engineering. This leads us to draw a control architecture enabling to drive virtual humans in a real-time immersed way, and to interact with the product, through motion capture. Unfortunately, we show this control scheme can lead to unfeasible movements because of the lack of balance control. Introducing such a controller is a problem that was never addressed in the context of real-time. We propose an implementation of a balance controller, that we insert into the previously described control scheme. Next section is dedicated to show the results we obtained. Finally, we propose a virtual reality platform into which the digital character controller is integrated.", ["Engineering", "Motion capture", "Prototype", "Virtual reality", "Concurrent engineering", "Architecture"]], ["Balanced Virtual Humans Interacting with their Environment", "The animation of human avatars seems very successful; the computer graphics industry shows outstanding results in films everyday, the game industry achieves exploits... Nevertheless, the animation and control processes of such manikins are very painful. It takes days to a specialist to build such animated sequences, and it is not adaptive to any type of modifications. Our main purpose is the virtual human for engineering, especially virtual prototyping. As for this domain of activity, such amounts of time are prohibitive.", ["Engineering", "Prototype", "Computer", "Avatar (computing)", "Computer graphics"]], ["Virtual reality: A human centered tool for improving Manufacturing", "Manufacturing is using Virtual Reality tools to enhance the product life cycle. Their definitions are still in flux and it is necessary to define their connections. Thus, firstly, we will introduce more closely some definitions where we will find that, if the Virtual manufacturing concepts originate from machining operations and evolve in this manufacturing area, there exist a lot of applications in different fields such as casting, forging, sheet metalworking and robotics (mechanisms). From the recent projects in Europe or in USA, we notice that the human perception or the simulation of mannequin is more and more needed in both fields. In this context, we have isolated some applications as ergonomic studies, assembly and maintenance simulation, design or training where the virtual reality tools can be applied. Thus, we find out a family of applications where the virtual reality tools give the engineers the main role in the optimization process. We will illustrate our paper by several examples where virtual reality interfaces are used and combined with optimization tools as multi-agent systems.", ["Manufacturing", "Perception", "Europe", "Metalworking", "Virtual reality", "Ergonomics", "Machining", "Mannequin", "Multi-agent system", "United States", "Robotics", "Mathematical optimization", "Product lifecycle management"]], ["A New Six Degree-of-Freedom Haptic Device based on the Orthoglide and the Agile Eye", "The aim of this paper is to present a new six degree-of-freedom (dof) haptic device using two parallel mechanisms. The first one, called orthoglide, provides the translation motions and the second one produces the rotational motions. These two motions are decoupled to simplify the direct and inverse kinematics, as it is needed for real-times control. To reduce the inertial load, the motors are fixed on the base and a transmission with two universal joints is used to transmit the rotational motions from the base to the end-effector. The main feature of the orthoglide and of the agile eye mechanism is the existence of an isotropic configuration. The length of the legs and the range limits of the orthoglide are optimized to have homogeneous performance throughout the Cartesian workspace, which has a nearly cubic workspace. These properties permit to have a high stiffness throughout the workspace and workspace limits that are easily understandable by the user.", ["Robot end effector", "Inverse kinematics", "Six degrees of freedom", "Kinematics", "Isotropy", "Inertial frame of reference", "Stiffness", "Haptic technology", "Cartesian coordinate system"]], ["L'orthoglide : une machine-outil rapide d'architecture parall\\`ele isotrope", "This article presents the Orthoglide project. The purpose of this project is the realization of a prototype of machine tool to three degrees of translation. The characteristic of this machine is a parallel kinematic architecture optimized to obtain a compact workspace with homogeneous performance. For that, the principal criterion of design which was used is the isotropy.", ["Prototype", "Machine tool", "Architecture", "Isotropy"]], ["An exploratory study of Google Scholar", "The paper discusses and analyzes the scientific search service Google Scholar (GS). The focus is on an exploratory study which investigates the coverage of scientific serials in GS. The study shows deficiencies in the coverage and up-to-dateness of the GS index. Furthermore, the study points up which Web servers are the most important data providers for this search service and which information sources are highly represented. We can show that there is a relatively large gap in Google Scholars coverage of German literature as well as weaknesses in the accessibility of Open Access content. Keywords: Search engines, Digital libraries, Worldwide Web, Serials, Electronic journals", ["World Wide Web", "Google Scholar", "Digital library", "Literature", "German literature", "Web server", "Web search engine", "Germany", "Accessibility"]], ["The effect of fading, channel inversion, and threshold scheduling on ad hoc networks", "This paper addresses three issues in the field of ad hoc network capacity: the impact of i)channel fading, ii) channel inversion power control, and iii) threshold-based scheduling on capacity. Channel inversion and threshold scheduling may be viewed as simple ways to exploit channel state information (CSI) without requiring cooperation across transmitters. We use the transmission capacity (TC) as our metric, defined as the maximum spatial intensity of successful simultaneous transmissions subject to a constraint on the outage probability (OP). By assuming the nodes are located on the infinite plane according to a Poisson process, we are able to employ tools from stochastic geometry to obtain asymptotically tight bounds on the distribution of the signal-to-interference (SIR) level, yielding in turn tight bounds on the OP (relative to a given SIR threshold) and the TC. We demonstrate that in the absence of CSI, fading can significantly reduce the TC and somewhat surprisingly, channel inversion only makes matters worse. We develop a threshold-based transmission rule where transmitters are active only if the channel to their receiver is acceptably strong, obtain expressions for the optimal threshold, and show that this simple, fully distributed scheme can significantly reduce the effect of fading.", ["Poisson process", "Channel state information", "Probability", "Big O notation", "Geometry", "Stochastic"]], ["Semi-local string comparison: algorithmic techniques and applications", "A classical measure of string comparison is given by the longest common subsequence (LCS) problem on a pair of strings. We consider its generalisation, called the semi-local LCS problem, which arises naturally in many string-related problems. The semi-local LCS problem asks for the LCS scores for each of the input strings against every substring of the other input string, and for every prefix of each input string against every suffix of the other input string. Such a comparison pattern provides a much more detailed picture of string similarity than a single LCS score; it also arises naturally in many string-related problems. In fact, the semi-local LCS problem turns out to be fundamental for string comparison, providing a powerful and flexible alternative to classical dynamic programming. It is especially useful when the input to a string comparison problem may not be available all at once: for example, comparison of dynamically changing strings; comparison of compressed strings; parallel string comparison. The same approach can also be applied to permutation strings, providing efficient solutions for local versions of the longest increasing subsequence (LIS) problem, and for the problem of computing a maximum clique in a circle graph. Furthermore, the semi-local LCS problem turns out to have surprising connections in a few seemingly unrelated fields, such as computational geometry and algebra of semigroups. This work is devoted to exploring the structure of the semi-local LCS problem, its efficient solutions, and its applications in string comparison and other related areas, including computational molecular biology.", ["Computational geometry", "Dynamic programming", "Geometry", "Longest increasing subsequence", "Algebra", "Maximum clique", "Algorithm", "Molecular biology", "Computing", "Permutation", "Circle graph", "Subsequence", "Biology", "Semigroup", "Longest common subsequence problem"]], ["Constant-degree graph expansions that preserve the treewidth", "Many hard algorithmic problems dealing with graphs, circuits, formulas and constraints admit polynomial-time upper bounds if the underlying graph has small treewidth. The same problems often encourage reducing the maximal degree of vertices to simplify theoretical arguments or address practical concerns. Such degree reduction can be performed through a sequence of splittings of vertices, resulting in an _expansion_ of the original graph. We observe that the treewidth of a graph may increase dramatically if the splittings are not performed carefully. In this context we address the following natural question: is it possible to reduce the maximum degree to a constant without substantially increasing the treewidth? Our work answers the above question affirmatively. We prove that any simple undirected graph G=(V, E) admits an expansion G'=(V', E') with the maximum degree <= 3 and treewidth(G') <= treewidth(G)+1. Furthermore, such an expansion will have no more than 2|E|+|V| vertices and 3|E| edges; it can be computed efficiently from a tree-decomposition of G. We also construct a family of examples for which the increase by 1 in treewidth cannot be avoided.", ["Tree decomposition", "Undirected graph", "Algorithm", "Graph (mathematics)", "Time complexity", "Tree (graph theory)", "Glossary of graph theory", "Vertex (geometry)", "Polynomial", "Degree (graph theory)"]], ["The Review and Analysis of Human Computer Interaction (HCI) Principles", "The History of HCI is briefly reviewed together with three HCI models and structure including CSCW, CSCL and CSCR. It is shown that a number of authorities consider HCI to be a fragmented discipline with no agreed set of unifying design principles. An analysis of usability criteria based upon citation frequency of authors is performed in order to discover the eight most recognised HCI principles.", ["Human Computer Interaction", "Computer supported cooperative work"]], ["A Comparative Study of Parallel Kinematic Architectures for Machining Applications", "Parallel kinematic mechanisms are interesting alternative designs for machining applications. Three 2-DOF parallel mechanism architectures dedicated to machining applications are studied in this paper. The three mechanisms have two constant length struts gliding along fixed linear actuated joints with different relative orientation. The comparative study is conducted on the basis of a same prescribed Cartesian workspace for the three mechanisms. The common desired workspace properties are a rectangular shape and given kinetostatic performances. The machine size of each resulting design is used as a comparative criterion. The 2-DOF machine mechanisms analyzed in this paper can be extended to 3-axis machines by adding a third joint.", ["Machining"]], ["Kinematic Analysis of a New Parallel Machine Tool: the Orthoglide", "This paper describes a new parallel kinematic architecture for machining applications: the orthoglide. This machine features three fixed parallel linear joints which are mounted orthogonally and a mobile platform which moves in the Cartesian x-y-z space with fixed orientation. The main interest of the orthoglide is that it takes benefit from the advantages of the popular PPP serial machines (regular Cartesian workspace shape and uniform performances) as well as from the parallel kinematic arrangement of the links (less inertia and better dynamic performances), which makes the orthoglide well suited to high-speed machining applications. Possible extension of the orthoglide to 5-axis machining is also investigated.", ["Orthogonality", "Architecture", "Kinematics", "Machining"]], ["Understanding the Characteristics of Internet Short Video Sharing: YouTube as a Case Study", "Established in 2005, YouTube has become the most successful Internet site providing a new generation of short video sharing service. Today, YouTube alone comprises approximately 20% of all HTTP traffic, or nearly 10% of all traffic on the Internet. Understanding the features of YouTube and similar video sharing sites is thus crucial to their sustainable development and to network traffic engineering. In this paper, using traces crawled in a 3-month period, we present an in-depth and systematic measurement study on the characteristics of YouTube videos. We find that YouTube videos have noticeably different statistics compared to traditional streaming videos, ranging from length and access pattern, to their active life span, ratings, and comments. The series of datasets also allows us to identify the growth trend of this fast evolving Internet site in various aspects, which has seldom been explored before. We also look closely at the social networking aspect of YouTube, as this is a key driving force toward its success. In particular, we find that the links to related videos generated by uploaders' choices form a small-world network. This suggests that the videos have strong correlations with each other, and creates opportunities for developing novel caching or peer-to-peer distribution schemes to efficiently deliver videos to end users.", ["Small-world network", "YouTube", "Peer-to-peer", "Hypertext Transfer Protocol", "Cache", "Sustainable development", "Engineering", "Social networking service", "Statistics", "Video hosting service", "Sustainability", "Teletraffic engineering", "Internet"]], ["Products of irreducible random matrices in the (Max,+) Algebra", "We consider the recursive equation ``x(n+1)=A(n)x(n)'' where x(n+1) and x(n) are column vectors of size k and where A(n) is an irreducible random matrix of size k x k. The matrix-vector multiplication in the (max,+) algebra is defined by (A(n)x(n))_i= max_j [ A(n)_{ij} +x(n)_j ]. This type of equation can be used to represent the evolution of Stochastic Event Graphs which include cyclic Jackson Networks, some manufacturing models and models with general blocking (such as Kanban). Let us assume that the sequence (A(n))_n is i.i.d or more generally stationary and ergodic. The main result of the paper states that the system couples in finite time with a unique stationary regime if and only if there exists a set of matrices C such that P {A(0) in C} > 0, and the matrices in C have a unique periodic regime.", ["Random matrix", "Algebra", "Equation", "If and only if", "Sequence", "Multiplication", "Matrix (mathematics)", "Euclidean vector", "Cross product", "Finite set", "Evolution", "Stochastic", "Recursion", "Column vector"]], ["The Computation of All 4R Serial Spherical Wrists With an Isotropic Architecture", "A spherical wrist of the serial type is said to be isotropic if it can attain a posture whereby the singular values of its Jacobian matrix are all identical and nonzero. What isotropy brings about is robustness to manufacturing, assembly, and measurement errors, thereby guaranteeing a maximum orientation accuracy. In this paper we investigate the existence of redundant isotropic architectures, which should add to the dexterity of the wrist under design by virtue of its extra degree of freedom. The problem formulation leads to a system of eight quadratic equations with eight unknowns. The Bezout number of this system is thus 2^8 = 256, its BKK bound being 192. However, the actual number of solutions is shown to be 32. We list all solutions of the foregoing algebraic problem. All these solutions are real, but distinct solutions do not necessarily lead to distinct manipulators. Upon discarding those algebraic solutions that yield no new wrists, we end up with exactly eight distinct architectures, the eight corresponding manipulators being displayed at their isotropic posture.", ["Quadratic equation", "Isotropy", "Jacobian matrix and determinant", "Observational error"]], ["GCP: Gossip-based Code Propagation for Large-scale Mobile Wireless Sensor Networks", "Wireless sensor networks (WSN) have recently received an increasing interest. They are now expected to be deployed for long periods of time, thus requiring software updates. Updating the software code automatically on a huge number of sensors is a tremendous task, as ''by hand'' updates can obviously not be considered, especially when all participating sensors are embedded on mobile entities. In this paper, we investigate an approach to automatically update software in mobile sensor-based application when no localization mechanism is available. We leverage the peer-to-peer cooperation paradigm to achieve a good trade-off between reliability and scalability of code propagation. More specifically, we present the design and evaluation of GCP ({\\emph Gossip-based Code Propagation}), a distributed software update algorithm for mobile wireless sensor networks. GCP relies on two different mechanisms (piggy-backing and forwarding control) to improve significantly the load balance without sacrificing on the propagation speed. We compare GCP against traditional dissemination approaches. Simulation results based on both synthetic and realistic workloads show that GCP achieves a good convergence speed while balancing the load evenly between sensors.", ["Wireless sensor network", "Peer-to-peer", "Scalability", "Paradigm", "Algorithm", "Patch (computing)", "Sensor", "Computer software", "Simulation", "Mobile phone", "Source code", "Phase velocity", "Load balancing (computing)"]], ["Further Comments on \"Residue-to-Binary Converters Based on New Chinese Remainder Theorems\"", "Ananda Mohan suggested that the first New Chinese Remainder Theorem introduced by Wang can be derived from the constructive proof of the well-known Chinese Remainder Theorem (CRT) and claimed that Wang's approach is the same as the one proposed earlier by Huang. Ananda Mohan's proof is however erroneous and we show here that Wang's New CRT I is a rewriting of an algorithm previously sketched by Hitz and Kaltofen.", ["Chinese remainder theorem", "Constructive proof", "Cathode ray tube"]], ["Recent Advances in Solving the Protein Threading Problem", "The fold recognition methods are promissing tools for capturing the structure of a protein by its amino acid residues sequence but their use is still restricted by the needs of huge computational resources and suitable efficient algorithms as well. In the recent version of FROST (Fold Recognition Oriented Search Tool) package the most efficient algorithm for solving the Protein Threading Problem (PTP) is implemented due to the strong collaboration between the SYMBIOSE group in IRISA and MIG in Jouy-en-Josas. In this paper, we present the diverse components of FROST, emphasizing on the recent advances in formulating and solving new versions of the PTP and on the way of solving on a computer cluster a million of instances in a easonable time.", ["Computer cluster", "Protein", "Amino acid", "Threading (protein sequence)", "Algorithm", "Protein folding", "Jouy-en-Josas", "Computer", "Acid"]], ["Bijective Faithful Translations among Default Logics", "In this article, we study translations between variants of defaults logics such that the extensions of the theories that are the input and the output of the translation are in a bijective correspondence. We assume that a translation can introduce new variables and that the result of translating a theory can either be produced in time polynomial in the size of the theory or its output is polynomial in that size; we however restrict to the case in which the original theory has extensions. This study fills a gap between two previous pieces of work, one studying bijective translations among restrictions of default logics, and the other one studying non-bijective translations between default logics variants.", ["Bijection", "Logic"]], ["Interactive Small-Step Algorithms I: Axiomatization", "In earlier work, the Abstract State Machine Thesis -- that arbitrary algorithms are behaviorally equivalent to abstract state machines -- was established for several classes of algorithms, including ordinary, interactive, small-step algorithms. This was accomplished on the basis of axiomatizations of these classes of algorithms. Here we extend the axiomatization and, in a companion paper, the proof, to cover interactive small-step algorithms that are not necessarily ordinary. This means that the algorithms (1) can complete a step without necessarily waiting for replies to all queries from that step and (2) can use not only the environment's replies but also the order in which the replies were received.", ["Axiomatic system", "Abstract state machines"]], ["Interactive Small-Step Algorithms II: Abstract State Machines and the<br> Characterization Theorem", "In earlier work, the Abstract State Machine Thesis -- that arbitrary algorithms are behaviorally equivalent to abstract state machines -- was established for several classes of algorithms, including ordinary, interactive, small-step algorithms. This was accomplished on the basis of axiomatizations of these classes of algorithms. In Part I (Interactive Small-Step Algorithms I: Axiomatization), the axiomatization was extended to cover interactive small-step algorithms that are not necessarily ordinary. This means that the algorithms (1) can complete a step without necessarily waiting for replies to all queries from that step and (2) can use not only the environment's replies but also the order in which the replies were received. In order to prove the thesis for algorithms of this generality, we extend here the definition of abstract state machines to incorporate explicit attention to the relative timing of replies and to the possible absence of replies. We prove the characterization theorem for extended abstract state machines with respect to general algorithms as axiomatized in Part I.", ["Abstract state machines", "Axiomatic system", "Thesis"]], ["How to be correct, lazy and efficient ?", "This paper is an introduction to Lambdix, a lazy Lisp interpreter implemented at the Research Laboratory of Paris XI University (Laboratoire de Recherche en Informatique, Orsay). Lambdix was devised in the course of an investigation into the relationship between the semantics of programming languages and their implementation; it was used to demonstrate that in the Lisp domain, semantic correctness is consistent with efficiency, contrary to what has often been claimed. The first part of the paper is an overview of well-known semantic difficulties encountered by Lisp as well as an informal presentation of Lambdix; it is shown that the difficulties which Lisp encouters do not arise in Lambdix. The second part is about efficiency in implementation models. It explains why Lambdix is better suited for lazy evaluation than previous models. The section ends by giving comparative execution time tables.", ["Lazy evaluation", "Lisp (programming language)", "Paris"]], ["Plotkin construction: rank and kernel", "Given two binary codes of length n, using Plotkin construction we obtain a code of length 2n. The construction works for linear and nonlinear codes. For the linear case, it is straightforward to see that the dimension of the final code is the sum of the dimensions of the starting codes. For nonlinear codes, the rank and the dimension of the kernel are standard mesures of linearity. In this report, we prove that both parameters are also the sum of the corresponding ones of the starting codes.", ["Nonlinear system", "Linear", "Dimension", "Kernel (algebra)"]], ["Use of a $d$-Constraint During LDPC Decoding in a Bliss Scheme", "Bliss schemes of a run length limited (RLL) codec in combination with an LDPC codec, generate LDPC parity bits over a systematic sequence of RLL channel bits that are inherently redundant as they satisfy e.g. a $d=1$ minimum run length constraint. That is the subsequences consisting of runs of length $d=1$, viz. $...010...$ and $...101...$, cannot occur. We propose to use this redundancy during LDPC decoding in a Bliss scheme by introducing additional $d$-constraint nodes in the factor graph used by the LDPC decoder. The messages sent from these new nodes to the variable or codeword bit nodes exert a ``force'' on the resulting soft-bit vector coming out of the LDPC decoding that give it a tendency to comply with the $d$-constraints. This way, we can significantly reduce the probability of decoding error.", ["Run-length limited", "Factor graph", "Low-density parity-check code", "Probability", "Bit array", "Decoder", "Redundancy (engineering)", "Bit", "Codec"]], ["Closed form solutions for symmetric water filling games", "We study power control in optimization and game frameworks. In the optimization framework there is a single decision maker who assigns network resources and in the game framework users share the network resources according to Nash equilibrium. The solution of these problems is based on so-called water-filling technique, which in turn uses bisection method for solution of non-linear equations for Lagrange multiplies. Here we provide a closed form solution to the water-filling problem, which allows us to solve it in a finite number of operations. Also, we produce a closed form solution for the Nash equilibrium in symmetric Gaussian interference game with an arbitrary number of users. Even though the game is symmetric, there is an intrinsic hierarchical structure induced by the quantity of the resources available to the users. We use this hierarchical structure to perform a successive reduction of the game. In addition, to its mathematical beauty, the explicit solution allows one to study limiting cases when the crosstalk coefficient is either small or large. We provide an alternative simple proof of the convergence of the Iterative Water Filling Algorithm. Furthermore, it turns out that the convergence of Iterative Water Filling Algorithm slows down when the crosstalk coefficient is large. Using the closed form solution, we can avoid this problem. Finally, we compare the non-cooperative approach with the cooperative approach and show that the non-cooperative approach results in a more fair resource distribution.", ["Nash equilibrium", "Mathematics", "Joseph Louis Lagrange", "Mathematical beauty", "Mathematical optimization", "Hierarchy", "Closed-form expression", "Bisection method", "Algorithm", "Linear equation", "Coefficient", "Crosstalk (electronics)"]], ["Four-Group Decodable Space-Time Block Codes", "Two new rate-one full-diversity space-time block codes (STBC) are proposed. They are characterized by the \\emph{lowest decoding complexity} among the known rate-one STBC, arising due to the complete separability of the transmitted symbols into four groups for maximum likelihood detection. The first and the second codes are delay-optimal if the number of transmit antennas is a power of 2 and even, respectively. The exact pair-wise error probability is derived to allow for the performance optimization of the two codes. Compared with existing low-decoding complexity STBC, the two new codes offer several advantages such as higher code rate, lower encoding/decoding delay and complexity, lower peak-to-average power ratio, and better performance.", ["Code rate", "Mathematical optimization", "Probability", "Antenna (radio)", "Maximum likelihood", "Spacetime", "Power of two"]], ["Learning Probabilistic Models of Word Sense Disambiguation", "This dissertation presents several new methods of supervised and unsupervised learning of word sense disambiguation models. The supervised methods focus on performing model searches through a space of probabilistic models, and the unsupervised methods rely on the use of Gibbs Sampling and the Expectation Maximization (EM) algorithm. In both the supervised and unsupervised case, the Naive Bayesian model is found to perform well. An explanation for this success is presented in terms of learning rates and bias-variance decompositions.", ["Unsupervised learning", "Expectation-maximization algorithm", "Gibbs sampling", "Bayesian probability", "Word-sense disambiguation", "Graphical model", "Variance", "Expected value", "Probability", "Algorithm", "Mathematical model"]], ["Clifford Algebra of the Vector Space of Conics for decision boundary Hyperplanes in m-Euclidean Space", "In this paper we embed $m$-dimensional Euclidean space in the geometric algebra $Cl_m $ to extend the operators of incidence in ${R^m}$ to operators of incidence in the geometric algebra to generalize the notion of separator to a decision boundary hyperconic in the Clifford algebra of hyperconic sections denoted as ${Cl}({Co}_{2})$. This allows us to extend the concept of a linear perceptron or the spherical perceptron in conformal geometry and introduce the more general conic perceptron, namely the {elliptical perceptron}. Using Clifford duality a vector orthogonal to the decision boundary hyperplane is determined. Experimental results are shown in 2-dimensional Euclidean space where we separate data that are naturally separated by some typical plane conic separators by this procedure. This procedure is more general in the sense that it is independent of the dimension of the input data and hence we can speak of the hyperconic elliptic perceptron.", ["Euclidean space", "Clifford algebra", "Hyperplane", "Conformal geometry", "Orthogonality", "Geometric algebra", "Dimension", "Conformal map", "Algebra", "Sphere", "Embedding", "Conic section", "Geometry", "Vector space", "Euclidean vector", "Ellipse", "Plane (geometry)", "Perceptron", "Space", "William Kingdon Clifford"]], ["One-way Hash Function Based on Neural Network", "A hash function is constructed based on a three-layer neural network. The three neuron-layers are used to realize data confusion, diffusion and compression respectively, and the multi-block hash mode is presented to support the plaintext with variable length. Theoretical analysis and experimental results show that this hash function is one-way, with high key sensitivity and plaintext sensitivity, and secure against birthday attacks or meet-in-the-middle attacks. Additionally, the neural network's property makes it practical to realize in a parallel way. These properties make it a suitable choice for data signature or authentication.", ["Plaintext", "Neural network", "Data", "Neuron", "Authentication", "Hash function", "Diffusion", "Function (mathematics)"]], ["Geometrical derivation of the Boltzmann factor", "We show that the Boltzmann factor has a geometrical origin. Its derivation follows from the microcanonical picture. The Maxwell-Boltzmann distribution or the wealth distribution in human society are some direct applications of this new interpretation.", ["Maxwell-Boltzmann distribution", "Boltzmann factor", "Boltzmann distribution", "Distribution of wealth", "Geometry"]], ["Chain of Separable Binary Goppa Codes and their Minimal Distance", "It is shown that subclasses of separable binary Goppa codes, $\\Gamma(L,G)$ - codes, with $L=\\{\\alpha \\in GF(2^{2l}):G(\\alpha)\\neq 0 \\}$ and special Goppa polynomials G(x) can be presented as a chain of embedded codes. The true minimal distance has been obtained for all codes of the chain.", ["Binary numeral system"]], ["Queues, stores, and tableaux", "Consider the single server queue with an infinite buffer and a FIFO discipline, either of type M/M/1 or Geom/Geom/1. Denote by A the arrival process and by s the services. Assume the stability condition to be satisfied. Denote by D the departure process in equilibrium and by r the time spent by the customers at the very back of the queue. We prove that (D,r) has the same law as (A,s) which is an extension of the classical Burke Theorem. In fact, r can be viewed as the departures from a dual storage model. This duality between the two models also appears when studying the transient behavior of a tandem by means of the RSK algorithm: the first and last row of the resulting semi-standard Young tableau are respectively the last instant of departure in the queue and the total number of departures in the store.", ["FIFO", "Server (computing)", "Young tableau"]], ["Services within a busy period of an M/M/1 queue and Dyck paths", "We analyze the service times of customers in a stable M/M/1 queue in equilibrium depending on their position in a busy period. We give the law of the service of a customer at the beginning, at the end, or in the middle of the busy period. It enables as a by-product to prove that the process of instants of beginning of services is not Poisson. We then proceed to a more precise analysis. We consider a family of polynomial generating series associated with Dyck paths of length 2n and we show that they provide the correlation function of the successive services in a busy period with (n+1) customers.", ["Economic equilibrium", "Polynomial", "Correlation function"]], ["Multiuser Successive Refinement and Multiple Description Coding", "We consider the multiuser successive refinement (MSR) problem, where the users are connected to a central server via links with different noiseless capacities, and each user wishes to reconstruct in a successive-refinement fashion. An achievable region is given for the two-user two-layer case and it provides the complete rate-distortion region for the Gaussian source under the MSE distortion measure. The key observation is that this problem includes the multiple description (MD) problem (with two descriptions) as a subsystem, and the techniques useful in the MD problem can be extended to this case. We show that the coding scheme based on the universality of random binning is sub-optimal, because multiple Gaussian side informations only at the decoders do incur performance loss, in contrast to the case of single side information at the decoder. We further show that unlike the single user case, when there are multiple users, the loss of performance by a multistage coding approach can be unbounded for the Gaussian source. The result suggests that in such a setting, the benefit of using successive refinement is not likely to justify the accompanying performance loss. The MSR problem is also related to the source coding problem where each decoder has its individual side information, while the encoder has the complete set of the side informations. The MSR problem further includes several variations of the MD problem, for which the specialization of the general result is investigated and the implication is discussed.", ["Encoder", "Server (computing)", "System", "Decoder", "Data compression"]], ["Parsimony Principles for Software Components and Metalanguages", "Software is a communication system. The usual topic of communication is program behavior, as encoded by programs. Domain-specific libraries are codebooks, domain-specific languages are coding schemes, and so forth. To turn metaphor into method, we adapt toolsfrom information theory--the study of efficient communication--to probe the efficiency with which languages and libraries let us communicate programs. In previous work we developed an information-theoretic analysis of software reuse in problem domains. This new paper uses information theory to analyze tradeoffs in the design of components, generators, and metalanguages. We seek answers to two questions: (1) How can we judge whether a component is over- or under-generalized? Drawing on minimum description length principles, we propose that the best component yields the most succinct representation of the use cases. (2) If we view a programming language as an assemblage of metalanguages, each providing a complementary style of abstraction, how can these metalanguages aid or hinder us in efficiently describing software? We describe a complex triangle of interactions between the power of an abstraction mechanism, the amount of reuse it enables, and the cognitive difficulty of its use.", ["Code reuse", "Minimum description length", "Occam's razor", "Cognition", "Domain-specific language", "Metaphor", "Communication", "Programming language", "Information theory", "Viable System Model", "Computer software", "Library", "Use case"]], ["Reductionism, emergence, and levels of abstractions", "Can there be independent higher level laws of nature if everything is reducible to the fundamental laws of physics? The computer science notion of level of abstraction explains why there can -- illustrating how computational thinking can solve one of philosophy's most vexing problems.", ["Reductionism", "Computer science", "Physics", "Philosophy"]], ["Complexity of Propositional Proofs under a Promise", "We study -- within the framework of propositional proof complexity -- the problem of certifying unsatisfiability of CNF formulas under the promise that any satisfiable formula has many satisfying assignments, where ``many'' stands for an explicitly specified function $\\Lam$ in the number of variables $n$. To this end, we develop propositional proof systems under different measures of promises (that is, different $\\Lam$) as extensions of resolution. This is done by augmenting resolution with axioms that, roughly, can eliminate sets of truth assignments defined by Boolean circuits. We then investigate the complexity of such systems, obtaining an exponential separation in the average-case between resolution under different size promises: 1. Resolution has polynomial-size refutations for all unsatisfiable 3CNF formulas when the promise is $\\eps\\cd2^n$, for any constant $0<\\eps<1$. 2. There are no sub-exponential size resolution refutations for random 3CNF formulas, when the promise is $2^{\\delta n}$ (and the number of clauses is $o(n^{3/2})$), for any constant $0<\\delta<1$.", ["Variable (mathematics)", "Boolean circuit", "Propositional calculus", "Axiom", "Boolean algebra", "Function (mathematics)", "Polynomial", "Formula", "Conjunctive normal form", "Mathematical proof", "Complexity"]], ["Star Unfolding Convex Polyhedra via Quasigeodesic Loops", "We extend the notion of star unfolding to be based on a quasigeodesic loop Q rather than on a point. This gives a new general method to unfold the surface of any convex polyhedron P to a simple (non-overlapping), planar polygon: cut along one shortest path from each vertex of P to Q, and cut all but one segment of Q.", ["Convex polytope", "Convex and concave polygons", "Vertex (geometry)"]], ["A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network", "In this paper, we employ Probabilistic Neural Network (PNN) with image and data processing techniques to implement a general purpose automated leaf recognition algorithm. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the PNN. The PNN is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. Compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.", ["Artificial intelligence"]], ["A note on equipartition", "The problem of the existence of an equi-partition of a curve in $\\R^n$ has recently been raised in the context of computational geometry. The problem is to show that for a (continuous) curve $\\Gamma : [0,1] \\to \\R^n$ and for any positive integer N, there exist points $t_0=0<t_1<...<t_{N-1}<1=t_N$, such that $d(\\Gamma(t_{i-1}),\\Gamma(t_i))=d(\\Gamma(t_{i}),\\Gamma(t_{i+1}))$ for all $i=1,...,N$, where d is a metric or even a semi-metric (a weaker notion) on $\\R^n$. We show here that the existence of such points, in a broader context, is a consequence of Brower's fixed point theorem.", ["Computational geometry", "Integer", "Fixed point theorem", "Fixed point (mathematics)", "Geometry", "Theorem", "Natural number"]], ["Spatial Aggregation: Data Model and Implementation", "Data aggregation in Geographic Information Systems (GIS) is only marginally present in commercial systems nowadays, mostly through ad-hoc solutions. In this paper, we first present a formal model for representing spatial data. This model integrates geographic data and information contained in data warehouses external to the GIS. We define the notion of geometric aggregation, a general framework for aggregate queries in a GIS setting. We also identify the class of summable queries, which can be efficiently evaluated by precomputing the overlay of two or more of the thematic layers involved in the query. We also sketch a language, denoted GISOLAP-QL, for expressing queries that involve GIS and OLAP features. In addition, we introduce Piet, an implementation of our proposal, that makes use of overlay precomputation for answering spatial queries (aggregate or not). Our experimental evaluation showed that for a certain class of geometric queries with or without aggregation, overlay precomputation outperforms R-tree-based techniques. Finally, as a particular application of our proposal, we study topological queries.", ["Topology", "Online analytical processing", "Information systems", "Geographic information system", "Geometry", "Ad hoc", "R-tree"]], ["Embedded Rank Distance Codes for ISI channels", "Designs for transmit alphabet constrained space-time codes naturally lead to questions about the design of rank distance codes. Recently, diversity embedded multi-level space-time codes for flat fading channels have been designed from sets of binary matrices with rank distance guarantees over the binary field by mapping them onto QAM and PSK constellations. In this paper we demonstrate that diversity embedded space-time codes for fading Inter-Symbol Interference (ISI) channels can be designed with provable rank distance guarantees. As a corollary we obtain an asymptotic characterization of the fixed transmit alphabet rate-diversity trade-off for multiple antenna fading ISI channels. The key idea is to construct and analyze properties of binary matrices with a particular structure induced by ISI channels.", ["Antenna (radio)", "MIMO", "Logical matrix", "Quadrature amplitude modulation", "Spacetime", "Phase-shift keying", "Matrix (mathematics)", "Alphabet"]], ["Linear-programming Decoding of Non-binary Linear Codes", "We develop a framework for linear-programming (LP) decoding of non-binary linear codes over rings. We prove that the resulting LP decoder has the `maximum likelihood certificate' property, and we show that the decoder output is the lowest cost pseudocodeword. Equivalence between pseudocodewords of the linear program and pseudocodewords of graph covers is proved. LP decoding performance is illustrated for the (11,6,5) ternary Golay code with ternary PSK modulation over AWGN, and in this case it is shown that the LP decoder performance is comparable to codeword-error-rate-optimum hard-decision based decoding.", ["Linear programming", "Modulation", "Maximum likelihood", "Additive white Gaussian noise", "Code word", "Decoder", "Binary numeral system", "Numeral system", "Binary Golay code", "Phase-shift keying"]], ["Blocking a transition in a Free Choice net and what it tells about its throughput", "In a live and bounded Free Choice Petri net, pick a non-conflicting transition. Then there exists a unique reachable marking in which no transition is enabled except the selected one. For a routed live and bounded Free Choice net, this property is true for any transition of the net. Consider now a live and bounded stochastic routed Free Choice net, and assume that the routings and the firing times are independent and identically distributed. Using the above results, we prove the existence of asymptotic firing throughputs for all transitions in the net. Furthermore the vector of the throughputs at the different transitions is explicitly computable up to a multiplicative constant.", ["Petri net", "Independent and identically distributed random variables", "Big O notation", "Euclidean vector"]], ["Separation Logic for Small-step Cminor", "Cminor is a mid-level imperative programming language; there are proved-correct optimizing compilers from C to Cminor and from Cminor to machine language. We have redesigned Cminor so that it is suitable for Hoare Logic reasoning and we have designed a Separation Logic for Cminor. In this paper, we give a small-step semantics (instead of the big-step of the proved-correct compiler) that is motivated by the need to support future concurrent extensions. We detail a machine-checked proof of soundness of our Separation Logic. This is the first large-scale machine-checked proof of a Separation Logic w.r.t. a small-step semantics. The work presented in this paper has been carried out in the Coq proof assistant. It is a first step towards an environment in which concurrent Cminor programs can be verified using Separation Logic and also compiled by a proved-correct compiler with formal end-to-end correctness guarantees.", ["Interactive theorem proving", "Imperative programming", "Programming language", "Machine code", "Semantics", "Compiler", "Compiler optimization", "Soundness"]], ["On sparse representations of linear operators and the approximation of matrix products", "Thus far, sparse representations have been exploited largely in the context of robustly estimating functions in a noisy environment from a few measurements. In this context, the existence of a basis in which the signal class under consideration is sparse is used to decrease the number of necessary measurements while controlling the approximation error. In this paper, we instead focus on applications in numerical analysis, by way of sparse representations of linear operators with the objective of minimizing the number of operations needed to perform basic operations (here, multiplication) on these operators. We represent a linear operator by a sum of rank-one operators, and show how a sparse representation that guarantees a low approximation error for the product can be obtained from analyzing an induced quadratic form. This construction in turn yields new algorithms for computing approximate matrix products.", ["Numerical analysis", "Quadratic form", "Linear map", "Approximation error", "Computing", "Multiplication", "Algorithm", "Matrix (mathematics)"]], ["Small weakly universal Turing machines", "We give small universal Turing machines with state-symbol pairs of (6, 2), (3, 3) and (2, 4). These machines are weakly universal, which means that they have an infinitely repeated word to the left of their input and another to the right. They simulate Rule 110 and are currently the smallest known weakly universal Turing machines.", ["Rule 110"]], ["Competitive minimax universal decoding for several ensembles of random codes", "Universally achievable error exponents pertaining to certain families of channels (most notably, discrete memoryless channels (DMC's)), and various ensembles of random codes, are studied by combining the competitive minimax approach, proposed by Feder and Merhav, with Chernoff bound and Gallager's techniques for the analysis of error exponents. In particular, we derive a single--letter expression for the largest, universally achievable fraction $\\xi$ of the optimum error exponent pertaining to the optimum ML decoding. Moreover, a simpler single--letter expression for a lower bound to $\\xi$ is presented. To demonstrate the tightness of this lower bound, we use it to show that $\\xi=1$, for the binary symmetric channel (BSC), when the random coding distribution is uniform over: (i) all codes (of a given rate), and (ii) all linear codes, in agreement with well--known results. We also show that $\\xi=1$ for the uniform ensemble of systematic linear codes, and for that of time--varying convolutional codes in the bit-error--rate sense. For the latter case, we also show how the corresponding universal decoder can be efficiently implemented using a slightly modified version of the Viterbi algorithm which em employs two trellises.", ["Binary symmetric channel", "Viterbi algorithm", "Algorithm", "Convolutional code", "Chernoff bound", "Minimax", "Exponentiation", "Robert G. Gallager", "Upper and lower bounds"]], ["On Throughput Scaling of Wireless Networks: Effect of Node Density and Propagation Model", "This paper derives a lower bound to the per-node throughput achievable by a wireless network when n source-destination pairs are randomly distributed throughout a disk of radius $n^\\gamma$, $ \\gamma \\geq 0$, propagation is modeled by attenuation of the form $1/(1+d)^\\alpha$, $\\alpha >2$, and successful transmission occurs at a fixed rate W when received signal to noise and interference ratio is greater than some threshold $\\beta$, and at rate 0 otherwise. The lower bound has the form $n^{1-\\gamma}$ when $\\gamma < 1/2$, and $(n \\ln n)^{-1/2}$ when $\\gamma \\geq 1/2$. The methods are similar to, but somewhat simpler than, those in the seminal paper by Gupta and Kumar.", ["Wireless", "Throughput", "Attenuation", "Radius", "Signal-to-noise ratio", "Wireless network", "Transmission (telecommunications)", "Density", "Noise"]], ["Image Authentication Based on Neural Networks", "Neural network has been attracting more and more researchers since the past decades. The properties, such as parameter sensitivity, random similarity, learning ability, etc., make it suitable for information protection, such as data encryption, data authentication, intrusion detection, etc. In this paper, by investigating neural networks' properties, the low-cost authentication method based on neural networks is proposed and used to authenticate images or videos. The authentication method can detect whether the images or videos are modified maliciously. Firstly, this chapter introduces neural networks' properties, such as parameter sensitivity, random similarity, diffusion property, confusion property, one-way property, etc. Secondly, the chapter gives an introduction to neural network based protection methods. Thirdly, an image or video authentication scheme based on neural networks is presented, and its performances, including security, robustness and efficiency, are analyzed. Finally, conclusions are drawn, and some open issues in this field are presented.", ["Neural network", "One-way function", "Diffusion", "Authentication", "Data", "Cryptography"]], ["On the Complexity of the Interlace Polynomial", "We consider the two-variable interlace polynomial introduced by Arratia, Bollobas and Sorkin (2004). We develop graph transformations which allow us to derive point-to-point reductions for the interlace polynomial. Exploiting these reductions we obtain new results concerning the computational complexity of evaluating the interlace polynomial at a fixed point. Regarding exact evaluation, we prove that the interlace polynomial is #P-hard to evaluate at every point of the plane, except on one line, where it is trivially polynomial time computable, and four lines, where the complexity is still open. This solves a problem posed by Arratia, Bollobas and Sorkin (2004). In particular, three specializations of the two-variable interlace polynomial, the vertex-nullity interlace polynomial, the vertex-rank interlace polynomial and the independent set polynomial, are almost everywhere #P-hard to evaluate, too. For the independent set polynomial, our reductions allow us to prove that it is even hard to approximate at any point except at 0.", ["Polynomial time", "Computational complexity theory", "Graph (mathematics)", "Complexity", "Hardness of approximation", "Independent set (graph theory)", "Almost everywhere", "Fixed point (mathematics)", "Computable function", "Polynomial"]], ["Note on edge-colored graphs and digraphs without properly colored cycles", "We study the following two functions: d(n,c) and $\\vec{d}(n,c)$; d(n,c) ($\\vec{d}(n,c)$) is the minimum number k such that every c-edge-colored undirected (directed) graph of order n and minimum monochromatic degree (out-degree) at least k has a properly colored cycle. Abouelaoualim et al. (2007) stated a conjecture which implies that d(n,c)=1. Using a recursive construction of c-edge-colored graphs with minimum monochromatic degree p and without properly colored cycles, we show that $d(n,c)\\ge {1 \\over c}(\\log_cn -\\log_c\\log_cn)$ and, thus, the conjecture does not hold. In particular, this inequality significantly improves a lower bound on $\\vec{d}(n,2)$ obtained by Gutin, Sudakov and Yeo in 1998.", ["Graph (mathematics)", "Degree (graph theory)", "Upper and lower bounds", "Conjecture", "Digraph (orthography)", "Undirected graph", "Recursion"]], ["Side-information Scalable Source Coding", "The problem of side-information scalable (SI-scalable) source coding is considered in this work, where the encoder constructs a progressive description, such that the receiver with high quality side information will be able to truncate the bitstream and reconstruct in the rate distortion sense, while the receiver with low quality side information will have to receive further data in order to decode. We provide inner and outer bounds for general discrete memoryless sources. The achievable region is shown to be tight for the case that either of the decoders requires a lossless reconstruction, as well as the case with degraded deterministic distortion measures. Furthermore we show that the gap between the achievable region and the outer bounds can be bounded by a constant when square error distortion measure is used. The notion of perfectly scalable coding is introduced as both the stages operate on the Wyner-Ziv bound, and necessary and sufficient conditions are given for sources satisfying a mild support condition. Using SI-scalable coding and successive refinement Wyner-Ziv coding as basic building blocks, a complete characterization is provided for the important quadratic Gaussian source with multiple jointly Gaussian side-informations, where the side information quality does not have to be monotonic along the scalable coding order. Partial result is provided for the doubly symmetric binary source with Hamming distortion when the worse side information is a constant, for which one of the outer bound is strictly tighter than the other one.", ["Memorylessness", "Multivariate normal distribution", "Necessary and sufficient condition", "Lossless data compression", "Bitstream", "Encoder", "Monotonic function", "Data compression", "International System of Units", "Distortion"]], ["Nonlinear Matroid Optimization and Experimental Design", "We study the problem of optimizing nonlinear objective functions over matroids presented by oracles or explicitly. Such functions can be interpreted as the balancing of multi-criteria optimization. We provide a combinatorial polynomial time algorithm for arbitrary oracle-presented matroids, that makes repeated use of matroid intersection, and an algebraic algorithm for vectorial matroids. Our work is partly motivated by applications to minimum-aberration model-fitting in experimental design in statistics, which we discuss and demonstrate in detail.", ["Polynomial", "Matroid", "Combinatorics", "Nonlinear system", "Statistics", "Design of experiments", "Euclidean vector", "Mathematical optimization", "Multi-objective optimization", "Polynomial time", "Algorithm"]], ["Comments on the Reliability of Lawson and Hanson's Linear Distance Programming Algorithm: Subroutine LDP", "This brief paper: (1) Discusses strategies to generate random test cases that can be used to extensively test any Linear Distance Program (LDP) software. (2) Gives three numerical examples of input cases generated by this strategy that cause problems in the Lawson and Hanson LDP module. (3) Proposes, as a standard matter of acceptable implementation procedures, that (unless it is done internally in the software itself, but, in general, this seems to be much rarer than one would expect) all users should test the returned output from any LDP module for self-consistency since it incurs only a small amount of added computational overhead and it is not hard to do.", ["Subroutine", "Randomness"]], ["Communication under Strong Asynchronism", "We consider asynchronous communication over point-to-point discrete memoryless channels. The transmitter starts sending one block codeword at an instant that is uniformly distributed within a certain time period, which represents the level of asynchronism. The receiver, by means of a sequential decoder, must isolate the message without knowing when the codeword transmission starts but being cognizant of the asynchronism level A. We are interested in how quickly can the receiver isolate the sent message, particularly in the regime where A is exponentially larger than the codeword length N, which we refer to as `strong asynchronism.' This model of sparse communication may represent the situation of a sensor that remains idle most of the time and, only occasionally, transmits information to a remote base station which needs to quickly take action. The first result shows that vanishing error probability can be guaranteed as N tends to infinity while A grows as Exp(N*k) if and only if k does not exceed the `synchronization threshold,' a constant that admits a simple closed form expression, and is at least as large as the capacity of the synchronized channel. The second result is the characterization of a set of achievable strictly positive rates in the regime where A is exponential in N, and where the rate is defined with respect to the expected delay between the time information starts being emitted until the time the receiver makes a decision. As an application of the first result we consider antipodal signaling over a Gaussian channel and derive a simple necessary condition between A, N, and SNR for achieving reliable communication.", ["Additive white Gaussian noise", "If and only if", "Memorylessness", "Probability", "Transmitter", "Code word", "Infinity", "Sensor", "Decoder", "Asynchronous communication", "Base station", "Synchronization", "Communication", "Closed-form expression"]], ["Difference Equations in Massive Higher Order Calculations", "The calculation of massive 2--loop operator matrix elements, required for the higher order Wilson coefficients for heavy flavor production in deeply inelastic scattering, leads to new types of multiple infinite sums over harmonic sums and related functions, which depend on the Mellin parameter $N$. We report on the solution of these sums through higher order difference equations using the summation package {\\tt Sigma}.", ["Scattering", "Recurrence relation", "Series (mathematics)", "Summation", "Matrix (mathematics)", "Mellin transform"]], ["Periodic complementary sets of binary sequences", "Let PCS_p^N denote a set of p binary sequences of length N such that the sum of their periodic auto-correlation functions is a delta-function. In the 1990, Boemer and Antweiler addressed the problem of constructing such sequences. They presented a table covering the range p <= 12, N <= 50 and showing in which cases it was known at that time whether such sequences exist, do not exist, or the question of existence is undecided. The number of undecided cases was rather large. Subsequently the number of undecided cases was reduced to 26 by the author. In the present note, several cyclic difference families are constructed and used to obtain new sets of periodic binary sequences. Thereby the original problem of Boemer and Antweiler is completely solved.", ["Complement (set theory)", "Autocorrelation"]], ["Virtual screening with support vector machines and structure kernels", "Support vector machines and kernel methods have recently gained considerable attention in chemoinformatics. They offer generally good performance for problems of supervised classification or regression, and provide a flexible and computationally efficient framework to include relevant information and prior knowledge about the data and problems to be handled. In particular, with kernel methods molecules do not need to be represented and stored explicitly as vectors or fingerprints, but only to be compared to each other through a comparison function technically called a kernel. While classical kernels can be used to compare vector or fingerprint representations of molecules, completely new kernels were developed in the recent years to directly compare the 2D or 3D structures of molecules, without the need for an explicit vectorization step through the extraction of molecular descriptors. While still in their infancy, these approaches have already demonstrated their relevance on several toxicity prediction and structure-activity relationship problems.", ["Structure-activity relationship", "Virtual screening", "Cheminformatics", "Support vector machine", "Toxicity", "Kernel methods", "2D computer graphics", "Fingerprint", "Computational complexity theory", "Molecule"]], ["A Note on Shortest Developments", "De Vrijer has presented a proof of the finite developments theorem which, in addition to showing that all developments are finite, gives an effective reduction strategy computing longest developments as well as a simple formula computing their length. We show that by applying a rather simple and intuitive principle of duality to de Vrijer's approach one arrives at a proof that some developments are finite which in addition yields an effective reduction strategy computing shortest developments as well as a simple formula computing their length. The duality fails for general beta-reduction. Our results simplify previous work by Khasidashvili.", ["Lambda calculus", "Finite set"]], ["Multisource Bayesian sequential change detection", "Suppose that local characteristics of several independent compound Poisson and Wiener processes change suddenly and simultaneously at some unobservable disorder time. The problem is to detect the disorder time as quickly as possible after it happens and minimize the rate of false alarms at the same time. These problems arise, for example, from managing product quality in manufacturing systems and preventing the spread of infectious diseases. The promptness and accuracy of detection rules improve greatly if multiple independent information sources are available. Earlier work on sequential change detection in continuous time does not provide optimal rules for situations in which several marked count data and continuously changing signals are simultaneously observable. In this paper, optimal Bayesian sequential detection rules are developed for such problems when the marked count data is in the form of independent compound Poisson processes, and the continuously changing signals form a multi-dimensional Wiener process. An auxiliary optimal stopping problem for a jump-diffusion process is solved by transforming it first into a sequence of optimal stopping problems for a pure diffusion by means of a jump operator. This method is new and can be very useful in other applications as well, because it allows the use of the powerful optimal stopping theory for diffusions.", ["Wiener process", "Optimal stopping", "Sequence", "Manufacturing", "Observable", "Diffusion", "Independence (probability theory)"]], ["Distributing the Kalman Filter for Large-Scale Systems", "This paper derives a \\emph{distributed} Kalman filter to estimate a sparsely connected, large-scale, $n-$dimensional, dynamical system monitored by a network of $N$ sensors. Local Kalman filters are implemented on the ($n_l-$dimensional, where $n_l\\ll n$) sub-systems that are obtained after spatially decomposing the large-scale system. The resulting sub-systems overlap, which along with an assimilation procedure on the local Kalman filters, preserve an $L$th order Gauss-Markovian structure of the centralized error processes. The information loss due to the $L$th order Gauss-Markovian approximation is controllable as it can be characterized by a divergence that decreases as $L\\uparrow$. The order of the approximation, $L$, leads to a lower bound on the dimension of the sub-systems, hence, providing a criterion for sub-system selection. The assimilation procedure is carried out on the local error covariances with a distributed iterate collapse inversion (DICI) algorithm that we introduce. The DICI algorithm computes the (approximated) centralized Riccati and Lyapunov equations iteratively with only local communication and low-order computation. We fuse the observations that are common among the local Kalman filters using bipartite fusion graphs and consensus averaging algorithms. The proposed algorithm achieves full distribution of the Kalman filter that is coherent with the centralized Kalman filter with an $L$th order Gaussian-Markovian structure on the centralized error processes. Nowhere storage, communication, or computation of $n-$dimensional vectors and matrices is needed; only $n_l \\ll n$ dimensional vectors and matrices are communicated or used in the computation at the sensors.", ["Kalman filter", "Dynamical system", "Algorithm", "Numerical ordinary differential equations", "Carl Friedrich Gauss", "Divergence", "Bipartite graph", "Matrix (mathematics)", "Upper and lower bounds", "Communication"]], ["Capacity Region of the Finite-State Multiple Access Channel with and without Feedback", "The capacity region of the Finite-State Multiple Access Channel (FS-MAC) with feedback that may be an arbitrary time-invariant function of the channel output samples is considered. We characterize both an inner and an outer bound for this region, using Masseys's directed information. These bounds are shown to coincide, and hence yield the capacity region, of FS-MACs where the state process is stationary and ergodic and not affected by the inputs. Though `multi-letter' in general, our results yield explicit conclusions when applied to specific scenarios of interest. E.g., our results allow us to: - Identify a large class of FS-MACs, that includes the additive mod-2 noise MAC where the noise may have memory, for which feedback does not enlarge the capacity region. - Deduce that, for a general FS-MAC with states that are not affected by the input, if the capacity (region) without feedback is zero, then so is the capacity (region) with feedback. - Deduce that the capacity region of a MAC that can be decomposed into a `multiplexer' concatenated by a point-to-point channel (with, without, or with partial feedback), the capacity region is given by $\\sum_{m} R_m \\leq C$, where C is the capacity of the point to point channel and m indexes the encoders. Moreover, we show that for this family of channels source-channel coding separation holds.", ["Message authentication code", "Modular arithmetic", "Feedback", "Multiplexer", "Concatenation", "Forward error correction", "Time-invariant system", "Encoder"]], ["The Local Fractal Properties of the Financial Time Series on the Polish Stock Exchange Market", "We investigate the local fractal properties of the financial time series based on the evolution of the Warsaw Stock Exchange Index (WIG) connected with the largest developing financial market in Europe. Calculating the local Hurst exponent for the WIG time series we find an interesting dependence between the behavior of the local fractal properties of the WIG time series and the crashes appearance on the financial market.", ["Warsaw Stock Exchange", "Europe", "Fractal", "Evolution", "Poland", "Financial market", "Time series", "Time (magazine)", "Warsaw", "Exponentiation"]], ["Why the relational data model can be considered as a formal basis for group operations in object-oriented systems", "Relational data model defines a specification of a type \"relation\". However, its simplicity does not mean that the system implementing this model must operate with structures having the same simplicity. We consider two principles allowing create a system which combines object-oriented paradigm (OOP) and relational data model (RDM) in one framework. The first principle -- \"complex data in encapsulated domains\" -- is well known from The Third Manifesto by Date and Darwen. The second principle --\"data complexity in names\"-- is the basis for a system where data are described as complex objects and uniquely represented as a set of relations. Names of these relations and names of their attributes are combinations of names entered in specifications of the complex objects. Below, we consider the main properties of such a system.", ["Relational model", "Paradigm", "The Third Manifesto", "Object-oriented programming", "Data model", "Relational database"]], ["Diversity of MIMO Multihop Relay Channels", "We consider slow fading relay channels with a single multi-antenna source-destination terminal pair. The source signal arrives at the destination via N hops through N-1 layers of relays. We analyze the diversity of such channels with fixed network size at high SNR. In the clustered case where the relays within the same layer can have full cooperation, the cooperative decode-and-forward (DF) scheme is shown to be optimal in terms of the diversity-multiplexing tradeoff (DMT). The upper bound on the DMT, the cut-set bound, is attained. In the non-clustered case, we show that the naive amplify-and-forward (AF) scheme has the maximum multiplexing gain of the channel but is suboptimal in diversity, as compared to the cut-set bound. To improve the diversity, space-time relay processing is introduced through the parallel partition of the multihop channel. The idea is to let the source signal go through K different \"AF paths\" in the multihop channel. This parallel AF scheme creates a parallel channel in the time domain and has the maximum diversity if the partition is properly designed. Since this scheme does not achieve the maximum multiplexing gain in general, we propose a flip-and-forward (FF) scheme that is built from the parallel AF scheme. It is shown that the FF scheme achieves both the maximum diversity and multiplexing gains in a distributed multihop channel of arbitrary size. In order to realize the DMT promised by the relaying strategies, approximately universal coding schemes are also proposed.", ["Time domain", "Orthogonal frequency-division multiplexing", "MIMO", "Multiplexing", "Spacetime", "Signal-to-noise ratio", "Fading"]], ["Virtual Manufacturing : Tools for improving Design and Production", "The research area \"Virtual Manufacturing\" can be defined as an integrated manufacturing environment which can enhance one or several levels of decision and control in manufacturing process. Several domains can be addressed: Product and Process Design, Process and Production Planning, Machine Tool, Robot and Manufacturing System. As automation technologies such as CAD/CAM have substantially shortened the time required to design products, Virtual Manufacturing will have a similar effect on the manufacturing phase thanks to the modelling, simulation and optimisation of the product and the processes involved in its fabrication.", ["Simulation", "Product design", "Computer-aided design", "Process simulation", "Automation", "Computer-aided manufacturing", "Industrial engineering", "Manufacturing"]], ["A preliminary analysis on metaheuristics methods applied to the Haplotype Inference Problem", "Haplotype Inference is a challenging problem in bioinformatics that consists in inferring the basic genetic constitution of diploid organisms on the basis of their genotype. This information allows researchers to perform association studies for the genetic variants involved in diseases and the individual responses to therapeutic agents. A notable approach to the problem is to encode it as a combinatorial problem (under certain hypotheses, such as the pure parsimony criterion) and to solve it using off-the-shelf combinatorial optimization techniques. The main methods applied to Haplotype Inference are either simple greedy heuristic or exact methods (Integer Linear Programming, Semidefinite Programming, SAT encoding) that, at present, are adequate only for moderate size instances. We believe that metaheuristic and hybrid approaches could provide a better scalability. Moreover, metaheuristics can be very easily combined with problem specific heuristics and they can also be integrated with tree-based search techniques, thus providing a promising framework for hybrid systems in which a good trade-off between effectiveness and efficiency can be reached. In this paper we illustrate a feasibility study of the approach and discuss some relevant design issues, such as modeling and design of approximate solvers that combine constructive heuristics, local search-based improvement strategies and learning mechanisms. Besides the relevance of the Haplotype Inference problem itself, this preliminary analysis is also an interesting case study because the formulation of the problem poses some challenges in modeling and hybrid metaheuristic solver design that can be generalized to other problems.", ["Metaheuristic", "Bioinformatics", "Combinatorial optimization", "Occam's razor", "Greedy algorithm", "Combinatorics", "Genetics", "Search algorithm", "Diploid", "Linear programming", "Inference", "Heuristic", "Mathematical optimization", "Hybrid (biology)", "Trade-off", "Genotype", "Local search (optimization)", "SAT", "Haplotype", "Scalability", "Hypothesis", "Semidefinite programming", "Computer simulation"]], ["Quasi-stationary distributions as centrality measures of reducible graphs", "Random walk can be used as a centrality measure of a directed graph. However, if the graph is reducible the random walk will be absorbed in some subset of nodes and will never visit the rest of the graph. In Google PageRank the problem was solved by introduction of uniform random jumps with some probability. Up to the present, there is no clear criterion for the choice this parameter. We propose to use parameter-free centrality measure which is based on the notion of quasi-stationary distribution. Specifically we suggest four quasi-stationary based centrality measures, analyze them and conclude that they produce approximately the same ranking. The new centrality measures can be applied in spam detection to detect ``link farms'' and in image search to find photo albums.", ["Random walk", "Subset", "PageRank", "Directed graph", "Graph (mathematics)", "Image retrieval", "Uniform distribution (discrete)", "Probability distribution", "Parameter", "Spam (electronic)", "Randomness", "Stationary distribution"]], ["Efficient Divide-and-Conquer Implementations Of Symmetric FSAs", "A deterministic finite-state automaton (FSA) is an abstract sequential machine that reads the symbols comprising an input word one at a time. An FSA is symmetric if its output is independent of the order in which the input symbols are read, i.e., if the output is invariant under permutations of the input. We show how to convert a symmetric FSA A into an automaton-like divide-and-conquer process whose intermediate results are no larger than the size of A's memory. In comparison, a similar result for general FSA's has been long known via functional composition, but entails an exponential increase in memory size. The new result has applications to parallel processing and symmetric FSA networks.", ["Finite-state machine", "Permutation", "Postal codes in Canada", "Function composition", "Finite set", "Automaton", "Exponential growth", "Exponential function"]], ["An Application of Chromatic Prototypes", "This paper has been withdrawn.", ["Diatonic and chromatic"]], ["Complementary algorithms for graphs and percolation", "A pair of complementary algorithms are presented. One of the pair is a fast method for connecting graphs with an edge. The other is a fast method for removing edges from a graph. Both algorithms employ the same tree based graph representation and so, in concert, can arbitrarily modify any graph. Since the clusters of a percolation model may be described as simple connected graphs, an efficient Monte Carlo scheme can be constructed that uses the algorithms to sweep the occupation probability back and forth between two turning points. This approach concentrates computational sampling time within a region of interest. A high precision value of pc = 0.59274603(9) was thus obtained, by Mersenne twister, for the two dimensional square site percolation threshold.", ["Percolation", "Probability", "Mersenne twister", "Percolation threshold", "Monte Carlo method"]], ["Public Cluster : parallel machine with multi-block approach", "We introduce a new approach to enable an open and public parallel machine which is accessible for multi users with multi jobs belong to different blocks running at the same time. The concept is required especially for parallel machines which are dedicated for public use as implemented at the LIPI Public Cluster. We have deployed the simplest technique by running multi daemons of parallel processing engine with different configuration files specified for each user assigned to access the system, and also developed an integrated system to fully control and monitor the whole system over web. A brief performance analysis is also given for Message Parsing Interface (MPI) engine. It is shown that the proposed approach is quite reliable and affect the whole performances only slightly.", ["Parallel computing", "Parsing", "Daemon (computing)"]], ["Introducing OPTO : Portal for Optical Communities in Indonesia", "Since January 1, 2005 we have launched \"OPTO\" Portal, a website dedicated to optical communities in Indonesia. The address of this portal is http://www.opto.lipi.go.id and is self-supporting managed and not for commercial purposes. Our aims in launching this portal are to benefit Internet facility in increasing the communities' scientific activity; to provide an online reference in Indonesian language for optics-based science and technology subjects; as well as to pioneer the communities' online activities with real impacts and benefits for our society. We will describe in the paper the features of this portal that can be utilized by all individuals or members of optical communities to store and share information and to build networks or partnership as well. We realized that this portal is still not popular and most of our aims are still not reached. This conference should be a good place for all of us to collaborate to properly utilize this portal for the advantages to the optical communities in Indonesia and our society at large.", ["Indonesia", "Internet", "Science", "Indonesian language", "Optics", "Technology"]], ["Open and Free Cluster for Public", "We introduce the LIPI Public Cluster, the first parallel machine facility fully open for public and for free in Indonesia and surrounding countries. In this paper, we focus on explaining our globally new concept on open cluster, and how to realize and manage it to meet the users needs. We show that after 2 years trial running and several upgradings, the Public Cluster performs well and is able to fulfil all requirements as expected.", ["Indonesia", "Cluster (band)"]], ["Real-time control and monitoring system for LIPI's Public Cluster", "We have developed a monitoring and control system for LIPI's Public Cluster. The system consists of microcontrollers and full web-based user interfaces for daily operation. It is argued that, due to its special natures, the cluster requires fully dedicated and self developed control and monitoring system. We discuss the implementation of using parallel port and dedicated micro-controller for this purpose. We also show that integrating such systems enables an autonomous control system based on the real time monitoring, for instance an autonomous power supply control based on the actual temperature, etc.", ["Parallel port", "Web application", "Temperature", "Time control", "Control system", "Power supply", "Microcontroller"]], ["Resource Allocation in Public Cluster with Extended Optimization Algorithm", "We introduce an optimization algorithm for resource allocation in the LIPI Public Cluster to optimize its usage according to incoming requests from users. The tool is an extended and modified genetic algorithm developed to match specific natures of public cluster. We present a detail analysis of optimization, and compare the results with the exact calculation. We show that it would be very useful and could realize an automatic decision making system for public clusters.", ["Genetic algorithm", "Cluster (band)", "Computer cluster", "Mathematical optimization"]], ["ADS-Directory Services for Mobile Ad-Hoc Networks Based on an Information Market Model", "Ubiquitous computing based on small mobile devices using wireless communication links is becoming very attractive. The computational power and storage capacities provided allow the execution of sophisticated applications. Due to the fact that sharing of information is a central problem for distributed applications, the development of self organizing middleware services providing high level interfaces for information managing is essential. ADS is a directory service for mobile ad-hoc networks dealing with local and nearby information as well as providing access to distant information. The approach discussed throughout this paper is based upon the concept of information markets.", ["Ubiquitous computing", "Wireless", "Computing", "Mobile ad hoc network", "Directory service", "Mobile phone", "Computer network", "Middleware", "Mobile device", "Ad hoc", "Communication", "Moore's law", "Domain Name System"]], ["ADS as Information Management Service in an M-Learning Environment", "Leveraging the potential power of even small handheld devices able to communicate wirelessly requires dedicated support. In particular, collaborative applications need sophisticated assistance in terms of querying and exchanging different kinds of data. Using a concrete example from the domain of mobile learning, the general need for information dissemination is motivated. Subsequently, and driven by infrastructural conditions, realization strategies of an appropriate middleware service are discussed.", ["Middleware", "Information management", "MLearning"]], ["Auction-Based Distributed Resource Allocation for Cooperation Transmission in Wireless Networks", "Cooperative transmission can greatly improve communication system performance by taking advantage of the broadcast nature of wireless channels. Most previous work on resource allocation for cooperation transmission is based on centralized control. In this paper, we propose two share auction mechanisms, the SNR auction and the power auction, to distributively coordinate the resource allocation among users. We prove the existence, uniqueness and effectiveness of the auction results. In particular, the SNR auction leads to a fair resource allocation among users, and the power auction achieves a solution that is close to the efficient allocation.", ["Wireless", "Wireless network", "Transmission (telecommunications)", "Communications system", "Auction", "Signal-to-noise ratio", "Broadcasting", "Communication"]], ["Structure or Noise?", "We show how rate-distortion theory provides a mechanism for automated theory building by naturally distinguishing between regularity and randomness. We start from the simple principle that model variables should, as much as possible, render the future and past conditionally independent. From this, we construct an objective function for model making whose extrema embody the trade-off between a model's structural complexity and its predictive power. The solutions correspond to a hierarchy of models that, at each level of complexity, achieve optimal predictive power at minimal cost. In the limit of maximal prediction the resulting optimal model identifies a process's intrinsic organization by extracting the underlying causal states. In this limit, the model's complexity is given by the statistical complexity, which is known to be minimal for achieving maximum prediction. Examples show how theory building can profit from analyzing a process's causal compressibility, which is reflected in the optimal models' rate-distortion curve--the process's characteristic for optimally balancing structure and noise at different levels of representation.", ["Conditional independence", "Mathematical optimization", "Predictive power", "Compressibility", "Hierarchy", "Randomness", "Prediction", "Theory"]], ["Network synchronizability analysis: the theory of subgraphs and complementary graphs", "In this paper, subgraphs and complementary graphs are used to analyze the network synchronizability. Some sharp and attainable bounds are provided for the eigenratio of the network structural matrix, which characterizes the network synchronizability, especially when the network's corresponding graph has cycles, chains, bipartite graphs or product graphs as its subgraphs.", ["Synchronization", "Matrix (mathematics)", "Bipartite graph", "Graph (mathematics)"]], ["Reconstruction of Protein-Protein Interaction Pathways by Mining Subject-Verb-Objects Intermediates", "The exponential increase in publication rate of new articles is limiting access of researchers to relevant literature. This has prompted the use of text mining tools to extract key biological information. Previous studies have reported extensive modification of existing generic text processors to process biological text. However, this requirement for modification had not been examined. In this study, we have constructed Muscorian, using MontyLingua, a generic text processor. It uses a two-layered generalization-specialization paradigm previously proposed where text was generically processed to a suitable intermediate format before domain-specific data extraction techniques are applied at the specialization layer. Evaluation using a corpus and experts indicated 86-90% precision and approximately 30% recall in extracting protein-protein interactions, which was comparable to previous studies using either specialized biological text processing tools or modified existing tools. Our study had also demonstrated the flexibility of the two-layered generalization-specialization paradigm by using the same generalization layer for two specialized information extraction tasks.", ["Paradigm", "Text mining", "Protein", "Verb", "Information extraction", "Reconstruction Era of the United States", "Word processing", "Literature", "Mining", "Biology", "Central processing unit", "Exponential growth"]], ["Virtual Environments for Training: From Individual Learning to Collaboration with Humanoids", "The next generation of virtual environments for training is oriented towards collaborative aspects. Therefore, we have decided to enhance our platform for virtual training environments, adding collaboration opportunities and integrating humanoids. In this paper we put forward a model of humanoid that suits both virtual humans and representations of real users, according to collaborative training activities. We suggest adaptations to the scenario model of our platform making it possible to write collaborative procedures. We introduce a mechanism of action selection made up of a global repartition and an individual choice. These models are currently being integrated and validated in GVT, a virtual training tool for maintenance of military equipments, developed in collaboration with the French company NEXTER-Group.", ["E-learning", "Virtual reality", "Humanoid", "Action selection"]], ["Edit and verify", "Automated theorem provers are used in extended static checking, where they are the performance bottleneck. Extended static checkers are run typically after incremental changes to the code. We propose to exploit this usage pattern to improve performance. We present two approaches of how to do so and a full solution.", ["Automated theorem proving"]], ["Characterising Web Site Link Structure", "The topological structures of the Internet and the Web have received considerable attention. However, there has been little research on the topological properties of individual web sites. In this paper, we consider whether web sites (as opposed to the entire Web) exhibit structural similarities. To do so, we exhaustively crawled 18 web sites as diverse as governmental departments, commercial companies and university departments in different countries. These web sites consisted of as little as a few thousand pages to millions of pages. Statistical analysis of these 18 sites revealed that the internal link structure of the web sites are significantly different when measured with first and second-order topological properties, i.e. properties based on the connectivity of an individual or a pairs of nodes. However, examination of a third-order topological property that consider the connectivity between three nodes that form a triangle, revealed a strong correspondence across web sites, suggestive of an invariant. Comparison with the Web, the AS Internet, and a citation network, showed that this third-order property is not shared across other types of networks. Nor is the property exhibited in generative network models such as that of Barabasi and Albert.", ["Statistics", "Topological property", "Internet", "Topology", "University"]], ["Cooperative Beamforming for Wireless Ad Hoc Networks", "Via collaborative beamforming, nodes in a wireless network are able to transmit a common message over long distances in an energy efficient fashion. However, the process of making available the same message to all collaborating nodes introduces delays. In this paper, a MAC-PHY cross-layer scheme is proposed that enables collaborative beamforming at significantly reduced collaboration overhead. It consists of two phases. In the first phase, nodes transmit locally in a random access time-slotted fashion. Simultaneous transmissions from multiple source nodes are viewed as linear mixtures of all transmitted packets. In the second phase, a set of collaborating nodes, acting as a distributed antenna system, beamform the received analog waveform to one or more faraway destinations. This step requires multiplication of the received analog waveform by a complex weight, which is independently computed by each cooperating node, and which allows packets bound to the same destination to add coherently at the destination node. Assuming that each node has access to location information, the proposed scheme can achieve high throughput, which in certain cases exceeds one. An analysis of the symbol error probability corresponding to the proposed scheme is provided.", ["Wireless network", "Beamforming", "Network packet", "PHY (chip)", "Waveform", "Access time", "Wireless", "Energy", "Antenna (radio)", "Random access", "Throughput", "Probability", "Media Access Control"]], ["Cooperative game theory and the Gaussian interference channel", "In this paper we discuss the use of cooperative game theory for analyzing interference channels. We extend our previous work, to games with N players as well as frequency selective channels and joint TDM/FDM strategies. We show that the Nash bargaining solution can be computed using convex optimization techniques. We also show that the same results are applicable to interference channels where only statistical knowledge of the channel is available. Moreover, for the special case of two players $2\\times K$ frequency selective channel (with K frequency bins) we provide an $O(K \\log_2 K)$ complexity algorithm for computing the Nash bargaining solution under mask constraint and using joint FDM/TDM strategies. Simulation results are also provided.", ["Simulation", "Game theory", "Mathematical optimization", "Convex optimization", "Computing", "Frequency", "Algorithm", "Statistics", "Time-division multiplexing", "Cooperative game"]], ["Relations between random coding exponents and the statistical physics of random codes", "The partition function pertaining to finite--temperature decoding of a (typical) randomly chosen code is known to have three types of behavior, corresponding to three phases in the plane of rate vs. temperature: the {\\it ferromagnetic phase}, corresponding to correct decoding, the {\\it paramagnetic phase}, of complete disorder, which is dominated by exponentially many incorrect codewords, and the {\\it glassy phase} (or the condensed phase), where the system is frozen at minimum energy and dominated by subexponentially many incorrect codewords. We show that the statistical physics associated with the two latter phases are intimately related to random coding exponents. In particular, the exponent associated with the probability of correct decoding at rates above capacity is directly related to the free energy in the glassy phase, and the exponent associated with probability of error (the error exponent) at rates below capacity, is strongly related to the free energy in the paramagnetic phase. In fact, we derive alternative expressions of these exponents in terms of the corresponding free energies, and make an attempt to obtain some insights from these expressions. Finally, as a side result, we also compare the phase diagram associated with a simple finite-temperature universal decoder for discrete memoryless channels, to that of the finite--temperature decoder that is aware of the channel statistics.", ["Ferromagnetism", "Statistical physics", "Phase diagram", "Partition function (statistical mechanics)", "Probability", "Memorylessness", "Thermodynamic free energy", "Paramagnetism", "Statistics", "Laplace operator", "Energy", "Physics", "Exponentiation", "Function (mathematics)"]], ["A Portal Analysis for the Design of a Collaborative Research Environment for Students and Supervisors (CRESS) within the CSCR Domain", "In a previous paper the CSCR domain was defined. Here this is taken to the next stage where we consider the design of a particular Collaborative Research Environment to support Students and Supervisors CRESS. Following the CSCR structure a preliminary design for CRESS has been established and a portal framework analysis is undertaken in order to determine the most appropriate set of tools for its implementation.", ["Domain of a function", "Design", "Enterprise portal"]], ["Permutation Decoding and the Stopping Redundancy Hierarchy of Cyclic and Extended Cyclic Codes", "We introduce the notion of the stopping redundancy hierarchy of a linear block code as a measure of the trade-off between performance and complexity of iterative decoding for the binary erasure channel. We derive lower and upper bounds for the stopping redundancy hierarchy via Lovasz's Local Lemma and Bonferroni-type inequalities, and specialize them for codes with cyclic parity-check matrices. Based on the observed properties of parity-check matrices with good stopping redundancy characteristics, we develop a novel decoding technique, termed automorphism group decoding, that combines iterative message passing and permutation decoding. We also present bounds on the smallest number of permutations of an automorphism group decoder needed to correct any set of erasures up to a prescribed size. Simulation results demonstrate that for a large number of algebraic codes, the performance of the new decoding method is close to that of maximum likelihood decoding.", ["Block code", "Binary erasure channel", "Permutation", "Matrix (mathematics)", "Maximum likelihood", "Automorphism group", "Iteration", "Simulation", "Automorphism", "Decoding methods", "Message passing"]], ["On the Self-stabilization of Mobile Robots in Graphs", "Self-stabilization is a versatile technique to withstand any transient fault in a distributed system. Mobile robots (or agents) are one of the emerging trends in distributed computing as they mimic autonomous biologic entities. The contribution of this paper is threefold. First, we present a new model for studying mobile entities in networks subject to transient faults. Our model differs from the classical robot model because robots have constraints about the paths they are allowed to follow, and from the classical agent model because the number of agents remains fixed throughout the execution of the protocol. Second, in this model, we study the possibility of designing self-stabilizing algorithms when those algorithms are run by mobile robots (or agents) evolving on a graph. We concentrate on the core building blocks of robot and agents problems: naming and leader election. Not surprisingly, when no constraints are given on the network graph topology and local execution model, both problems are impossible to solve. Finally, using minimal hypothesis with respect to impossibility results, we provide deterministic and probabilistic solutions to both problems, and show equivalence of these problems by an algorithmic reduction mechanism.", ["Computing", "Robot", "Mobile robot", "Glossary of graph theory", "Algorithm", "Topology", "Distributed computing", "Determinism"]], ["Modeling Visual Information Processing in Brain: A Computer Vision Point of View and Approach", "We live in the Information Age, and information has become a critically important component of our life. The success of the Internet made huge amounts of it easily available and accessible to everyone. To keep the flow of this information manageable, means for its faultless circulation and effective handling have become urgently required. Considerable research efforts are dedicated today to address this necessity, but they are seriously hampered by the lack of a common agreement about \"What is information?\" In particular, what is \"visual information\" - human's primary input from the surrounding world. The problem is further aggravated by a long-lasting stance borrowed from the biological vision research that assumes human-like information processing as an enigmatic mix of perceptual and cognitive vision faculties. I am trying to find a remedy for this bizarre situation. Relying on a new definition of \"information\", which can be derived from Kolmogorov's compexity theory and Chaitin's notion of algorithmic information, I propose a unifying framework for visual information processing, which explicitly accounts for the perceptual and cognitive image processing peculiarities. I believe that this framework will be useful to overcome the difficulties that are impeding our attempts to develop the right model of human-like intelligent image processing.", ["Andrey Kolmogorov", "Faculty (division)", "Internet", "Image processing", "Information Age", "Visual perception", "Information processing", "Gregory Chaitin"]], ["Nodally 3-connected planar graphs and convex combination mappings", "A convex combination mapping of a planar graph is a plane mapping in which the external vertices are mapped to the corners of a convex polygon and every internal vertex is a proper weighted average of its neighbours. If a planar graph is nodally 3-connected or triangulated then every such mapping is an embedding (Tutte, Floater). We give a simple characterisation of nodally 3-connected planar graphs, and generalise the above result to any planar graph which admits any convex embedding.", ["Planar graph", "Convex and concave polygons", "Convex combination", "Polygon", "Convex set", "Connectivity (graph theory)", "Vertex (geometry)", "Graph (mathematics)", "Embedding", "Plane (geometry)"]], ["Near Optimal Broadcast with Network Coding in Large Sensor Networks", "We study efficient broadcasting for wireless sensor networks, with network coding. We address this issue for homogeneous sensor networks in the plane. Our results are based on a simple principle (IREN/IRON), which sets the same rate on most of the nodes (wireless links) of the network. With this rate selection, we give a value of the maximum achievable broadcast rate of the source: our central result is a proof of the value of the min-cut for such networks, viewed as hypergraphs. Our metric for efficiency is the number of transmissions necessary to transmit one packet from the source to every destination: we show that IREN/IRON achieves near optimality for large networks; that is, asymptotically, nearly every transmission brings new information from the source to the receiver. As a consequence, network coding asymptotically outperforms any scheme that does not use network coding.", ["Transmission (telecommunications)", "Wireless", "Forward error correction", "Network packet"]], ["From symmetry break to Poisson point process in 2D Voronoi tessellations: the generic nature of hexagons", "We bridge the properties of the regular square and honeycomb Voronoi tessellations of the plane to those of the Poisson-Voronoi case, thus analyzing in a common framework symmetry-break processes and the approach to uniformly random distributions of tessellation-generating points. We consider ensemble simulations of tessellations generated by points whose regular positions are perturbed through a Gaussian noise controlled by the parameter alpha. We study the number of sides, the area, and the perimeter of the Voronoi cells. For alpha>0, hexagons are the most common class of cells, and 2-parameter gamma distributions describe well the statistics of the geometrical characteristics. The symmetry break due to noise destroys the square tessellation, whereas the honeycomb hexagonal tessellation is very stable and all Voronoi cells are hexagon for small but finite noise with alpha<0.1. For a moderate amount of Gaussian noise, memory of the specific unperturbed tessellation is lost, because the statistics of the two perturbed tessellations is indistinguishable. When alpha>2, results converge to those of Poisson-Voronoi tessellations. The properties of n-sided cells change with alpha until the Poisson-Voronoi limit is reached for alpha>2. The Desch law for perimeters is confirmed to be not valid and a square root dependence on n is established. The ensemble mean of the cells area and perimeter restricted to the hexagonal cells coincides with the full ensemble mean; this might imply that the number of sides acts as a thermodynamic state variable fluctuating about n=6; this reinforces the idea that hexagons, beyond their ubiquitous numerical prominence, can be taken as generic polygons in 2D Voronoi tessellations.", ["Poisson process", "Statistics", "Point process", "Uniform distribution (discrete)", "Hexagon", "Tessellation", "Square root", "Cell (biology)", "Thermodynamics", "2D computer graphics", "Voronoi diagram", "Symmetry", "Normal distribution", "State variable", "Thermodynamic state", "Perturbation (astronomy)", "Gaussian noise", "Honeycomb", "Parameter"]], ["A Formulation of the Channel Capacity of Multiple-Access Channel", "The necessary and sufficient condition of the channel capacity is rigorously formulated for the N-user discrete memoryless multiple-access channel (MAC). The essence of the formulation is to invoke an {\\em elementary} MAC where sizes of input alphabets are not greater than the size of output alphabet. The main objective is to demonstrate that the channel capacity of an MAC is achieved by an elementary MAC included in the original MAC. The proof is quite straightforward by the very definition of the elementary MAC. Moreover it is proved that the Kuhn-Tucker conditions of the elementary MAC are strictly sufficient and obviously necessary for the channel capacity. The latter proof requires some steps such that for the elementary MAC every solution of the Kuhn-Tucker conditions reveals itself as local maximum on the domain of all possible input probability distributions and then it achieves the channel capacity. As a result, in respect of the channel capacity, the MAC in general can be regarded as an aggregate of a finite number of elementary MAC's.", ["Channel capacity", "Maxima and minima", "Necessary and sufficient condition", "Alphabet"]], ["An Interval Analysis Based Study for the Design and the Comparison of 3-DOF Parallel Kinematic Machines", "This paper addresses an interval analysis based study that is applied to the design and the comparison of 3-DOF parallel kinematic machines. Two design criteria are used, (i) a regular workspace shape and, (ii) a kinetostatic performance index that needs to be as homogeneous as possible throughout the workspace. The interval analysis based method takes these two criteria into account: on the basis of prescribed kinetostatic performances, the workspace is analysed to find out the largest regular dextrous workspace enclosed in the Cartesian workspace. An algorithm describing this method is introduced. Two 3-DOF translational parallel mechanisms designed for machining applications are compared using this method. The first machine features three fixed linear joints which are mounted orthogonally and the second one features three linear joints which are mounted in parallel. In both cases, the mobile platform moves in the Cartesian x-y-z space with fixed orientation.", ["Orthogonality", "Linear", "Machining", "Kinematics", "Cartesian coordinate system", "Degrees of freedom (mechanics)"]], ["Nearly MDS expander codes with reduced alphabet size", "Recently, Roth and Skachek proposed two methods for constructing nearly maximum-distance separable (MDS) expander codes. We show that through the simple modification of using mixed-alphabet codes derived from MDS codes as constituent codes in their code designs, one can obtain nearly MDS codes of significantly smaller alphabet size, albeit at the expense of a (very slight) reduction in code rate.", ["Alphabet"]], ["A variant of the Recoil Growth algorithm to generate multi-polymer systems", "The Recoil Growth algorithm, proposed in 1999 by Consta et al., is one of the most efficient algorithm available in the literature to sample from a multi-polymer system. Such problems are closely related to the generation of self-avoiding paths. In this paper, we study a variant of the original Recoil Growth algorithm, where we constrain the generation of a new polymer to take place on a specific class of graphs. This makes it possible to make a fine trade-off between computational cost and success rate. We moreover give a simple proof for a lower bound on the irreducibility of this new algorithm, which applies to the original algorithm as well.", ["Time complexity", "Computational complexity theory", "Graph (mathematics)", "Polymer", "Algorithm", "Literature", "Upper and lower bounds"]], ["A Practical Ontology for the Large-Scale Modeling of Scholarly Artifacts and their Usage", "The large-scale analysis of scholarly artifact usage is constrained primarily by current practices in usage data archiving, privacy issues concerned with the dissemination of usage data, and the lack of a practical ontology for modeling the usage domain. As a remedy to the third constraint, this article presents a scholarly ontology that was engineered to represent those classes for which large-scale bibliographic and usage data exists, supports usage research, and whose instantiation is scalable to the order of 50 million articles along with their associated artifacts (e.g. authors and journals) and an accompanying 1 billion usage events. The real world instantiation of the presented abstract ontology is a semantic network model of the scholarly community which lends the scholarly process to statistical analysis and computational support. We present the ontology, discuss its instantiation, and provide some example inference rules for calculating various scholarly artifact metrics.", ["Ontology", "Semantic network", "Statistics", "Privacy", "Rule of inference", "Artifact (archaeology)", "Archive", "Data", "Bibliography"]], ["Diversity-Multiplexing Tradeoff of Asynchronous Cooperative Diversity in Wireless Networks", "Synchronization of relay nodes is an important and critical issue in exploiting cooperative diversity in wireless networks. In this paper, two asynchronous cooperative diversity schemes are proposed, namely, distributed delay diversity and asynchronous space-time coded cooperative diversity schemes. In terms of the overall diversity-multiplexing (DM) tradeoff function, we show that the proposed independent coding based distributed delay diversity and asynchronous space-time coded cooperative diversity schemes achieve the same performance as the synchronous space-time coded approach which requires an accurate symbol-level timing synchronization to ensure signals arriving at the destination from different relay nodes are perfectly synchronized. This demonstrates diversity order is maintained even at the presence of asynchronism between relay node. Moreover, when all relay nodes succeed in decoding the source information, the asynchronous space-time coded approach is capable of achieving better DM-tradeoff than synchronous schemes and performs equivalently to transmitting information through a parallel fading channel as far as the DM-tradeoff is concerned. Our results suggest the benefits of fully exploiting the space-time degrees of freedom in multiple antenna systems by employing asynchronous space-time codes even in a frequency flat fading channel. In addition, it is shown asynchronous space-time coded systems are able to achieve higher mutual information than synchronous space-time coded systems for any finite signal-to-noise-ratio (SNR) when properly selected baseband waveforms are employed.", ["Baseband", "Waveform", "Wireless network", "Noise", "Antenna (radio)", "Frequency", "Mutual information", "Fading", "Multiplexing", "Wireless", "Signal (electronics)", "Synchronization", "MIMO", "Signal-to-noise ratio", "Cooperative diversity", "Forward error correction", "Channel (communications)", "Spacetime", "Relay"]], ["A Deterministic Sub-linear Time Sparse Fourier Algorithm via Non-adaptive Compressed Sensing Methods", "We study the problem of estimating the best B term Fourier representation for a given frequency-sparse signal (i.e., vector) $\\textbf{A}$ of length $N \\gg B$. More explicitly, we investigate how to deterministically identify B of the largest magnitude frequencies of $\\hat{\\textbf{A}}$, and estimate their coefficients, in polynomial$(B,\\log N)$ time. Randomized sub-linear time algorithms which have a small (controllable) probability of failure for each processed signal exist for solving this problem. However, for failure intolerant applications such as those involving mission-critical hardware designed to process many signals over a long lifetime, deterministic algorithms with no probability of failure are highly desirable. In this paper we build on the deterministic Compressed Sensing results of Cormode and Muthukrishnan (CM) \\cite{CMDetCS3,CMDetCS1,CMDetCS2} in order to develop the first known deterministic sub-linear time sparse Fourier Transform algorithm suitable for failure intolerant applications. Furthermore, in the process of developing our new Fourier algorithm, we present a simplified deterministic Compressed Sensing algorithm which improves on CM's algebraic compressibility results while simultaneously maintaining their results concerning exponential decay.", ["Compressed sensing", "Exponential function", "Polynomial", "Fourier transform", "Fourier series", "Logarithm", "Frequency", "Exponential decay", "Probability", "Compressibility", "Hardware", "Linear time", "Algorithm", "Determinism", "Order of Canada", "Linear"]], ["Cost-minimising strategies for data labelling : optimal stopping and active learning", "Supervised learning deals with the inference of a distribution over an output or label space $\\CY$ conditioned on points in an observation space $\\CX$, given a training dataset $D$ of pairs in $\\CX \\times \\CY$. However, in a lot of applications of interest, acquisition of large amounts of observations is easy, while the process of generating labels is time-consuming or costly. One way to deal with this problem is {\\em active} learning, where points to be labelled are selected with the aim of creating a model with better performance than that of an model trained on an equal number of randomly sampled points. In this paper, we instead propose to deal with the labelling cost directly: The learning goal is defined as the minimisation of a cost which is a function of the expected model performance and the total cost of the labels used. This allows the development of general strategies and specific algorithms for (a) optimal stopping, where the expected cost dictates whether label acquisition should continue (b) empirical evaluation, where the cost is used as a performance metric for a given combination of inference, stopping and sampling methods. Though the main focus of the paper is optimal stopping, we also aim to provide the background for further developments and discussion in the related field of active learning.", ["Optimal stopping", "Empirical", "Probability distribution", "Data set", "Performance metric", "Supervised learning", "Algorithm", "Data", "Sampling (statistics)"]], ["A Matrix Ring Description for Cyclic Convolutional Codes", "In this paper, we study convolutional codes with a specific cyclic structure. By definition, these codes are left ideals in a certain skew polynomial ring. Using that the skew polynomial ring is isomorphic to a matrix ring we can describe the algebraic parameters of the codes in a more accessible way. We show that the existence of such codes with given algebraic parameters can be reduced to the solvability of a modified rook problem. It is our strong belief that the rook problem is always solvable, and we present solutions in particular cases.", ["Matrix ring", "Algebraic number", "Polynomial ring", "Matrix (mathematics)", "Solvable group", "Isomorphism", "Polynomial", "Cyclic group", "Ideal (ring theory)", "Ring (mathematics)"]], ["Physical limits of inference", "I show that physical devices that perform observation, prediction, or recollection share an underlying mathematical structure. I call devices with that structure \"inference devices\". I present a set of existence and impossibility results concerning inference devices. These results hold independent of the precise physical laws governing our universe. In a limited sense, the impossibility results establish that Laplace was wrong to claim that even in a classical, non-chaotic universe the future can be unerringly predicted, given sufficient knowledge of the present. Alternatively, these impossibility results can be viewed as a non-quantum mechanical \"uncertainty principle\". Next I explore the close connections between the mathematics of inference devices and of Turing Machines. In particular, the impossibility results for inference devices are similar to the Halting theorem for TM's. Furthermore, one can define an analog of Universal TM's (UTM's) for inference devices. I call those analogs \"strong inference devices\". I use strong inference devices to define the \"inference complexity\" of an inference task, which is the analog of the Kolmogorov complexity of computing a string. However no universe can contain more than one strong inference device. So whereas the Kolmogorov complexity of a string is arbitrary up to specification of the UTM, there is no such arbitrariness in the inference complexity of an inference task. I end by discussing the philosophical implications of these results, e.g., for whether the universe \"is\" a computer.", ["Uncertainty principle", "Pierre-Simon Laplace", "Quantum mechanics", "Kolmogorov complexity", "Mathematics", "Computing", "Theorem", "Inference", "Chaos theory", "Mathematical structure", "Philosophy", "Prediction", "Classical mechanics", "Complexity", "Knowledge", "Observation"]], ["Achievable Outage Rates with Improved Decoding of Bicm Multiband Ofdm Under Channel Estimation Errors", "We consider the decoding of bit interleaved coded modulation (BICM) applied to multiband OFDM for practical scenarios where only a noisy (possibly very bad) estimate of the channel is available at the receiver. First, a decoding metric based on the channel it a posteriori probability density, conditioned on the channel estimate is derived and used for decoding BICM multiband OFDM. Then, we characterize the limits of reliable information rates in terms of the maximal achievable outage rates associated to the proposed metric. We also compare our results with the outage rates of a system using a theoretical decoder. Our results are useful for designing a communication system where a prescribed quality of service (QoS), in terms of achievable target rates with small error probability, must be satisfied even in the presence of imperfect channel estimation. Numerical results over both realistic UWB and theoretical Rayleigh fading channels show that the proposed method provides significant gain in terms of BER and outage rates compared to the classical mismatched detector, without introducing any additional complexity.", ["Rayleigh fading", "Orthogonal frequency-division multiplexing", "A priori and a posteriori", "Channel state information", "Quality of service", "Modulation", "Decoder", "Bit", "Communication", "Probability", "Channel (communications)", "Bit error rate", "Fading"]], ["On Optimal Turbo Decoding of Wideband MIMO-OFDM Systems Under Imperfect Channel State Information", "We consider the decoding of bit interleaved coded modulation (BICM) applied to both multiband and MIMO OFDM systems for typical scenarios where only a noisy (possibly very bad) estimate of the channel is provided by sending a limited number of pilot symbols. First, by using a Bayesian framework involving the channel a posteriori density, we adopt a practical decoding metric that is robust to the presence of channel estimation errors. Then this metric is used in the demapping part of BICM multiband and MIMO OFDM receivers. We also compare our results with the performance of a mismatched decoder that replaces the channel by its estimate in the decoding metric. Numerical results over both realistic UWB and theoretical Rayleigh fading channels show that the proposed method provides significant gain in terms of bit error rate compared to the classical mismatched detector, without introducing any additional complexity.", ["Rayleigh fading", "Bit error rate", "Orthogonal frequency-division multiplexing", "A priori and a posteriori", "MIMO", "Decoder", "Fading", "Modulation", "Interleaving", "Bit", "Channel state information", "Noise (electronics)"]], ["Wavelet Based Semi-blind Channel Estimation For Multiband OFDM", "This paper introduces an expectation-maximization (EM) algorithm within a wavelet domain Bayesian framework for semi-blind channel estimation of multiband OFDM based UWB communications. A prior distribution is chosen for the wavelet coefficients of the unknown channel impulse response in order to model a sparseness property of the wavelet representation. This prior yields, in maximum a posteriori estimation, a thresholding rule within the EM algorithm. We particularly focus on reducing the number of estimated parameters by iteratively discarding ``unsignificant'' wavelet coefficients from the estimation process. Simulation results using UWB channels issued from both models and measurements show that under sparsity conditions, the proposed algorithm outperforms pilot based channel estimation in terms of mean square error and bit error rate and enhances the estimation accuracy with less computational complexity than traditional semi-blind methods.", ["Expected value", "Expectation-maximization algorithm", "Maximum a posteriori estimation", "Prior probability", "Mean squared error", "Computational complexity theory", "Bayesian probability", "Bit error rate", "A priori and a posteriori", "Orthogonal frequency-division multiplexing", "Simulation", "Estimation", "Wavelet", "Algorithm", "Probability distribution", "Neural network", "Bayes' theorem", "Impulse response", "Mean", "Ultra-wideband", "Errors and residuals in statistics"]], ["MIMO-OFDM Optimal Decoding and Achievable Information Rates Under Imperfect Channel Estimation", "Optimal decoding of bit interleaved coded modulation (BICM) MIMO-OFDM where an imperfect channel estimate is available at the receiver is investigated. First, by using a Bayesian approach involving the channel a posteriori density, we derive a practical decoding metric for general memoryless channels that is robust to the presence of channel estimation errors. Then, we evaluate the outage rates achieved by a decoder that uses our proposed metric. The performance of the proposed decoder is compared to the classical mismatched decoder and a theoretical decoder defined as the best decoder in the presence of imperfect channel estimation. Numerical results over Rayleigh block fading MIMO-OFDM channels show that the proposed decoder outperforms mismatched decoding in terms of bit error rate and outage capacity without introducing any additional complexity.", ["A priori and a posteriori", "MIMO", "Bit error rate", "Memorylessness", "Orthogonal frequency-division multiplexing", "Bayesian probability", "Channel state information", "Bit", "Decoder"]], ["Valid formulas, games and network protocols", "We describe a remarkable relation between the notion of valid formula of predicate logic and the specification of network protocols. We give several examples such as the acknowledgement of one packet or of a sequence of packets. We show how to specify the composition of protocols.", ["Predicate logic", "Communications protocol", "Network packet"]], ["On perfect, amicable, and sociable chains", "Let $x = (x_0,...,x_{n-1})$ be an n-chain, i.e., an n-tuple of non-negative integers $< n$. Consider the operator $s: x \\mapsto x' = (x'_0,...,x'_{n-1})$, where x'_j represents the number of $j$'s appearing among the components of x. An n-chain x is said to be perfect if $s(x) = x$. For example, (2,1,2,0,0) is a perfect 5-chain. Analogously to the theory of perfect, amicable, and sociable numbers, one can define from the operator s the concepts of amicable pair and sociable group of chains. In this paper we give an exhaustive list of all the perfect, amicable, and sociable chains.", ["Tuple"]], ["A Light-Based Device for Solving the Hamiltonian Path Problem", "In this paper we suggest the use of light for performing useful computations. Namely, we propose a special device which uses light rays for solving the Hamiltonian path problem on a directed graph. The device has a graph-like representation and the light is traversing it following the routes given by the connections between nodes. In each node the rays are uniquely marked so that they can be easily identified. At the destination node we will search only for particular rays that have passed only once through each node. We show that the proposed device can solve small and medium instances of the problem in reasonable time.", ["Graph (mathematics)", "Directed graph", "Hamiltonian path", "Hamiltonian path problem", "Hamiltonian (quantum mechanics)", "Vertex (graph theory)"]], ["Defensive forecasting for optimal prediction with expert advice", "The method of defensive forecasting is applied to the problem of prediction with expert advice for binary outcomes. It turns out that defensive forecasting is not only competitive with the Aggregating Algorithm but also handles the case of \"second-guessing\" experts, whose advice depends on the learner's prediction; this paper assumes that the dependence on the learner's prediction is continuous.", ["Prediction", "Forecasting"]], ["Solving the Hamiltonian path problem with a light-based computer", "In this paper we propose a special computational device which uses light rays for solving the Hamiltonian path problem on a directed graph. The device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. In each node the rays are uniquely marked so that they can be easily identified. At the destination node we will search only for particular rays that have passed only once through each node. We show that the proposed device can solve small and medium instances of the problem in reasonable time.", ["Directed graph", "Computer", "Graph (mathematics)", "Hamiltonian path problem", "Finite-state machine", "Hamiltonian path", "Vertex (graph theory)"]], ["A Data-Parallel Version of Aleph", "This is to present work on modifying the Aleph ILP system so that it evaluates the hypothesised clauses in parallel by distributing the data-set among the nodes of a parallel or distributed machine. The paper briefly discusses MPI, the interface used to access message- passing libraries for parallel computers and clusters. It then proceeds to describe an extension of YAP Prolog with an MPI interface and an implementation of data-parallel clause evaluation for Aleph through this interface. The paper concludes by testing the data-parallel Aleph on artificially constructed data-sets.", ["Data", "Library", "Parallel computing", "Prolog", "Computer"]], ["Resolution over Linear Equations and Multilinear Proofs", "We develop and study the complexity of propositional proof systems of varying strength extending resolution by allowing it to operate with disjunctions of linear equations instead of clauses. We demonstrate polynomial-size refutations for hard tautologies like the pigeonhole principle, Tseitin graph tautologies and the clique-coloring tautologies in these proof systems. Using the (monotone) interpolation by a communication game technique we establish an exponential-size lower bound on refutations in a certain, considerably strong, fragment of resolution over linear equations, as well as a general polynomial upper bound on (non-monotone) interpolants in this fragment. We then apply these results to extend and improve previous results on multilinear proofs (over fields of characteristic 0), as studied in [RazTzameret06]. Specifically, we show the following: 1. Proofs operating with depth-3 multilinear formulas polynomially simulate a certain, considerably strong, fragment of resolution over linear equations. 2. Proofs operating with depth-3 multilinear formulas admit polynomial-size refutations of the pigeonhole principle and Tseitin graph tautologies. The former improve over a previous result that established small multilinear proofs only for the \\emph{functional} pigeonhole principle. The latter are different than previous proofs, and apply to multilinear proofs of Tseitin mod p graph tautologies over any field of characteristic 0. We conclude by connecting resolution over linear equations with extensions of the cutting planes proof system.", ["Tautology (logic)", "Upper and lower bounds", "Graph (mathematics)", "Characteristic (algebra)", "Exponential function", "Multilinear map", "Pigeonhole principle", "Monotonic function", "Polynomial", "Linear equation", "Linear", "Modular arithmetic", "Communication", "Proof calculus", "Interpolation"]], ["Construction of a 3-Dimensional MDS code", "In this paper, we describe a procedure for constructing $q$--ary $[N,3,N-2]$--MDS codes, of length $N\\leq q+1$ (for $q$ odd) or $N\\leq q+2$ (for $q$ even), using a set of non--degenerate Hermitian forms in $PG(2,q^2)$.", ["3d"]], ["Learning Phonotactics Using ILP", "This paper describes experiments on learning Dutch phonotactic rules using Inductive Logic Programming, a machine learning discipline based on inductive logical operators. Two different ways of approaching the problem are experimented with, and compared against each other as well as with related work on the task. The results show a direct correspondence between the quality and informedness of the background knowledge and the constructed theory, demonstrating the ability of ILP to take good advantage of the prior domain knowledge available. Further research is outlined.", ["Machine learning", "Logic", "Inductive logic programming", "Domain knowledge", "Logic programming", "Phonotactics", "Inductive reasoning"]], ["Homogeneous temporal activity patterns in a large online communication space", "The many-to-many social communication activity on the popular technology-news website Slashdot has been studied. We have concentrated on the dynamics of message production without considering semantic relations and have found regular temporal patterns in the reaction time of the community to a news-post as well as in single user behavior. The statistics of these activities follow log-normal distributions. Daily and weekly oscillatory cycles, which cause slight variations of this simple behavior, are identified. A superposition of two log-normal distributions can account for these variations. The findings are remarkable since the distribution of the number of comments per users, which is also analyzed, indicates a great amount of heterogeneity in the community. The reader may find surprising that only a few parameters allow a detailed description, or even prediction, of social many-to-many information exchange in this kind of popular public spaces.", ["Communication", "Statistics", "Slashdot", "Semantics", "Technology", "Homogeneity and heterogeneity", "Mental chronometry"]], ["Optimal Causal Inference: Estimating Stored Information and Approximating Causal Architecture", "We introduce an approach to inferring the causal architecture of stochastic dynamical systems that extends rate distortion theory to use causal shielding---a natural principle of learning. We study two distinct cases of causal inference: optimal causal filtering and optimal causal estimation. Filtering corresponds to the ideal case in which the probability distribution of measurement sequences is known, giving a principled method to approximate a system's causal structure at a desired level of representation. We show that, in the limit in which a model complexity constraint is relaxed, filtering finds the exact causal architecture of a stochastic dynamical system, known as the causal-state partition. From this, one can estimate the amount of historical information the process stores. More generally, causal filtering finds a graded model-complexity hierarchy of approximations to the causal architecture. Abrupt changes in the hierarchy, as a function of approximation, capture distinct scales of structural organization. For nonideal cases with finite data, we show how the correct number of underlying causal states can be found by optimal causal estimation. A previously derived model complexity control term allows us to correct for the effect of statistical fluctuations in probability estimates and thereby avoid over-fitting.", ["Probability distribution", "Dynamical system", "Statistics", "Mathematical model", "Stochastic process", "Probability", "Causality", "Stochastic", "Complexity", "Architecture", "Hierarchy", "Inference", "Fluctuation theorem", "Theory"]], ["Updating Probabilities with Data and Moments", "We use the method of Maximum (relative) Entropy to process information in the form of observed data and moment constraints. The generic \"canonical\" form of the posterior distribution for the problem of simultaneous updating with data and moments is obtained. We discuss the general problem of non-commuting constraints, when they should be processed sequentially and when simultaneously. As an illustration, the multinomial example of die tosses is solved in detail for two superficially similar but actually very different problems.", ["Commutative property", "Posterior probability", "Entropy"]], ["Designing a Collaborative Research Environment for Students and their Supervisors (CRESS)", "In a previous paper the CSCR domain was defined. Here this is taken to the next stage where the design of a particular Collaborative Research Environment to support Students and Supervisors (CRESS) is considered. Following the CSCR structure this paper deals with an analysis of 13 collaborative working environments to determine a preliminary design for CRESS in order to discover the most appropriate set of tools for its implementation.", ["Paper"]], ["Hybrid Branching-Time Logics", "Hybrid branching-time logics are introduced as extensions of CTL-like logics with state variables and the downarrow-binder. Following recent work in the linear framework, only logics with a single variable are considered. The expressive power and the complexity of satisfiability of the resulting logics is investigated. As main result, the satisfiability problem for the hybrid versions of several branching-time logics is proved to be 2EXPTIME-complete. These branching-time logics range from strict fragments of CTL to extensions of CTL that can talk about the past and express fairness-properties. The complexity gap relative to CTL is explained by a corresponding succinctness result. To prove the upper bound, the automata-theoretic approach to branching-time logics is extended to hybrid logics, showing that non-emptiness of alternating one-pebble Buchi tree automata is 2EXPTIME-complete.", ["Upper and lower bounds", "Logic", "Tree automaton", "Expressive power", "Satisfiability", "EXPTIME", "Automaton"]], ["Design: One, but in different forms", "This overview paper defends an augmented cognitively oriented generic-design hypothesis: there are both significant similarities between the design activities implemented in different situations and crucial differences between these and other cognitive activities; yet, characteristics of a design situation (related to the design process, the designers, and the artefact) introduce specificities in the corresponding cognitive activities and structures that are used, and in the resulting designs. We thus augment the classical generic-design hypothesis with that of different forms of designing. We review the data available in the cognitive design research literature and propose a series of candidates underlying such forms of design, outlining a number of directions requiring further elaboration.", ["Design research", "Artifact (archaeology)", "Hypothesis", "Design"]], ["Cryptanalysis of shifted conjugacy authentication protocol", "In this paper we present the first practical attack on the shifted conjugacy-based authentication protocol proposed by P. Dehornoy. We discuss the weaknesses of that primitive and propose ways to improve the protocol.", ["Cryptanalysis", "Authentication", "Primitive (phylogenetics)"]], ["Computational Simulation and 3D Virtual Reality Engineering Tools for Dynamical Modeling and Imaging of Composite Nanomaterials", "An adventure at engineering design and modeling is possible with a Virtual Reality Environment (VRE) that uses multiple computer-generated media to let a user experience situations that are temporally and spatially prohibiting. In this paper, an approach to developing some advanced architecture and modeling tools is presented to allow multiple frameworks work together while being shielded from the application program. This architecture is being developed in a framework of workbench interactive tools for next generation nanoparticle-reinforced damping/dynamic systems. Through the use of system, an engineer/programmer can respectively concentrate on tailoring an engineering design concept of novel system and the application software design while using existing databases/software outputs.", ["Computer software", "Application software", "Virtual reality", "Programmer", "Database", "Nanoparticle", "Interactivity", "Simulation", "Software design", "Dynamical system", "Design", "Engineering", "User experience", "Damping", "Nanomaterials", "Architecture", "Computer", "3D computer graphics"]], ["Multiple-Description Coding by Dithered Delta-Sigma Quantization", "We address the connection between the multiple-description (MD) problem and Delta-Sigma quantization. The inherent redundancy due to oversampling in Delta-Sigma quantization, and the simple linear-additive noise model resulting from dithered lattice quantization, allow us to construct a symmetric and time-invariant MD coding scheme. We show that the use of a noise shaping filter makes it possible to trade off central distortion for side distortion. Asymptotically as the dimension of the lattice vector quantizer and order of the noise shaping filter approach infinity, the entropy rate of the dithered Delta-Sigma quantization scheme approaches the symmetric two-channel MD rate-distortion function for a memoryless Gaussian source and MSE fidelity criterion, at any side-to-central distortion ratio and any resolution. In the optimal scheme, the infinite-order noise shaping filter must be minimum phase and have a piece-wise flat power spectrum with a single jump discontinuity. An important advantage of the proposed design is that it is symmetric in rate and distortion by construction, so the coding rates of the descriptions are identical and there is therefore no need for source splitting.", ["Entropy rate", "Spectral density", "Oversampling", "Noise shaping", "Linear", "Entropy", "Memorylessness", "Standard illuminant", "Minimum phase", "Time-invariant system", "Quantization (signal processing)", "Dimension", "Infinity", "Additive white Gaussian noise"]], ["A nearly tight memory-redundancy trade-off for one-pass compression", "Let $s$ be a string of length $n$ over an alphabet of constant size $\\sigma$ and let $c$ and $\\epsilon$ be constants with (1 \\geq c \\geq 0) and (\\epsilon > 0). Using (O (n)) time, (O (n^c)) bits of memory and one pass we can always encode $s$ in (n H_k (s) + O (\\sigma^k n^{1 - c + \\epsilon})) bits for all integers (k \\geq 0) simultaneously. On the other hand, even with unlimited time, using (O (n^c)) bits of memory and one pass we cannot always encode $s$ in (O (n H_k (s) + \\sigma^k n^{1 - c - \\epsilon})) bits for, e.g., (k = \\lceil (c + \\epsilon / 2) \\log_\\sigma n \\rceil).", ["Trade-off"]], ["On Edge-Disjoint Pairs Of Matchings", "For a graph G, consider the pairs of edge-disjoint matchings whose union consists of as many edges as possible. Let H be the largest matching among such pairs. Let M be a maximum matching of G. We show that 5/4 is a tight upper bound for |M|/|H|.", ["Matching (graph theory)"]], ["Lower Bounds for the Complexity of the Voronoi Diagram of Polygonal Curves under the Discrete Frechet Distance", "We give lower bounds for the combinatorial complexity of the Voronoi diagram of polygonal curves under the discrete Frechet distance. We show that the Voronoi diagram of n curves in R^d with k vertices each, has complexity Omega(n^{dk}) for dimension d=1,2 and Omega(n^{d(k-1)+2}) for d>2.", ["Voronoi diagram", "Combinatorics", "Polygonal chain", "Dimension", "Complexity"]], ["Exact Cover with light", "We suggest a new optical solution for solving the YES/NO version of the Exact Cover problem by using the massive parallelism of light. The idea is to build an optical device which can generate all possible solutions of the problem and then to pick the correct one. In our case the device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. The nodes are connected by arcs in a special way which lets us to generate all possible covers (exact or not) of the given set. For selecting the correct solution we assign to each item, from the set to be covered, a special integer number. These numbers will actually represent delays induced to light when it passes through arcs. The solution is represented as a subray arriving at a certain moment in the destination node. This will tell us if an exact cover does exist or not.", ["Integer", "Exact cover"]], ["Solving the subset-sum problem with a light-based device", "We propose a special computational device which uses light rays for solving the subset-sum problem. The device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. The nodes are connected by arcs in a special way which lets us to generate all possible subsets of the given set. To each arc we assign either a number from the given set or a predefined constant. When the light is passing through an arc it is delayed by the amount of time indicated by the number placed in that arc. At the destination node we will check if there is a ray whose total delay is equal to the target value of the subset sum problem (plus some constants).", ["Subset sum problem", "Graph (mathematics)", "Finite-state machine"]], ["Who is the best connected EC researcher? Centrality analysis of the complex network of authors in evolutionary computation", "Co-authorship graphs (that is, the graph of authors linked by co-authorship of papers) are complex networks, which expresses the dynamics of a complex system. Only recently its study has started to draw interest from the EC community, the first paper dealing with it having been published two years ago. In this paper we will study the co-authorship network of EC at a microscopic level. Our objective is ascertaining which are the most relevant nodes (i.e. authors) in it. For this purpose, we examine several metrics defined in the complex-network literature, and analyze them both in isolation and combined within a Pareto-dominance approach. The result of our analysis indicates that there are some well-known researchers that appear systematically in top rankings. This also provides some hints on the social behavior of our community.", ["Complex network", "Evolutionary computation", "Computation", "Centrality", "Social behavior", "Literature", "Complex system"]], ["Nonantagonistic noisy duels of discrete type with an arbitrary number of actions", "We study a nonzero-sum game of two players which is a generalization of the antagonistic noisy duel of discrete type. The game is considered from the point of view of various criterions of optimality. We prove existence of epsilon-equilibrium situations and show that the epsilon-equilibrium strategies that we have found are epsilon-maxmin. Conditions under which the equilibrium plays are Pareto-optimal are given. Keywords: noisy duel, payoff function, strategy, equilibrium situation, Pareto optimality, the value of a game.", ["Pareto efficiency"]], ["Derivative of BICM Mutual Information", "In this letter we determine the derivative of the mutual information corresponding to bit-interleaved coded modulation systems. The derivative follows as a linear combination of minimum-mean-squared error functions of coded modulation sets. The result finds applications to the analysis of communications systems in the wideband regime and to the design of power allocation over parallel channels.", ["Mutual information", "Linear combination", "Modulation", "Wideband", "Derivative", "Bit"]], ["Repairing Inconsistent XML Write-Access Control Policies", "XML access control policies involving updates may contain security flaws, here called inconsistencies, in which a forbidden operation may be simulated by performing a sequence of allowed operations. This paper investigates the problem of deciding whether a policy is consistent, and if not, how its inconsistencies can be repaired. We consider policies expressed in terms of annotated DTDs defining which operations are allowed or denied for the XML trees that are instances of the DTD. We show that consistency is decidable in PTIME for such policies and that consistent partial policies can be extended to unique \"least-privilege\" consistent total policies. We also consider repair problems based on deleting privileges to restore consistency, show that finding minimal repairs is NP-complete, and give heuristics for finding repairs.", ["NP-complete", "P (complexity)", "XML", "Heuristic", "Decision problem"]], ["Obstructions to Genericity in Study of Parametric Problems in Control Theory", "We investigate systems of equations, involving parameters from the point of view of both control theory and computer algebra. The equations might involve linear operators such as partial (q-)differentiation, (q-)shift, (q-)difference as well as more complicated ones, which act trivially on the parameters. Such a system can be identified algebraically with a certain left module over a non-commutative algebra, where the operators commute with the parameters. We develop, implement and use in practice the algorithm for revealing all the expressions in parameters, for which e.g. homological properties of a system differ from the generic properties. We use Groebner bases and Groebner basics in rings of solvable type as main tools. In particular, we demonstrate an optimized algorithm for computing the left inverse of a matrix over a ring of solvable type. We illustrate the article with interesting examples. In particular, we provide a complete solution to the \"two pendula, mounted on a cart\" problem from the classical book of Polderman and Willems, including the case, where the friction at the joints is essential . To the best of our knowledge, the latter example has not been solved before in a complete way.", ["Commutative algebra", "Inverse element", "Solvable group", "Noncommutative geometry", "Algebra", "Commutative property", "Matrix (mathematics)", "Control theory", "Module (mathematics)", "Inverse function", "Linear map", "Invertible matrix", "Homological algebra", "Derivative", "Computing", "Computer", "Ring (mathematics)", "Simultaneous equations", "Computer algebra system", "Algorithm", "Generic property"]], ["Empirical entropy in context", "We trace the history of empirical entropy, touching briefly on its relation to Markov processes, normal numbers, Shannon entropy, the Chomsky hierarchy, Kolmogorov complexity, Ziv-Lempel compression, de Bruijn sequences and stochastic complexity.", ["Kolmogorov complexity", "Entropy (information theory)", "Chomsky hierarchy", "Markov chain", "De Bruijn sequence", "Empirical", "Entropy", "Stochastic", "Claude Shannon", "Hierarchy", "Andrey Kolmogorov"]], ["Attribute Estimation and Testing Quasi-Symmetry", "A Boolean function is symmetric if it is invariant under all permutations of its arguments; it is quasi-symmetric if it is symmetric with respect to the arguments on which it actually depends. We present a test that accepts every quasi-symmetric function and, except with an error probability at most delta>0, rejects every function that differs from every quasi-symmetric function on at least a fraction epsilon>0 of the inputs. For a function of n arguments, the test probes the function at O((n/epsilon)\\log(n/delta)) inputs. Our quasi-symmetry test acquires information concerning the arguments on which the function actually depends. To do this, it employs a generalization of the property testing paradigm that we call attribute estimation. Like property testing, attribute estimation uses random sampling to obtain results that have only \"one-sided'' errors and that are close to accurate with high probability.", ["Boolean function", "Probability", "Symmetric function", "Function (mathematics)", "Paradigm", "Permutation", "Symmetry"]], ["Provenance as Dependency Analysis", "Provenance is information recording the source, derivation, or history of some information. Provenance tracking has been studied in a variety of settings; however, although many design points have been explored, the mathematical or semantic foundations of data provenance have received comparatively little attention. In this paper, we argue that dependency analysis techniques familiar from program analysis and program slicing provide a formal foundation for forms of provenance that are intended to show how (part of) the output of a query depends on (parts of) its input. We introduce a semantic characterization of such dependency provenance, show that this form of provenance is not computable, and provide dynamic and static approximation techniques.", ["Mathematics", "Program analysis", "Provenance", "Semantics"]], ["Moderate Growth Time Series for Dynamic Combinatorics Modelisation", "Here, we present a family of time series with a simple growth constraint. This family can be the basis of a model to apply to emerging computation in business and micro-economy where global functions can be expressed from local rules. We explicit a double statistics on these series which allows to establish a one-to-one correspondence between three other ballot-like strunctures.", ["Computation"]], ["Collection analysis for Horn clause programs", "We consider approximating data structures with collections of the items that they contain. For examples, lists, binary trees, tuples, etc, can be approximated by sets or multisets of the items within them. Such approximations can be used to provide partial correctness properties of logic programs. For example, one might wish to specify than whenever the atom $sort(t,s)$ is proved then the two lists $t$ and $s$ contain the same multiset of items (that is, $s$ is a permutation of $t$). If sorting removes duplicates, then one would like to infer that the sets of items underlying $t$ and $s$ are the same. Such results could be useful to have if they can be determined statically and automatically. We present a scheme by which such collection analysis can be structured and automated. Central to this scheme is the use of linear logic as a omputational logic underlying the logic of Horn clauses.", ["Horn clause", "Multiset", "Linear logic", "Correctness (computer science)", "Permutation", "Atom", "Logic programming", "Tuple", "Data structure", "Binary tree", "Logic"]], ["Focusing and Polarization in Intuitionistic Logic", "A focused proof system provides a normal form to cut-free proofs that structures the application of invertible and non-invertible inference rules. The focused proof system of Andreoli for linear logic has been applied to both the proof search and the proof normalization approaches to computation. Various proof systems in literature exhibit characteristics of focusing to one degree or another. We present a new, focused proof system for intuitionistic logic, called LJF, and show how other proof systems can be mapped into the new system by inserting logical connectives that prematurely stop focusing. We also use LJF to design a focused proof system for classical logic. Our approach to the design and analysis of these systems is based on the completeness of focusing in linear logic and on the notion of polarity that appears in Girard's LC and LU proof systems.", ["Linear logic", "Intuitionistic logic", "Classical logic", "Automated theorem proving", "Cut-elimination theorem", "Completeness", "Rule of inference", "Logical connective", "Inference", "Structure (mathematical logic)", "Proof calculus", "Inverse element", "Literature", "Logic", "Mathematical proof"]], ["A Language for Generic Programming in the Large", "Generic programming is an effective methodology for developing reusable software libraries. Many programming languages provide generics and have features for describing interfaces, but none completely support the idioms used in generic programming. To address this need we developed the language G. The central feature of G is the concept, a mechanism for organizing constraints on generics that is inspired by the needs of modern C++ libraries. G provides modular type checking and separate compilation (even of generics). These characteristics support modular software development, especially the smooth integration of independently developed components. In this article we present the rationale for the design of G and demonstrate the expressiveness of G with two case studies: porting the Standard Template Library and the Boost Graph Library from C++ to G. The design of G shares much in common with the concept extension proposed for the next C++ Standard (the authors participated in its design) but there are important differences described in this article.", ["Standard Template Library", "Generic programming", "Porting", "Boost C++ Libraries", "Library (computing)", "Software development", "Programming language", "Compiler", "Modular programming", "C (programming language)", "Type checking", "Computer programming", "Library", "Computer software"]], ["The study of a new gerrymandering methodology", "This paper is to obtain a simple dividing-diagram of the congressional districts, where the only limit is that each district should contain the same population if possibly. In order to solve this problem, we introduce three different standards of the \"simple\" shape. The first standard is that the final shape of the congressional districts should be of a simplest figure and we apply a modified \"shortest split line algorithm\" where the factor of the same population is considered only. The second standard is that the gerrymandering should ensure the integrity of the current administrative area as the convenience for management. Thus we combine the factor of the administrative area with the first standard, and generate an improved model resulting in the new diagram in which the perimeters of the districts are along the boundaries of some current counties. Moreover, the gerrymandering should consider the geographic features.The third standard is introduced to describe this situation. Finally, it can be proved that the difference between the supporting ratio of a certain party in each district and the average supporting ratio of that particular party in the whole state obeys the Chi-square distribution approximately. Consequently, we can obtain an archetypal formula to check whether the gerrymandering we propose is fair.", ["Gerrymandering", "Algorithm", "Chi-square distribution", "Archetype"]], ["Capacity of the Degraded Half-Duplex Relay Channel", "A discrete memoryless half-duplex relay channel is constructed from a broadcast channel from the source to the relay and destination and a multiple access channel from the source and relay to the destination. When the relay listens, the channel operates in the broadcast mode. The channel switches to the multiple access mode when the relay transmits. If the broadcast component channel is physically degraded, the half-duplex relay channel will also be referred to as physically degraded. The capacity of this degraded half-duplex relay channel is examined. It is shown that the block Markov coding suggested in the seminal paper by Cover and El Gamal can be modified to achieve capacity for the degraded half-duplex relay channel. In the code construction, the listen-transmit schedule of the relay is made to depend on the message to be sent and hence the schedule carries information itself. If the schedule is restricted to be deterministic, it is shown that the capacity can be achieved by a simple management of information flows across the broadcast and multiple access component channels.", ["Half-duplex", "Duplex (telecommunications)", "Memorylessness"]], ["Opportunism in Multiuser Relay Channels: Scheduling, Routing and Spectrum Reuse", "In order to understand the key merits of multiuser diversity techniques in relay-assisted cellular multihop networks, this paper analyzes the spectral efficiency of opportunistic (i.e., channel-aware) scheduling algorithms over a fading multiuser relay channel with $K$ users in the asymptotic regime of large (but finite) number of users. Using tools from extreme-value theory, we characterize the limiting distribution of spectral efficiency focusing on Type I convergence and utilize it in investigating the large system behavior of the multiuser relay channel as a function of the number of users and physical channel signal-to-noise ratios (SNRs). Our analysis results in very accurate formulas in the large (but finite) $K$ regime, provides insights on the potential performance enhancements from multihop routing and spectrum reuse policies in the presence of multiuser diversity gains from opportunistic scheduling and helps to identify the regimes and conditions in which relay-assisted multiuser communication provides a clear advantage over direct multiuser communication.", ["Diversity scheme", "Algorithm", "Communication", "Value theory", "Signal-to-noise ratio", "Spectral efficiency", "Routing", "Scheduling algorithm"]], ["Compositional Semantics Grounded in Commonsense Metaphysics", "We argue for a compositional semantics grounded in a strongly typed ontology that reflects our commonsense view of the world and the way we talk about it in ordinary language. Assuming the existence of such a structure, we show that the semantics of various natural language phenomena may become nearly trivial.", ["Ontology", "Semantics", "Strong typing", "Natural language"]], ["On Compact Routing for the Internet", "While there exist compact routing schemes designed for grids, trees, and Internet-like topologies that offer routing tables of sizes that scale logarithmically with the network size, we demonstrate in this paper that in view of recent results in compact routing research, such logarithmic scaling on Internet-like topologies is fundamentally impossible in the presence of topology dynamics or topology-independent (flat) addressing. We use analytic arguments to show that the number of routing control messages per topology change cannot scale better than linearly on Internet-like topologies. We also employ simulations to confirm that logarithmic routing table size scaling gets broken by topology-independent addressing, a cornerstone of popular locator-identifier split proposals aiming at improving routing scaling in the presence of network topology dynamics or host mobility. These pessimistic findings lead us to the conclusion that a fundamental re-examination of assumptions behind routing models and abstractions is needed in order to find a routing architecture that would be able to scale ``indefinitely.''", ["Internet", "Routing table", "Network topology", "Topology", "Architecture"]], ["Benefiting from Disorder: Source Coding for Unordered Data", "The order of letters is not always relevant in a communication task. This paper discusses the implications of order irrelevance on source coding, presenting results in several major branches of source coding theory: lossless coding, universal lossless coding, rate-distortion, high-rate quantization, and universal lossy coding. The main conclusions demonstrate that there is a significant rate savings when order is irrelevant. In particular, lossless coding of n letters from a finite alphabet requires Theta(log n) bits and universal lossless coding requires n + o(n) bits for many countable alphabet sources. However, there are no universal schemes that can drive a strong redundancy measure to zero. Results for lossy coding include distribution-free expressions for the rate savings from order irrelevance in various high-rate quantization schemes. Rate-distortion bounds are given, and it is shown that the analogue of the Shannon lower bound is loose at all finite rates.", ["Countable set", "Non-parametric statistics", "Coding theory", "Communication", "Data compression", "Finite set", "Alphabet", "Upper and lower bounds", "Lossy compression"]], ["On Semimeasures Predicting Martin-Loef Random Sequences", "Solomonoff's central result on induction is that the posterior of a universal semimeasure M converges rapidly and with probability 1 to the true sequence generating posterior mu, if the latter is computable. Hence, M is eligible as a universal sequence predictor in case of unknown mu. Despite some nearby results and proofs in the literature, the stronger result of convergence for all (Martin-Loef) random sequences remained open. Such a convergence result would be particularly interesting and natural, since randomness can be defined in terms of M itself. We show that there are universal semimeasures M which do not converge for all random sequences, i.e. we give a partial negative answer to the open problem. We also provide a positive answer for some non-universal semimeasures. We define the incomputable measure D as a mixture over all computable measures and the enumerable semimeasure W as a mixture over all enumerable nearly-measures. We show that W converges to D and D to mu on all random sequences. The Hellinger distance measuring closeness of two distributions plays a central role.", ["Hellinger distance", "Sequence", "Limit of a sequence", "Computability theory", "Randomness", "Mathematical induction", "Almost surely", "Probability", "Measure (mathematics)", "Open problem", "Mathematical proof", "Convergent series"]], ["Unsatisfiable Linear k-CNFs Exist, for every k", "We call a CNF formula linear if any two clauses have at most one variable in common. Let Linear k-SAT be the problem of deciding whether a given linear k-CNF formula is satisfiable. Here, a k-CNF formula is a CNF formula in which every clause has size exactly k. It was known that for k >= 3, Linear k-SAT is NP-complete if and only if an unsatisfiable linear k-CNF formula exists, and that they do exist for k >= 4. We prove that unsatisfiable linear k-CNF formulas exist for every k. Let f(k) be the minimum number of clauses in an unsatisfiable linear k-CNF formula. We show that f(k) is Omega(k2^k) and O(4^k*k^4), i.e., minimum size unsatisfiable linear k-CNF formulas are significantly larger than minimum size unsatisfiable k-CNF formulas. Finally, we prove that, surprisingly, linear k-CNF formulas do not allow for a larger fraction of clauses to be satisfied than general k-CNF formulas.", ["NP-complete", "If and only if", "Decision problem"]], ["Randomized algorithm for the k-server problem on decomposable spaces", "We study the randomized k-server problem on metric spaces consisting of widely separated subspaces. We give a method which extends existing algorithms to larger spaces with the growth rate of the competitive quotients being at most O(log k). This method yields o(k)-competitive algorithms solving the randomized k-server problem, for some special underlying metric spaces, e.g. HSTs of \"small\" height (but unbounded degree). HSTs are important tools for probabilistic approximation of metric spaces.", ["Randomized algorithm", "Algorithm", "Metric space", "Metric (mathematics)"]], ["Continuous and randomized defensive forecasting: unified view", "Defensive forecasting is a method of transforming laws of probability (stated in game-theoretic terms as strategies for Sceptic) into forecasting algorithms. There are two known varieties of defensive forecasting: \"continuous\", in which Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous manner and which produces deterministic forecasts, and \"randomized\", in which the dependence of Sceptic's moves on the forecasts is arbitrary and Forecaster's moves are allowed to be randomized. This note shows that the randomized variety can be obtained from the continuous variety by smearing Sceptic's moves to make them continuous.", ["Probability theory", "Probability", "Skepticism", "Game theory"]], ["On a constructive characterization of a class of trees related to pairs of disjoint matchings", "For a graph consider the pairs of disjoint matchings which union contains as many edges as possible, and define a parameter $\\alpha$ which eqauls the cardinality of the largest matching in those pairs. Also, define $\\betta$ to be the cardinality of a maximum matching of the graph. We give a constructive characterization of trees which satisfy the $\\alpha$=$\\betta$ equality. The proof of our main theorem is based on a new decomposition algorithm obtained for trees.", ["Matching (graph theory)", "Cardinality", "Algorithm", "Theorem", "Graph (mathematics)"]], ["Key Agreement and Authentication Schemes Using Non-Commutative Semigroups", "We give a new two-pass authentication scheme, whichis a generalisation of an authentication scheme of Sibert-Dehornoy-Girault based on the Diffie-Hellman conjugacy problem. Compared to the above scheme, for some parameters it is more efficient with respect to multiplications. We sketch a proof that our authentication scheme is secure. We give a new key agreement protocols.", ["Diffie-Hellman", "Key-agreement protocol", "Whitfield Diffie"]], ["On the AAGL Protocol", "Recently the AAGL (Anshel-Anshel-Goldfeld-Lemieux) has been proposed which can be used for RFID tags. We give algorithms for the problem (we call the MSCSPv) on which the security of the AAGL protocol is based upon. Hence we give various attacks for general parameters on the recent AAGL protocol proposed. One of our attacks is a deterministic algorithm which has space complexity and time complexity both atleast exponentialin the worst case. In a better case using a probabilistic algorithm the time complexity canbe O(|XSS(ui')^L5*(n^(1+e)) and the space complexity can be O(|XSS(ui')|^L6), where the element ui' is part of a public key, n is the index of braid group, XSS is a summit type set and e is a constant in a limit. The above shows the AAGL protocol is potentially not significantly more secure as using key agreement protocols based on the conjugacy problem such as the AAG (Anshel-Anshel-Goldfeld) protocol because both protocols can be broken with complexity which do not significantly differ. We think our attacks can be improved.", ["Time complexity", "Public-key cryptography", "Randomized algorithm", "Analysis of algorithms", "Algorithm", "Deterministic algorithm", "Best, worst and average case", "Braid group", "Conjugacy problem", "Radio-frequency identification"]], ["User Participation in Social Media: Digg Study", "The social news aggregator Digg allows users to submit and moderate stories by voting on (digging) them. As is true of most social sites, user participation on Digg is non-uniformly distributed, with few users contributing a disproportionate fraction of content. We studied user participation on Digg, to see whether it is motivated by competition, fueled by user ranking, or social factors, such as community acceptance. For our study we collected activity data of the top users weekly over the course of a year. We computed the number of stories users submitted, dugg or commented on weekly. We report a spike in user activity in September 2006, followed by a gradual decline, which seems unaffected by the elimination of user ranking. The spike can be explained by a controversy that broke out at the beginning of September 2006. We believe that the lasting acrimony that this incident has created led to a decline of top user participation on Digg.", ["Digg", "News aggregator", "Social media"]], ["A structure from motion inequality", "We state an elementary inequality for the structure from motion problem for m cameras and n points. This structure from motion inequality relates space dimension, camera parameter dimension, the number of cameras and number points and global symmetry properties and provides a rigorous criterion for which reconstruction is not possible with probability 1. Mathematically the inequality is based on Frobenius theorem which is a geometric incarnation of the fundamental theorem of linear algebra. The paper also provides a general mathematical formalism for the structure from motion problem. It includes the situation the points can move while the camera takes the pictures.", ["Linear algebra", "Algebra", "Geometry", "Symmetry", "Mathematics", "Dimension", "Frobenius theorem (differential topology)", "Basis (linear algebra)", "Theorem", "Structure from motion", "Global symmetry", "Probability"]], ["On Ullman's theorem in computer vision", "Both in the plane and in space, we invert the nonlinear Ullman transformation for 3 points and 3 orthographic cameras. While Ullman's theorem assures a unique reconstruction modulo a reflection for 3 cameras and 4 points, we find a locally unique reconstruction for 3 cameras and 3 points. Explicit reconstruction formulas allow to decide whether picture data of three cameras seeing three points can be realized as a point-camera configuration.", ["Computer vision", "Camera"]], ["Space and camera path reconstruction for omni-directional vision", "In this paper, we address the inverse problem of reconstructing a scene as well as the camera motion from the image sequence taken by an omni-directional camera. Our structure from motion results give sharp conditions under which the reconstruction is unique. For example, if there are three points in general position and three omni-directional cameras in general position, a unique reconstruction is possible up to a similarity. We then look at the reconstruction problem with m cameras and n points, where n and m can be large and the over-determined system is solved by least square methods. The reconstruction is robust and generalizes to the case of a dynamic environment where landmarks can move during the movie capture. Possible applications of the result are computer assisted scene reconstruction, 3D scanning, autonomous robot navigation, medical tomography and city reconstructions.", ["Autonomous robot", "Overdetermined system", "Robot", "Tomography", "3D scanner", "Structure from motion", "General position", "Inverse problem", "Computer", "Camera", "Navigation"]], ["On the subset sum problem over finite fields", "The subset sum problem over finite fields is a well-known {\\bf NP}-complete problem. It arises naturally from decoding generalized Reed-Solomon codes. In this paper, we study the number of solutions of the subset sum problem from a mathematical point of view. In several interesting cases, we obtain explicit or asymptotic formulas for the solution number. As a consequence, we obtain some results on the decoding problem of Reed-Solomon codes.", ["Subset sum problem", "Mathematics", "Finite field", "Reed-Solomon", "Reed-Solomon error correction", "Finite set", "Subset"]], ["Eigenvalue bounds on the pseudocodeword weight of expander codes", "Four different ways of obtaining low-density parity-check codes from expander graphs are considered. For each case, lower bounds on the minimum stopping set size and the minimum pseudocodeword weight of expander (LDPC) codes are derived. These bounds are compared with the known eigenvalue-based lower bounds on the minimum distance of expander codes. Furthermore, Tanner's parity-oriented eigenvalue lower bound on the minimum distance is generalized to yield a new lower bound on the minimum pseudocodeword weight. These bounds are useful in predicting the performance of LDPC codes under graph-based iterative decoding and linear programming decoding.", ["Eigenvalues and eigenvectors", "Linear programming", "Iteration", "Graph (mathematics)", "Expander graph", "Upper and lower bounds", "Low-density parity-check code"]], ["Minimum Cost Homomorphisms to Reflexive Digraphs", "For digraphs $G$ and $H$, a homomorphism of $G$ to $H$ is a mapping $f:\\ V(G)\\dom V(H)$ such that $uv\\in A(G)$ implies $f(u)f(v)\\in A(H)$. If moreover each vertex $u \\in V(G)$ is associated with costs $c_i(u), i \\in V(H)$, then the cost of a homomorphism $f$ is $\\sum_{u\\in V(G)}c_{f(u)}(u)$. For each fixed digraph $H$, the {\\em minimum cost homomorphism problem} for $H$, denoted MinHOM($H$), is the following problem. Given an input digraph $G$, together with costs $c_i(u)$, $u\\in V(G)$, $i\\in V(H)$, and an integer $k$, decide if $G$ admits a homomorphism to $H$ of cost not exceeding $k$. We focus on the minimum cost homomorphism problem for {\\em reflexive} digraphs $H$ (every vertex of $H$ has a loop). It is known that the problem MinHOM($H$) is polynomial time solvable if the digraph $H$ has a {\\em Min-Max ordering}, i.e., if its vertices can be linearly ordered by $<$ so that $i<j, s<r$ and $ir, js \\in A(H)$ imply that $is \\in A(H)$ and $jr \\in A(H)$. We give a forbidden induced subgraph characterization of reflexive digraphs with a Min-Max ordering; our characterization implies a polynomial time test for the existence of a Min-Max ordering. Using this characterization, we show that for a reflexive digraph $H$ which does not admit a Min-Max ordering, the minimum cost homomorphism problem is NP-complete. Thus we obtain a full dichotomy classification of the complexity of minimum cost homomorphism problems for reflexive digraphs.", ["Forbidden graph characterization", "Induced subgraph", "NP-complete", "Directed graph", "Subgraph", "Polynomial time", "Homomorphism", "Vertex (graph theory)", "Solvable group", "Total order", "Reflexive relation", "Digraph (orthography)", "Integer"]], ["On the Complexity of the Minimum Cost Homomorphism Problem for Reflexive Multipartite Tournaments", "For digraphs $D$ and $H$, a mapping $f: V(D)\\dom V(H)$ is a homomorphism of $D$ to $H$ if $uv\\in A(D)$ implies $f(u)f(v)\\in A(H).$ For a fixed digraph $H$, the homomorphism problem is to decide whether an input digraph $D$ admits a homomorphism to $H$ or not, and is denoted as HOMP($H$). Digraphs are allowed to have loops, but not allowed to have parallel arcs. A natural optimization version of the homomorphism problem is defined as follows. If each vertex $u \\in V(D)$ is associated with costs $c_i(u), i \\in V(H)$, then the cost of the homomorphism $f$ is $\\sum_{u\\in V(D)}c_{f(u)}(u)$. For each fixed digraph $H$, we have the {\\em minimum cost homomorphism problem for} $H$ and denote it as MinHOMP($H$). The problem is to decide, for an input graph $D$ with costs $c_i(u),$ $u \\in V(D), i\\in V(H)$, whether there exists a homomorphism of $D$ to $H$ and, if one exists, to find one of minimum cost. In a recent paper, we posed a problem of characterizing polynomial time solvable and NP-hard cases of the minimum cost homomorphism problem for acyclic multipartite tournaments with possible loops (w.p.l.). In this paper, we solve the problem for reflexive multipartite tournaments and demonstrate a considerate difficulty of the problem for the whole class of multipartite tournaments w.p.l. using, as an example, acyclic 3-partite tournaments of order 4 w.p.l.\\footnote{This paper was submitted to Discrete Mathematics on April 6, 2007}", ["NP-hard", "Directed graph", "Homomorphism", "Polynomial time", "Optimization problem", "Vertex (graph theory)", "Solvable group", "Graph (mathematics)", "Complexity", "Mathematical optimization", "Glossary of graph theory", "NP (complexity)", "Mathematics", "Directed acyclic graph", "Discrete mathematics"]], ["Complexity of the Minimum Cost Homomorphism Problem for Semicomplete Digraphs with Possible Loops", "For digraphs $D$ and $H$, a mapping $f: V(D)\\dom V(H)$ is a homomorphism of $D$ to $H$ if $uv\\in A(D)$ implies $f(u)f(v)\\in A(H).$ For a fixed digraph $H$, the homomorphism problem is to decide whether an input digraph $D$ admits a homomorphism to $H$ or not, and is denoted as HOM($H$). An optimization version of the homomorphism problem was motivated by a real-world problem in defence logistics and was introduced in \\cite{gutinDAM154a}. If each vertex $u \\in V(D)$ is associated with costs $c_i(u), i \\in V(H)$, then the cost of the homomorphism $f$ is $\\sum_{u\\in V(D)}c_{f(u)}(u)$. For each fixed digraph $H$, we have the {\\em minimum cost homomorphism problem for} $H$ and denote it as MinHOM($H$). The problem is to decide, for an input graph $D$ with costs $c_i(u),$ $u \\in V(D), i\\in V(H)$, whether there exists a homomorphism of $D$ to $H$ and, if one exists, to find one of minimum cost. Although a complete dichotomy classification of the complexity of MinHOM($H$) for a digraph $H$ remains an unsolved problem, complete dichotomy classifications for MinHOM($H$) were proved when $H$ is a semicomplete digraph \\cite{gutinDAM154b}, and a semicomplete multipartite digraph \\cite{gutinDAM}. In these studies, it is assumed that the digraph $H$ is loopless. In this paper, we present a full dichotomy classification for semicomplete digraphs with possible loops, which solves a problem in \\cite{gutinRMS}.\\footnote{This paper was submitted to SIAM J. Discrete Math. on October 27, 2006}", ["Homomorphism", "Optimization problem", "Directed graph", "Logistics", "Graph (mathematics)", "Vertex (graph theory)", "Mathematical optimization", "Complexity"]], ["Discrete Denoising with Shifts", "We introduce S-DUDE, a new algorithm for denoising DMC-corrupted data. The algorithm, which generalizes the recently introduced DUDE (Discrete Universal DEnoiser) of Weissman et al., aims to compete with a genie that has access, in addition to the noisy data, also to the underlying clean data, and can choose to switch, up to $m$ times, between sliding window denoisers in a way that minimizes the overall loss. When the underlying data form an individual sequence, we show that the S-DUDE performs essentially as well as this genie, provided that $m$ is sub-linear in the size of the data. When the clean data is emitted by a piecewise stationary process, we show that the S-DUDE achieves the optimum distribution-dependent performance, provided that the same sub-linearity condition is imposed on the number of switches. To further substantiate the universal optimality of the S-DUDE, we show that when the number of switches is allowed to grow linearly with the size of the data, \\emph{any} (sequence of) scheme(s) fails to compete in the above senses. Using dynamic programming, we derive an efficient implementation of the S-DUDE, which has complexity (time and memory) growing only linearly with the data size and the number of switches $m$. Preliminary experimental results are presented, suggesting that S-DUDE has the capacity to significantly improve on the performance attained by the original DUDE in applications where the nature of the data abruptly changes in time (or space), as is often the case in practice.", ["Dynamic programming", "Stationary process", "Algorithm", "Linear", "Piecewise", "Linear function", "Data", "Noise reduction", "Sequence"]], ["On the Security of the Cha-Ko-Lee-Han-Cheon Braid Group Public Key Cryptosystem", "We show that a number of cryptographic protocols using non-commutative semigroups including the Cha-Ko-Lee-Han-Cheon braid group public-key cryptosystem and related public-key cryptosystems such as the Shpilrain-Ushakov public-key cryptosystems are based on the MSCSP.", ["Public-key cryptography", "Braid group", "Cryptosystem", "Cryptography", "Semigroup"]]]